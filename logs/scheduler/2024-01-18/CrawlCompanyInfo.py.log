[2024-01-18T09:52:07.043+0000] {processor.py:161} INFO - Started process (PID=755) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:52:07.044+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:52:07.048+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:52:07.048+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:52:07.074+0000] {processor.py:840} INFO - DAG(s) 'DEMO-DAG1' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:52:07.193+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:52:07.192+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:DEMO-DAG1
[2024-01-18T09:52:07.196+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:52:07.196+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:DEMO-DAG1
[2024-01-18T09:52:07.198+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:52:07.198+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:DEMO-DAG1
[2024-01-18T09:52:07.199+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:52:07.199+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:52:07.204+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:52:07.204+0000] {dag.py:3055} INFO - Creating ORM DAG for DEMO-DAG1
[2024-01-18T09:52:07.210+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:52:07.210+0000] {dag.py:3820} INFO - Setting next_dagrun for DEMO-DAG1 to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:52:07.216+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.177 seconds
[2024-01-18T09:52:11.289+0000] {processor.py:161} INFO - Started process (PID=756) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:52:11.291+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:52:11.294+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:52:11.294+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:52:11.315+0000] {processor.py:840} INFO - DAG(s) 'DEMO-DAG1' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:52:11.326+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:52:11.326+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:52:11.341+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:52:11.341+0000] {dag.py:3820} INFO - Setting next_dagrun for DEMO-DAG1 to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:52:11.351+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-18T09:52:59.776+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:52:59.777+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:52:59.779+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:52:59.778+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:52:59.795+0000] {processor.py:840} INFO - DAG(s) 'DEMO-DAG1' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:52:59.932+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:52:59.932+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:52:59.941+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:52:59.941+0000] {dag.py:3820} INFO - Setting next_dagrun for DEMO-DAG1 to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:52:59.950+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.177 seconds
[2024-01-18T09:53:30.050+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:30.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:53:30.056+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:30.056+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:30.080+0000] {processor.py:840} INFO - DAG(s) 'DEMO-DAG1' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:30.100+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:30.099+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:53:30.111+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:30.110+0000] {dag.py:3820} INFO - Setting next_dagrun for DEMO-DAG1 to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:53:30.118+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-18T09:53:33.085+0000] {processor.py:161} INFO - Started process (PID=44) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:33.086+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:53:33.088+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:33.087+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:33.098+0000] {processor.py:840} INFO - DAG(s) 'Stock' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:33.155+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:33.155+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:Stock
[2024-01-18T09:53:33.159+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:33.159+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:Stock
[2024-01-18T09:53:33.161+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:33.161+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:Stock
[2024-01-18T09:53:33.162+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:33.162+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:53:33.166+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:33.166+0000] {dag.py:3055} INFO - Creating ORM DAG for Stock
[2024-01-18T09:53:33.171+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:33.171+0000] {dag.py:3820} INFO - Setting next_dagrun for Stock to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:53:33.177+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.094 seconds
[2024-01-18T09:53:35.185+0000] {processor.py:161} INFO - Started process (PID=45) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:35.187+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:53:35.189+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:35.189+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:35.202+0000] {processor.py:840} INFO - DAG(s) 'S' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:35.254+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:35.254+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:S
[2024-01-18T09:53:35.257+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:35.257+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:S
[2024-01-18T09:53:35.259+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:35.259+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:S
[2024-01-18T09:53:35.260+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:35.260+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:53:35.264+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:35.264+0000] {dag.py:3055} INFO - Creating ORM DAG for S
[2024-01-18T09:53:35.269+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:35.269+0000] {dag.py:3820} INFO - Setting next_dagrun for S to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:53:35.278+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.096 seconds
[2024-01-18T09:53:36.229+0000] {processor.py:161} INFO - Started process (PID=46) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:36.231+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:53:36.233+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:36.232+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:36.249+0000] {processor.py:840} INFO - DAG(s) 'Comp' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:36.306+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:36.306+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:Comp
[2024-01-18T09:53:36.310+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:36.310+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:Comp
[2024-01-18T09:53:36.312+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:36.312+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:Comp
[2024-01-18T09:53:36.312+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:36.312+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:53:36.317+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:36.317+0000] {dag.py:3055} INFO - Creating ORM DAG for Comp
[2024-01-18T09:53:36.321+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:36.321+0000] {dag.py:3820} INFO - Setting next_dagrun for Comp to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:53:36.328+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.102 seconds
[2024-01-18T09:53:37.321+0000] {processor.py:161} INFO - Started process (PID=47) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:37.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:53:37.324+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:37.324+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:37.337+0000] {processor.py:840} INFO - DAG(s) 'Company' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:37.392+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:37.391+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:Company
[2024-01-18T09:53:37.396+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:37.396+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:Company
[2024-01-18T09:53:37.398+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:37.398+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:Company
[2024-01-18T09:53:37.398+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:37.398+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:53:37.404+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:37.404+0000] {dag.py:3055} INFO - Creating ORM DAG for Company
[2024-01-18T09:53:37.408+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:37.408+0000] {dag.py:3820} INFO - Setting next_dagrun for Company to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:53:37.415+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.099 seconds
[2024-01-18T09:53:38.366+0000] {processor.py:161} INFO - Started process (PID=48) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:38.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:53:38.369+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:38.368+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:38.383+0000] {processor.py:840} INFO - DAG(s) 'Company_pr' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:38.433+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:38.432+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:Company_pr
[2024-01-18T09:53:38.437+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:38.437+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:Company_pr
[2024-01-18T09:53:38.441+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:38.441+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:Company_pr
[2024-01-18T09:53:38.442+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:38.442+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:53:38.447+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:38.447+0000] {dag.py:3055} INFO - Creating ORM DAG for Company_pr
[2024-01-18T09:53:38.452+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:38.451+0000] {dag.py:3820} INFO - Setting next_dagrun for Company_pr to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:53:38.459+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.097 seconds
[2024-01-18T09:53:39.451+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:39.453+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:53:39.455+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:39.454+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:39.470+0000] {processor.py:840} INFO - DAG(s) 'Company_profi' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:39.532+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:39.532+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:Company_profi
[2024-01-18T09:53:39.536+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:39.536+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:Company_profi
[2024-01-18T09:53:39.538+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:39.538+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:Company_profi
[2024-01-18T09:53:39.538+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:39.538+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:53:39.543+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:39.543+0000] {dag.py:3055} INFO - Creating ORM DAG for Company_profi
[2024-01-18T09:53:39.548+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:39.548+0000] {dag.py:3820} INFO - Setting next_dagrun for Company_profi to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:53:39.557+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.111 seconds
[2024-01-18T09:53:40.509+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:40.510+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:53:40.512+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:40.512+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:40.530+0000] {processor.py:840} INFO - DAG(s) 'Company_profile_pi' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:40.587+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:40.587+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:Company_profile_pi
[2024-01-18T09:53:40.592+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:40.592+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:Company_profile_pi
[2024-01-18T09:53:40.595+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:40.595+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:Company_profile_pi
[2024-01-18T09:53:40.595+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:40.595+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:53:40.600+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:40.600+0000] {dag.py:3055} INFO - Creating ORM DAG for Company_profile_pi
[2024-01-18T09:53:40.604+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:40.604+0000] {dag.py:3820} INFO - Setting next_dagrun for Company_profile_pi to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:53:40.611+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.107 seconds
[2024-01-18T09:53:41.609+0000] {processor.py:161} INFO - Started process (PID=51) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:41.610+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:53:41.612+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:41.612+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:41.631+0000] {processor.py:840} INFO - DAG(s) 'Company_profile_pipe' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:41.691+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:41.690+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:Company_profile_pipe
[2024-01-18T09:53:41.695+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:41.695+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:Company_profile_pipe
[2024-01-18T09:53:41.698+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:41.698+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:Company_profile_pipe
[2024-01-18T09:53:41.698+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:41.698+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:53:41.703+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:41.703+0000] {dag.py:3055} INFO - Creating ORM DAG for Company_profile_pipe
[2024-01-18T09:53:41.708+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:41.708+0000] {dag.py:3820} INFO - Setting next_dagrun for Company_profile_pipe to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:53:41.715+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.111 seconds
[2024-01-18T09:53:42.642+0000] {processor.py:161} INFO - Started process (PID=52) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:42.643+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:53:42.644+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:42.644+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:42.655+0000] {processor.py:840} INFO - DAG(s) 'Company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:42.706+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:42.705+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:Company_profile_pipeline
[2024-01-18T09:53:42.710+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:42.710+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:Company_profile_pipeline
[2024-01-18T09:53:42.712+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:42.712+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:Company_profile_pipeline
[2024-01-18T09:53:42.712+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:42.712+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:53:42.717+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:42.717+0000] {dag.py:3055} INFO - Creating ORM DAG for Company_profile_pipeline
[2024-01-18T09:53:42.722+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:42.722+0000] {dag.py:3820} INFO - Setting next_dagrun for Company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:53:42.729+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.089 seconds
[2024-01-18T09:53:44.754+0000] {processor.py:161} INFO - Started process (PID=53) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:44.755+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:53:44.756+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:44.756+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:44.767+0000] {processor.py:840} INFO - DAG(s) 'ompany_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:44.818+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:44.818+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:ompany_profile_pipeline
[2024-01-18T09:53:44.822+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:44.822+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:ompany_profile_pipeline
[2024-01-18T09:53:44.825+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:44.825+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:ompany_profile_pipeline
[2024-01-18T09:53:44.825+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:44.825+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:53:44.830+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:44.830+0000] {dag.py:3055} INFO - Creating ORM DAG for ompany_profile_pipeline
[2024-01-18T09:53:44.835+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:44.835+0000] {dag.py:3820} INFO - Setting next_dagrun for ompany_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:53:44.842+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.090 seconds
[2024-01-18T09:53:45.772+0000] {processor.py:161} INFO - Started process (PID=54) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:45.773+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:53:45.775+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:45.774+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:45.789+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:53:45.843+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:45.843+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:company_profile_pipeline
[2024-01-18T09:53:45.848+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:45.848+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:company_profile_pipeline
[2024-01-18T09:53:45.850+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:45.850+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:company_profile_pipeline
[2024-01-18T09:53:45.850+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:45.850+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:53:45.856+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:45.856+0000] {dag.py:3055} INFO - Creating ORM DAG for company_profile_pipeline
[2024-01-18T09:53:45.861+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:53:45.861+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:53:45.868+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.100 seconds
[2024-01-18T09:54:08.077+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:54:08.079+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:54:08.082+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:54:08.082+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:54:08.100+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:54:08.111+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:54:08.111+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:54:08.125+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:54:08.125+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:54:08.137+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-18T09:55:26.469+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:55:26.470+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:55:26.472+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:55:26.472+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:55:26.509+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:55:26.588+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:55:26.588+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:55:26.608+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:55:26.608+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:55:26.622+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.154 seconds
[2024-01-18T09:55:56.813+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:55:56.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:55:56.817+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:55:56.816+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:55:56.833+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:55:56.857+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:55:56.857+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:55:56.870+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:55:56.870+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:55:56.877+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.069 seconds
[2024-01-18T09:56:27.150+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:56:27.151+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:56:27.152+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:56:27.152+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:56:27.170+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:56:27.191+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:56:27.191+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:56:27.204+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:56:27.204+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:56:27.214+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-18T09:56:48.370+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:56:48.371+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:56:48.372+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:56:48.372+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:56:48.386+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:56:48.450+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:56:48.450+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:56:48.459+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:56:48.459+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:56:48.468+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.102 seconds
[2024-01-18T09:56:49.400+0000] {processor.py:161} INFO - Started process (PID=51) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:56:49.401+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:56:49.403+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:56:49.402+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:56:49.428+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:56:49.439+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:56:49.439+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:56:49.454+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:56:49.454+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T09:56:49.464+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-18T09:57:19.672+0000] {processor.py:161} INFO - Started process (PID=62) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:57:19.673+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:57:19.674+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:57:19.674+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:57:19.685+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:57:19.741+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:57:19.741+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:57:19.753+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:57:19.753+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T09:57:19.762+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.093 seconds
[2024-01-18T09:57:49.982+0000] {processor.py:161} INFO - Started process (PID=69) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:57:49.984+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:57:49.985+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:57:49.985+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:57:50.003+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:57:50.027+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:57:50.027+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:57:50.039+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:57:50.039+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T09:57:50.047+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-18T09:58:20.322+0000] {processor.py:161} INFO - Started process (PID=76) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:58:20.324+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:58:20.324+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:58:20.324+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:58:20.338+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:58:20.363+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:58:20.363+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:58:20.379+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:58:20.379+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T09:58:20.390+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-18T09:58:41.532+0000] {processor.py:161} INFO - Started process (PID=83) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:58:41.534+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:58:41.534+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:58:41.534+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:58:41.552+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:58:41.578+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:58:41.578+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:58:41.592+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:58:41.592+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T09:58:41.604+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.076 seconds
[2024-01-18T09:58:42.555+0000] {processor.py:161} INFO - Started process (PID=84) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:58:42.556+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:58:42.557+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:58:42.557+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:58:42.571+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:58:42.587+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:58:42.587+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:58:42.597+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:58:42.597+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T09:58:42.606+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.055 seconds
[2024-01-18T09:59:12.863+0000] {processor.py:161} INFO - Started process (PID=91) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:59:12.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:59:12.866+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:59:12.866+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:59:12.882+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:59:12.903+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:59:12.903+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:59:12.915+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:59:12.915+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T09:59:12.923+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-18T09:59:43.162+0000] {processor.py:161} INFO - Started process (PID=98) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:59:43.163+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:59:43.165+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:59:43.164+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:59:43.184+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:59:43.210+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:59:43.209+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:59:43.221+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:59:43.221+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T09:59:43.229+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-18T09:59:55.304+0000] {processor.py:161} INFO - Started process (PID=105) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:59:55.306+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T09:59:55.307+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:59:55.306+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:59:55.319+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T09:59:55.338+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:59:55.338+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T09:59:55.351+0000] {logging_mixin.py:188} INFO - [2024-01-18T09:59:55.351+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T09:59:55.361+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.061 seconds
[2024-01-18T10:00:25.676+0000] {processor.py:161} INFO - Started process (PID=112) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:00:25.677+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:00:25.678+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:00:25.678+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:00:25.698+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:00:25.722+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:00:25.722+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:00:25.735+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:00:25.735+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:00:25.744+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-18T10:00:56.012+0000] {processor.py:161} INFO - Started process (PID=119) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:00:56.013+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:00:56.013+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:00:56.013+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:00:56.024+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:00:56.038+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:00:56.038+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:00:56.049+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:00:56.049+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-18T10:00:56.057+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.048 seconds
[2024-01-18T10:01:26.321+0000] {processor.py:161} INFO - Started process (PID=126) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:01:26.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:01:26.324+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:01:26.323+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:01:26.341+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:01:26.367+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:01:26.367+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:01:26.380+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:01:26.380+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:01:26.389+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-18T10:01:56.695+0000] {processor.py:161} INFO - Started process (PID=133) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:01:56.696+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:01:56.697+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:01:56.697+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:01:56.714+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:01:56.739+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:01:56.739+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:01:56.754+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:01:56.754+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:01:56.764+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-18T10:02:27.030+0000] {processor.py:161} INFO - Started process (PID=140) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:02:27.031+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:02:27.032+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:02:27.031+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:02:27.049+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:02:27.069+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:02:27.069+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:02:27.080+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:02:27.080+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:02:27.087+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-18T10:02:57.338+0000] {processor.py:161} INFO - Started process (PID=147) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:02:57.339+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:02:57.340+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:02:57.339+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:02:57.355+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:02:57.376+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:02:57.376+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:02:57.388+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:02:57.387+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:02:57.395+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-18T10:03:27.686+0000] {processor.py:161} INFO - Started process (PID=154) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:03:27.687+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:03:27.688+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:03:27.688+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:03:27.699+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:03:27.717+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:03:27.717+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:03:27.730+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:03:27.730+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:03:27.738+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.054 seconds
[2024-01-18T10:03:57.995+0000] {processor.py:161} INFO - Started process (PID=161) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:03:57.996+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:03:57.997+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:03:57.996+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:03:58.009+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:03:58.025+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:03:58.025+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:03:58.037+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:03:58.037+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:03:58.046+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.053 seconds
[2024-01-18T10:04:26.256+0000] {processor.py:161} INFO - Started process (PID=168) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:26.258+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:04:26.259+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:26.259+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:26.279+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:26.301+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:26.301+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:04:26.312+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:26.312+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:04:26.323+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-18T10:04:33.341+0000] {processor.py:161} INFO - Started process (PID=169) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:33.342+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:04:33.342+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:33.342+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:33.351+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:33.350+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 258, in <module>
    A
NameError: name 'A' is not defined
[2024-01-18T10:04:33.352+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:33.365+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.029 seconds
[2024-01-18T10:04:34.403+0000] {processor.py:161} INFO - Started process (PID=170) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:34.405+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:04:34.406+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:34.406+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:34.413+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:34.412+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 839, in exec_module
  File "<frozen importlib._bootstrap_external>", line 976, in get_code
  File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 258
    A =
       ^
SyntaxError: invalid syntax
[2024-01-18T10:04:34.413+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:34.432+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.034 seconds
[2024-01-18T10:04:35.433+0000] {processor.py:161} INFO - Started process (PID=171) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:35.434+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:04:35.434+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:35.434+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:35.447+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:35.446+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 258, in <module>
    A = ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 27, in ConnectPostgres
    params = ReadConfigFile(relative_path,section)
  File "/opt/airflow/dags/GetAuthInfoFuncton.py", line 25, in ReadConfigFile
    raise Exception('Section {section} not found in the {text_file_path} file'.format(section=section, text_file_path=text_file_path))
Exception: Section Postgresql not found in the /Users/anhho/Desktop/DataEngineer/CrawlStockData/ApacheAirflow/Authorization.ini file
[2024-01-18T10:04:35.447+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:35.458+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.030 seconds
[2024-01-18T10:04:36.484+0000] {processor.py:161} INFO - Started process (PID=172) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:36.485+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:04:36.486+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:36.486+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:36.502+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:36.501+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 258, in <module>
    A = ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 27, in ConnectPostgres
    params = ReadConfigFile(relative_path,section)
  File "/opt/airflow/dags/GetAuthInfoFuncton.py", line 25, in ReadConfigFile
    raise Exception('Section {section} not found in the {text_file_path} file'.format(section=section, text_file_path=text_file_path))
Exception: Section Postgresql not found in the /Users/anhho/Desktop/DataEngineer/CrawlStockData/ApacheAirflow/Authorization.ini file
[2024-01-18T10:04:36.503+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:36.516+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.036 seconds
[2024-01-18T10:04:37.514+0000] {processor.py:161} INFO - Started process (PID=173) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:37.515+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:04:37.516+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:37.516+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:37.532+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:37.530+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 258, in <module>
    A = ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 27, in ConnectPostgres
    params = ReadConfigFile(relative_path,section)
  File "/opt/airflow/dags/GetAuthInfoFuncton.py", line 25, in ReadConfigFile
    raise Exception('Section {section} not found in the {text_file_path} file'.format(section=section, text_file_path=text_file_path))
Exception: Section Postgresql not found in the /Users/anhho/Desktop/DataEngineer/CrawlStockData/ApacheAirflow/Authorization.ini file
[2024-01-18T10:04:37.532+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:37.546+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.038 seconds
[2024-01-18T10:04:54.704+0000] {processor.py:161} INFO - Started process (PID=180) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:54.705+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:04:54.706+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:54.706+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:54.725+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:54.749+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:54.749+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:04:54.759+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:54.759+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:04:54.768+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-18T10:04:55.735+0000] {processor.py:161} INFO - Started process (PID=181) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:55.737+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:04:55.738+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:55.737+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:55.755+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:04:55.780+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:55.780+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:04:55.794+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:04:55.794+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:04:55.805+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.075 seconds
[2024-01-18T10:05:26.060+0000] {processor.py:161} INFO - Started process (PID=188) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:05:26.061+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:05:26.061+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:05:26.061+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:05:26.074+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:05:26.095+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:05:26.095+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:05:26.112+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:05:26.112+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:05:26.124+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-18T10:05:56.386+0000] {processor.py:161} INFO - Started process (PID=195) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:05:56.388+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:05:56.389+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:05:56.389+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:05:56.406+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:05:56.426+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:05:56.426+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:05:56.438+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:05:56.438+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:05:56.446+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-18T10:06:13.583+0000] {processor.py:161} INFO - Started process (PID=196) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:06:13.583+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:06:13.584+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:06:13.584+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:06:13.595+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:06:13.613+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:06:13.613+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:06:13.626+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:06:13.626+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:06:13.637+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.056 seconds
[2024-01-18T10:06:15.624+0000] {processor.py:161} INFO - Started process (PID=197) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:06:15.626+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:06:15.627+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:06:15.627+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:06:15.646+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:06:15.667+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:06:15.667+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:06:15.678+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:06:15.678+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:06:15.686+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-18T10:06:45.944+0000] {processor.py:161} INFO - Started process (PID=204) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:06:45.945+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:06:45.945+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:06:45.945+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:06:45.957+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:06:45.978+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:06:45.978+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:06:45.994+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:06:45.994+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:06:46.004+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-18T10:07:16.214+0000] {processor.py:161} INFO - Started process (PID=211) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:07:16.215+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:07:16.216+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:07:16.216+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:07:16.233+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:07:16.253+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:07:16.252+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:07:16.263+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:07:16.263+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:07:16.271+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.061 seconds
[2024-01-18T10:07:46.532+0000] {processor.py:161} INFO - Started process (PID=218) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:07:46.533+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:07:46.533+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:07:46.533+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:07:46.544+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:07:46.560+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:07:46.560+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:07:46.571+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:07:46.571+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:07:46.578+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.048 seconds
[2024-01-18T10:08:16.838+0000] {processor.py:161} INFO - Started process (PID=225) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:08:16.840+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:08:16.841+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:08:16.840+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:08:16.859+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:08:16.881+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:08:16.881+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:08:16.892+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:08:16.892+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:08:16.899+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-18T10:08:47.113+0000] {processor.py:161} INFO - Started process (PID=231) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:08:47.115+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:08:47.116+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:08:47.115+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:08:47.136+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:08:47.156+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:08:47.156+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:08:47.167+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:08:47.167+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:08:47.175+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-18T10:09:17.386+0000] {processor.py:161} INFO - Started process (PID=238) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:09:17.387+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:09:17.389+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:09:17.388+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:09:17.408+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:09:17.434+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:09:17.434+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:09:17.446+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:09:17.446+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:09:17.454+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-18T10:09:47.760+0000] {processor.py:161} INFO - Started process (PID=245) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:09:47.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:09:47.763+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:09:47.763+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:09:47.783+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:09:47.808+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:09:47.808+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:09:47.821+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:09:47.821+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:09:47.829+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-18T10:10:18.080+0000] {processor.py:161} INFO - Started process (PID=252) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:10:18.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:10:18.083+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:10:18.082+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:10:18.100+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:10:18.119+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:10:18.119+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:10:18.131+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:10:18.131+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:10:18.141+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-18T10:10:48.440+0000] {processor.py:161} INFO - Started process (PID=259) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:10:48.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:10:48.443+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:10:48.442+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:10:48.460+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:10:48.481+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:10:48.481+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:10:48.493+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:10:48.492+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:10:48.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-18T10:11:18.770+0000] {processor.py:161} INFO - Started process (PID=266) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:11:18.772+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:11:18.772+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:11:18.772+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:11:18.788+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:11:18.808+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:11:18.808+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:11:18.819+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:11:18.819+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:11:18.826+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.060 seconds
[2024-01-18T10:11:48.920+0000] {processor.py:161} INFO - Started process (PID=273) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:11:48.921+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:11:48.922+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:11:48.922+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:11:48.937+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:11:48.956+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:11:48.956+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:11:48.968+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:11:48.968+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:11:48.975+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.060 seconds
[2024-01-18T10:12:19.274+0000] {processor.py:161} INFO - Started process (PID=280) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:12:19.276+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:12:19.277+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:12:19.276+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:12:19.294+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:12:19.316+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:12:19.316+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:12:19.327+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:12:19.327+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:12:19.334+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-18T10:12:49.608+0000] {processor.py:161} INFO - Started process (PID=287) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:12:49.609+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:12:49.610+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:12:49.610+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:12:49.624+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:12:49.643+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:12:49.642+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:12:49.655+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:12:49.655+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:12:49.662+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.058 seconds
[2024-01-18T10:13:16.899+0000] {processor.py:161} INFO - Started process (PID=294) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:16.900+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:13:16.901+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:16.901+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:16.919+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:16.942+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:16.942+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:13:16.956+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:16.956+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:13:16.966+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-18T10:13:19.014+0000] {processor.py:161} INFO - Started process (PID=295) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:19.015+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:13:19.016+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:19.016+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:19.034+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:19.055+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:19.055+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:13:19.068+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:19.068+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:13:19.077+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-18T10:13:21.121+0000] {processor.py:161} INFO - Started process (PID=296) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:21.122+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:13:21.123+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:21.123+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:21.141+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:21.162+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:21.162+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:13:21.175+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:21.175+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:13:21.184+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-18T10:13:29.188+0000] {processor.py:161} INFO - Started process (PID=303) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:29.189+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:13:29.189+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:29.189+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:29.201+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:29.217+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:29.217+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:13:29.228+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:29.228+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:13:29.238+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.051 seconds
[2024-01-18T10:13:55.472+0000] {processor.py:161} INFO - Started process (PID=310) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:55.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:13:55.476+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:55.475+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:55.494+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:55.517+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:55.517+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:13:55.530+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:55.530+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:13:55.539+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-18T10:13:56.561+0000] {processor.py:161} INFO - Started process (PID=311) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:56.562+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:13:56.563+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:56.562+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:56.579+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:56.600+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:56.600+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:13:56.612+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:56.612+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:13:56.621+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-18T10:13:57.646+0000] {processor.py:161} INFO - Started process (PID=312) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:57.646+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:13:57.647+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:57.647+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:57.658+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:57.678+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:57.678+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:13:57.690+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:57.690+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:13:57.701+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.058 seconds
[2024-01-18T10:13:59.672+0000] {processor.py:161} INFO - Started process (PID=313) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:59.673+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:13:59.674+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:59.674+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:59.688+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:13:59.711+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:59.711+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:13:59.723+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:13:59.723+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:13:59.734+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-18T10:14:01.758+0000] {processor.py:161} INFO - Started process (PID=314) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:14:01.760+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:14:01.761+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:14:01.761+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:14:01.784+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:14:01.809+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:14:01.809+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:14:01.822+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:14:01.822+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:14:01.834+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-18T10:14:02.888+0000] {processor.py:161} INFO - Started process (PID=315) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:14:02.889+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:14:02.891+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:14:02.891+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:14:02.907+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:14:02.927+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:14:02.927+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:14:02.939+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:14:02.939+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:14:02.949+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-18T10:14:03.975+0000] {processor.py:161} INFO - Started process (PID=316) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:14:03.976+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:14:03.977+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:14:03.977+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:14:03.994+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:14:04.015+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:14:04.014+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:14:04.026+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:14:04.026+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:14:04.035+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-18T10:14:34.264+0000] {processor.py:161} INFO - Started process (PID=323) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:14:34.265+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:14:34.266+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:14:34.266+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:14:34.282+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:14:34.303+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:14:34.303+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:14:34.315+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:14:34.314+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:14:34.322+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-18T10:15:04.578+0000] {processor.py:161} INFO - Started process (PID=330) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:15:04.580+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:15:04.581+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:15:04.581+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:15:04.599+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:15:04.619+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:15:04.619+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:15:04.630+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:15:04.630+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:15:04.638+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-18T10:15:34.910+0000] {processor.py:161} INFO - Started process (PID=337) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:15:34.912+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:15:34.913+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:15:34.913+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:15:34.929+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:15:34.953+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:15:34.953+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:15:34.964+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:15:34.964+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:15:34.972+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-18T10:15:53.150+0000] {processor.py:161} INFO - Started process (PID=338) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:15:53.151+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:15:53.152+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:15:53.152+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:15:53.171+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:15:53.193+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:15:53.193+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:15:53.206+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:15:53.206+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:15:53.216+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-18T10:15:58.280+0000] {processor.py:161} INFO - Started process (PID=345) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:15:58.282+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:15:58.282+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:15:58.282+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:15:58.298+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-18T10:15:58.300+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:15:58.299+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-18T10:15:58.300+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:15:58.314+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.039 seconds
[2024-01-18T10:16:28.631+0000] {processor.py:161} INFO - Started process (PID=352) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:16:28.632+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:16:28.633+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:16:28.633+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:16:28.647+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-18T10:16:28.650+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:16:28.649+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-18T10:16:28.650+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:16:28.667+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.041 seconds
[2024-01-18T10:16:58.920+0000] {processor.py:161} INFO - Started process (PID=359) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:16:58.922+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:16:58.923+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:16:58.923+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:16:58.934+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-18T10:16:58.937+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:16:58.935+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-18T10:16:58.937+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:16:58.953+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.036 seconds
[2024-01-18T10:17:29.207+0000] {processor.py:161} INFO - Started process (PID=366) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:17:29.207+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:17:29.208+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:17:29.208+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:17:29.217+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-18T10:17:29.219+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:17:29.218+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-18T10:17:29.219+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:17:29.232+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.029 seconds
[2024-01-18T10:17:59.538+0000] {processor.py:161} INFO - Started process (PID=373) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:17:59.538+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:17:59.539+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:17:59.539+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:17:59.547+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-18T10:17:59.549+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:17:59.548+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-18T10:17:59.550+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:17:59.562+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.027 seconds
[2024-01-18T10:18:29.883+0000] {processor.py:161} INFO - Started process (PID=380) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:18:29.884+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:18:29.885+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:18:29.885+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:18:29.896+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-18T10:18:29.898+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:18:29.897+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-18T10:18:29.898+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:18:29.913+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.033 seconds
[2024-01-18T10:19:00.198+0000] {processor.py:161} INFO - Started process (PID=387) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:19:00.199+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:19:00.201+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:19:00.200+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:19:00.216+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-18T10:19:00.218+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:19:00.217+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-18T10:19:00.219+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:19:00.233+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.041 seconds
[2024-01-18T10:19:30.524+0000] {processor.py:161} INFO - Started process (PID=394) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:19:30.526+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:19:30.527+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:19:30.527+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:19:30.542+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-18T10:19:30.544+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:19:30.543+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-18T10:19:30.545+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:19:30.560+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.042 seconds
[2024-01-18T10:20:00.811+0000] {processor.py:161} INFO - Started process (PID=401) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:20:00.812+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:20:00.813+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:20:00.812+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:20:00.823+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-18T10:20:00.825+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:20:00.824+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-18T10:20:00.825+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:20:00.841+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.033 seconds
[2024-01-18T10:20:31.096+0000] {processor.py:161} INFO - Started process (PID=408) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:20:31.097+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:20:31.098+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:20:31.098+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:20:31.111+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-18T10:20:31.113+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:20:31.112+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-18T10:20:31.113+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:20:31.129+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.038 seconds
[2024-01-18T10:21:01.399+0000] {processor.py:161} INFO - Started process (PID=415) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:21:01.400+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:21:01.402+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:21:01.401+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:21:01.418+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-18T10:21:01.421+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:21:01.420+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-18T10:21:01.422+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:21:01.437+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.045 seconds
[2024-01-18T10:21:31.723+0000] {processor.py:161} INFO - Started process (PID=422) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:21:31.724+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:21:31.725+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:21:31.725+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:21:31.736+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-18T10:21:31.738+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:21:31.737+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-18T10:21:31.738+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:21:31.753+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.033 seconds
[2024-01-18T10:22:01.970+0000] {processor.py:161} INFO - Started process (PID=429) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:22:01.970+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:22:01.971+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:22:01.971+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:22:01.982+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-18T10:22:01.984+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:22:01.983+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-18T10:22:01.984+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:22:01.996+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.030 seconds
[2024-01-18T10:22:29.259+0000] {processor.py:161} INFO - Started process (PID=436) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:22:29.260+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:22:29.262+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:22:29.261+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:22:29.288+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:22:29.382+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:22:29.382+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:22:29.395+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:22:29.395+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:22:29.408+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.154 seconds
[2024-01-18T10:22:59.599+0000] {processor.py:161} INFO - Started process (PID=442) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:22:59.601+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:22:59.602+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:22:59.601+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:22:59.633+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:22:59.659+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:22:59.659+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:22:59.674+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:22:59.674+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:22:59.684+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.093 seconds
[2024-01-18T10:23:29.991+0000] {processor.py:161} INFO - Started process (PID=449) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:23:29.993+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:23:29.994+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:23:29.994+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:23:30.012+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:23:30.034+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:23:30.034+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:23:30.046+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:23:30.046+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:23:30.054+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.069 seconds
[2024-01-18T10:24:00.362+0000] {processor.py:161} INFO - Started process (PID=456) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:24:00.363+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:24:00.365+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:24:00.364+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:24:00.388+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:24:00.410+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:24:00.410+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:24:00.422+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:24:00.422+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:24:00.431+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.075 seconds
[2024-01-18T10:24:30.688+0000] {processor.py:161} INFO - Started process (PID=463) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:24:30.689+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:24:30.690+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:24:30.690+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:24:30.707+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:24:30.728+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:24:30.728+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:24:30.739+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:24:30.739+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:24:30.746+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-18T10:25:01.071+0000] {processor.py:161} INFO - Started process (PID=470) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:25:01.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:25:01.075+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:25:01.074+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:25:01.101+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:25:01.128+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:25:01.128+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:25:01.143+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:25:01.143+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:25:01.154+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.090 seconds
[2024-01-18T10:25:31.458+0000] {processor.py:161} INFO - Started process (PID=477) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:25:31.460+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:25:31.461+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:25:31.460+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:25:31.477+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:25:31.497+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:25:31.497+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:25:31.512+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:25:31.512+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:25:31.522+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-18T10:26:01.781+0000] {processor.py:161} INFO - Started process (PID=484) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:26:01.782+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:26:01.784+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:26:01.783+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:26:01.801+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:26:01.823+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:26:01.823+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:26:01.836+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:26:01.836+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:26:01.845+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.069 seconds
[2024-01-18T10:26:32.132+0000] {processor.py:161} INFO - Started process (PID=491) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:26:32.133+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:26:32.134+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:26:32.134+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:26:32.154+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:26:32.178+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:26:32.178+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:26:32.190+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:26:32.189+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:26:32.198+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-18T10:27:02.437+0000] {processor.py:161} INFO - Started process (PID=498) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:27:02.439+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:27:02.440+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:27:02.440+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:27:02.460+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:27:02.486+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:27:02.486+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:27:02.497+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:27:02.497+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:27:02.506+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.075 seconds
[2024-01-18T10:27:32.770+0000] {processor.py:161} INFO - Started process (PID=505) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:27:32.771+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:27:32.772+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:27:32.772+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:27:32.785+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:27:32.806+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:27:32.806+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:27:32.820+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:27:32.820+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:27:32.831+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-18T10:28:03.096+0000] {processor.py:161} INFO - Started process (PID=512) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:28:03.098+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T10:28:03.100+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:28:03.099+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:28:03.118+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T10:28:03.141+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:28:03.141+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T10:28:03.153+0000] {logging_mixin.py:188} INFO - [2024-01-18T10:28:03.153+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T10:28:03.161+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-18T11:05:44.283+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:05:44.284+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:05:44.286+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:05:44.286+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:05:44.315+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:05:44.404+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:05:44.404+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:05:44.417+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:05:44.417+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:05:44.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.148 seconds
[2024-01-18T11:06:14.624+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:06:14.625+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:06:14.628+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:06:14.628+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:06:14.644+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:06:14.662+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:06:14.662+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:06:14.675+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:06:14.674+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:06:14.682+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-18T11:06:44.949+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:06:44.950+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:06:44.955+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:06:44.955+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:06:44.976+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:06:45.004+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:06:45.004+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:06:45.016+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:06:45.016+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:06:45.024+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-18T11:07:15.237+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:07:15.238+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:07:15.240+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:07:15.239+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:07:15.252+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:07:15.273+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:07:15.273+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:07:15.286+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:07:15.286+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:07:15.296+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-18T11:07:45.511+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:07:45.512+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:07:45.515+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:07:45.514+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:07:45.530+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:07:45.551+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:07:45.551+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:07:45.565+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:07:45.565+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:07:45.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-18T11:13:30.913+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:13:30.914+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:13:30.916+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:13:30.916+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:13:30.927+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:13:31.007+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:13:31.007+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:13:31.023+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:13:31.023+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:13:31.032+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.123 seconds
[2024-01-18T11:14:01.210+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:14:01.212+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:14:01.216+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:14:01.216+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:14:01.236+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:14:01.255+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:14:01.255+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:14:01.267+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:14:01.267+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:14:01.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-18T11:14:31.572+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:14:31.573+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:14:31.575+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:14:31.575+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:14:31.591+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:14:31.613+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:14:31.612+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:14:31.625+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:14:31.625+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:14:31.637+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-18T11:15:01.852+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:15:01.853+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:15:01.855+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:15:01.855+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:15:01.869+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:15:01.888+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:15:01.888+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:15:01.898+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:15:01.898+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:15:01.905+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.058 seconds
[2024-01-18T11:15:32.135+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:15:32.136+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:15:32.138+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:15:32.137+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:15:32.149+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:15:32.165+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:15:32.165+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:15:32.176+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:15:32.176+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:15:32.183+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.051 seconds
[2024-01-18T11:16:02.430+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:16:02.430+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:16:02.433+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:16:02.433+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:16:02.446+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:16:02.465+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:16:02.465+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:16:02.481+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:16:02.480+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:16:02.490+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-18T11:16:32.736+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:16:32.736+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:16:32.738+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:16:32.737+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:16:32.745+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:16:32.759+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:16:32.759+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:16:32.770+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:16:32.770+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:16:32.777+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.043 seconds
[2024-01-18T11:17:02.977+0000] {processor.py:161} INFO - Started process (PID=85) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:17:02.977+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:17:02.979+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:17:02.979+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:17:02.988+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:17:03.005+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:17:03.005+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:17:03.017+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:17:03.017+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:17:03.026+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.051 seconds
[2024-01-18T11:17:33.217+0000] {processor.py:161} INFO - Started process (PID=92) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:17:33.218+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:17:33.221+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:17:33.221+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:17:33.235+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:17:33.255+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:17:33.255+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:17:33.269+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:17:33.269+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:17:33.278+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-18T11:18:03.448+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:18:03.449+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:18:03.451+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:18:03.451+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:18:03.465+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:18:03.484+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:18:03.484+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:18:03.497+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:18:03.497+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:18:03.505+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.060 seconds
[2024-01-18T11:18:33.749+0000] {processor.py:161} INFO - Started process (PID=106) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:18:33.750+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:18:33.753+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:18:33.752+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:18:33.766+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:18:33.785+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:18:33.785+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:18:33.797+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:18:33.797+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:18:33.804+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.058 seconds
[2024-01-18T11:19:04.007+0000] {processor.py:161} INFO - Started process (PID=113) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:19:04.008+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:19:04.012+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:19:04.011+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:19:04.028+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:19:04.048+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:19:04.047+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:19:04.060+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:19:04.060+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:19:04.068+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-18T11:19:34.302+0000] {processor.py:161} INFO - Started process (PID=120) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:19:34.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:19:34.305+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:19:34.305+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:19:34.318+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:19:34.335+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:19:34.335+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:19:34.347+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:19:34.347+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:19:34.355+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.057 seconds
[2024-01-18T11:23:16.519+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:23:16.520+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T11:23:16.522+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:23:16.522+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:23:16.532+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T11:23:16.620+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:23:16.620+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T11:23:16.657+0000] {logging_mixin.py:188} INFO - [2024-01-18T11:23:16.657+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T11:23:16.709+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.193 seconds
[2024-01-18T14:30:23.477+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:30:23.480+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T14:30:23.485+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:30:23.485+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:30:23.496+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:30:23.561+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:30:23.561+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T14:30:23.572+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:30:23.572+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T14:30:23.582+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.107 seconds
[2024-01-18T14:30:53.817+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:30:53.818+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T14:30:53.822+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:30:53.821+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:30:53.841+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:30:53.863+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:30:53.862+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T14:30:53.876+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:30:53.876+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T14:30:53.884+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-18T14:31:24.178+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:31:24.179+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T14:31:24.182+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:31:24.182+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:31:24.198+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:31:24.220+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:31:24.220+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T14:31:24.233+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:31:24.233+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T14:31:24.242+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-18T14:31:54.537+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:31:54.538+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T14:31:54.541+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:31:54.541+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:31:54.556+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:31:54.575+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:31:54.575+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T14:31:54.586+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:31:54.586+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T14:31:54.594+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.061 seconds
[2024-01-18T14:32:24.889+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:32:24.891+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T14:32:24.893+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:32:24.893+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:32:24.909+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:32:24.930+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:32:24.929+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T14:32:24.941+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:32:24.941+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T14:32:24.948+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-18T14:32:55.278+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:32:55.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T14:32:55.282+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:32:55.281+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:32:55.297+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:32:55.319+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:32:55.319+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T14:32:55.331+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:32:55.331+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T14:32:55.339+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-18T14:33:25.644+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:33:25.645+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T14:33:25.648+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:33:25.648+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:33:25.663+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:33:25.684+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:33:25.684+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T14:33:25.695+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:33:25.695+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T14:33:25.703+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-18T14:33:56.003+0000] {processor.py:161} INFO - Started process (PID=85) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:33:56.004+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T14:33:56.006+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:33:56.006+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:33:56.020+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:33:56.040+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:33:56.040+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T14:33:56.053+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:33:56.053+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T14:33:56.062+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-18T14:34:26.346+0000] {processor.py:161} INFO - Started process (PID=92) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:34:26.347+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T14:34:26.348+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:34:26.348+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:34:26.357+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:34:26.373+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:34:26.373+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T14:34:26.384+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:34:26.384+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T14:34:26.392+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.049 seconds
[2024-01-18T14:34:56.702+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:34:56.703+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T14:34:56.706+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:34:56.705+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:34:56.721+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:34:56.741+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:34:56.741+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T14:34:56.753+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:34:56.753+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T14:34:56.761+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-18T14:35:27.129+0000] {processor.py:161} INFO - Started process (PID=106) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:35:27.130+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T14:35:27.133+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:35:27.132+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:35:27.148+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:35:27.170+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:35:27.169+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T14:35:27.181+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:35:27.181+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T14:35:27.188+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-18T14:35:57.479+0000] {processor.py:161} INFO - Started process (PID=113) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:35:57.480+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-18T14:35:57.483+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:35:57.483+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:35:57.498+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-18T14:35:57.519+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:35:57.519+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-18T14:35:57.530+0000] {logging_mixin.py:188} INFO - [2024-01-18T14:35:57.530+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-18T14:35:57.538+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
