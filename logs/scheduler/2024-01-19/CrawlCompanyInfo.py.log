[2024-01-19T02:20:17.492+0000] {processor.py:161} INFO - Started process (PID=35) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:20:17.493+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:20:17.495+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:20:17.495+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:20:17.515+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:20:17.622+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:20:17.622+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:20:17.635+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:20:17.635+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-19T02:20:17.649+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.159 seconds
[2024-01-19T02:20:47.833+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:20:47.834+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:20:47.839+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:20:47.839+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:20:47.861+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:20:47.883+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:20:47.883+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:20:47.895+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:20:47.895+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:20:47.903+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T02:21:18.131+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:21:18.132+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:21:18.134+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:21:18.134+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:21:18.150+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:21:18.170+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:21:18.170+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:21:18.181+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:21:18.181+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:21:18.188+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T02:21:48.413+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:21:48.414+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:21:48.415+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:21:48.415+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:21:48.426+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:21:48.444+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:21:48.443+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:21:48.455+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:21:48.455+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:21:48.463+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.053 seconds
[2024-01-19T02:22:18.696+0000] {processor.py:161} INFO - Started process (PID=63) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:18.697+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:22:18.700+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:18.700+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:18.715+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:18.737+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:18.737+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:22:18.751+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:18.751+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:22:18.761+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.069 seconds
[2024-01-19T02:22:48.997+0000] {processor.py:161} INFO - Started process (PID=70) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:48.998+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:22:49.002+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:49.001+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:49.020+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:49.043+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:49.043+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:22:49.055+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:49.055+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:22:49.063+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T02:22:53.072+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:53.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:22:53.076+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:53.076+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:53.083+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:53.082+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 839, in exec_module
  File "<frozen importlib._bootstrap_external>", line 976, in get_code
  File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 22
    params = {'host': 'Authorization=Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IkdYdExONzViZlZQakdvNERWdjV4QkRITHpnSSIsImtpZCI6IkdYdExONzViZlZQakdvNERWdjV4QkRITHpnSSJ9.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmZpcmVhbnQudm4iLCJhdWQiOiJodHRwczovL2FjY291bnRzLmZpcmVhbnQudm4vcmVzb3VyY2VzIiwiZXhwIjoxODg5NjIyNTMwLCJuYmYiOjE1ODk2MjI1MzAsImNsaWVudF9pZCI6ImZpcmVhbnQudHJhZGVzdGF0aW9uIiwic2NvcGUiOlsiYWNhZGVteS1yZWFkIiwiYWNhZGVteS13cml0ZSIsImFjY291bnRzLXJlYWQiLCJhY2NvdW50cy13cml0ZSIsImJsb2ctcmVhZCIsImNvbXBhbmllcy1yZWFkIiwiZmluYW5jZS1yZWFkIiwiaW5kaXZpZHVhbHMtcmVhZCIsImludmVzdG9wZWRpYS1yZWFkIiwib3JkZXJzLXJlYWQiLCJvcmRlcnMtd3JpdGUiLCJwb3N0cy1yZWFkIiwicG9zdHMtd3JpdGUiLCJzZWFyY2giLCJzeW1ib2xzLXJlYWQiLCJ1c2VyLWRhdGEtcmVhZCIsInVzZXItZGF0YS13cml0ZSIsInVzZXJzLXJlYWQiXSwianRpIjoiMjYxYTZhYWQ2MTQ5Njk1ZmJiYzcwODM5MjM0Njc1NWQifQ.dA5-HVzWv-BRfEiAd24uNBiBxASO-PAyWeWESovZm_hj4aXMAZA1-bWNZeXt88dqogo18AwpDQ-h6gefLPdZSFrG5umC1dVWaeYvUnGm62g4XS29fj6p01dhKNNqrsu5KrhnhdnKYVv9VdmbmqDfWR8wDgglk5cJFqalzq6dJWJInFQEPmUs9BW_Zs8tQDn-i5r4tYq2U8vCdqptXoM7YgPllXaPVDeccC9QNu2Xlp9WUvoROzoQXg25lFub1IYkTrM66gJ6t9fJRZToewCt495WNEOQFa_rwLCZ1QwzvL0iYkONHS_jZ0BOhBCdW9dWSawD6iF1SIQaFROvMDH1rg
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^
SyntaxError: EOL while scanning string literal
[2024-01-19T02:22:53.084+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:53.103+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.035 seconds
[2024-01-19T02:22:54.138+0000] {processor.py:161} INFO - Started process (PID=72) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:54.139+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:22:54.141+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:54.141+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:54.159+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:54.185+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:54.185+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:22:54.199+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:54.199+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:22:54.209+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.075 seconds
[2024-01-19T02:22:57.200+0000] {processor.py:161} INFO - Started process (PID=73) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:57.202+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:22:57.205+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:57.205+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:57.224+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:57.246+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:57.245+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:22:57.257+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:57.257+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:22:57.268+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T02:23:11.425+0000] {processor.py:161} INFO - Started process (PID=74) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:11.426+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:23:11.429+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:11.429+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:11.452+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:11.469+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:11.469+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:23:11.480+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:11.480+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:23:11.491+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T02:23:12.458+0000] {processor.py:161} INFO - Started process (PID=75) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:12.460+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:23:12.464+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:12.464+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:12.482+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:12.503+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:12.503+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:23:12.516+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:12.515+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:23:12.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T02:23:42.863+0000] {processor.py:161} INFO - Started process (PID=82) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:42.866+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:23:42.870+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:42.870+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:42.894+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:42.918+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:42.918+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:23:42.933+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:42.933+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T02:23:42.941+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.084 seconds
[2024-01-19T02:24:13.122+0000] {processor.py:161} INFO - Started process (PID=95) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:24:13.123+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:24:13.126+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:24:13.125+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:24:13.142+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:24:13.165+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:24:13.165+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:24:13.187+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T02:24:43.360+0000] {processor.py:161} INFO - Started process (PID=102) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:24:43.361+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:24:43.363+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:24:43.363+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:24:43.374+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:24:43.391+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:24:43.391+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:24:43.435+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T02:25:13.626+0000] {processor.py:161} INFO - Started process (PID=109) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:13.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:25:13.632+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:25:13.631+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:13.653+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:13.672+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:25:13.672+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:25:13.691+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T02:25:43.965+0000] {processor.py:161} INFO - Started process (PID=116) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:43.967+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:25:43.972+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:25:43.972+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:43.995+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:44.022+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:25:44.022+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:25:44.040+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:25:44.040+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-16T00:00:00+00:00, run_after=2024-01-17T00:00:00+00:00
[2024-01-19T02:25:44.049+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.091 seconds
[2024-01-19T02:25:47.013+0000] {processor.py:161} INFO - Started process (PID=117) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:47.015+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:25:47.020+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:25:47.020+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:47.046+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:47.064+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:25:47.064+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:25:47.087+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.085 seconds
[2024-01-19T02:26:17.347+0000] {processor.py:161} INFO - Started process (PID=124) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:17.349+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:26:17.353+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:17.352+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:17.372+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:17.398+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:17.398+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:26:17.415+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:17.415+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:26:17.437+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.095 seconds
[2024-01-19T02:26:47.645+0000] {processor.py:161} INFO - Started process (PID=131) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:47.648+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:26:47.653+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:47.652+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:47.685+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:47.707+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:47.707+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:26:47.721+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:47.721+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:26:47.730+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.092 seconds
[2024-01-19T02:26:48.668+0000] {processor.py:161} INFO - Started process (PID=132) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:48.669+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:26:48.672+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:48.672+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:48.686+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:48.684+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    C
NameError: name 'C' is not defined
[2024-01-19T02:26:48.687+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:48.702+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.037 seconds
[2024-01-19T02:26:49.680+0000] {processor.py:161} INFO - Started process (PID=133) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:49.682+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:26:49.684+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:49.684+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:49.702+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:49.701+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    Co
NameError: name 'Co' is not defined
[2024-01-19T02:26:49.703+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:49.719+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.041 seconds
[2024-01-19T02:26:50.745+0000] {processor.py:161} INFO - Started process (PID=134) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:50.745+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:26:50.747+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:50.747+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:50.761+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:50.778+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:50.778+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:26:50.789+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:50.789+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:26:50.798+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.057 seconds
[2024-01-19T02:26:51.765+0000] {processor.py:161} INFO - Started process (PID=135) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:51.766+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:26:51.768+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:51.768+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:51.778+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:51.778+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    Conn
NameError: name 'Conn' is not defined
[2024-01-19T02:26:51.779+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:51.791+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.030 seconds
[2024-01-19T02:26:53.791+0000] {processor.py:161} INFO - Started process (PID=136) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:53.792+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:26:53.794+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:53.794+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:53.803+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:28:08.468+0000] {processor.py:161} INFO - Started process (PID=149) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:08.469+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:28:08.472+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:08.471+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:08.483+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:28:08.674+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:08.671+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres(
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "loca" to address: Name or service not known
[2024-01-19T02:28:08.676+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:08.699+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.234 seconds
[2024-01-19T02:28:09.487+0000] {processor.py:161} INFO - Started process (PID=150) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:09.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:28:09.490+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:09.490+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:09.500+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:28:09.502+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:09.501+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:28:09.502+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:09.515+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.032 seconds
[2024-01-19T02:28:22.648+0000] {processor.py:161} INFO - Started process (PID=157) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:22.649+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:28:22.650+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:22.649+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:22.661+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:28:22.663+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:22.663+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:28:22.664+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:22.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.044 seconds
[2024-01-19T02:28:31.748+0000] {processor.py:161} INFO - Started process (PID=158) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:31.749+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:28:31.750+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:31.750+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:31.758+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:28:31.759+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:31.758+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:28:31.759+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:31.773+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.028 seconds
[2024-01-19T02:28:32.773+0000] {processor.py:161} INFO - Started process (PID=159) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:32.774+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:28:32.775+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:32.774+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:32.781+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:28:32.783+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:32.782+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:28:32.783+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:32.798+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.030 seconds
[2024-01-19T02:29:03.083+0000] {processor.py:161} INFO - Started process (PID=166) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:03.084+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:29:03.085+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:03.084+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:03.092+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:29:03.095+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:03.093+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:29:03.095+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:03.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.033 seconds
[2024-01-19T02:29:28.353+0000] {processor.py:161} INFO - Started process (PID=173) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:28.354+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:29:28.355+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:28.355+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:28.364+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:29:28.367+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:28.365+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:29:28.367+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:28.387+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.038 seconds
[2024-01-19T02:29:39.484+0000] {processor.py:161} INFO - Started process (PID=174) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:39.486+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:29:39.487+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:39.487+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:39.507+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:29:39.510+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:39.509+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:29:39.511+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:39.528+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.049 seconds
[2024-01-19T02:29:40.527+0000] {processor.py:161} INFO - Started process (PID=175) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:40.529+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:29:40.531+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:40.530+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:40.546+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:29:40.676+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:40.674+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "localhos" to address: Name or service not known
[2024-01-19T02:29:40.677+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:40.698+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.177 seconds
[2024-01-19T02:29:43.603+0000] {processor.py:161} INFO - Started process (PID=176) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:43.604+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:29:43.605+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:43.605+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:43.627+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:43.710+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:43.710+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:29:43.720+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:43.720+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:29:43.732+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.134 seconds
[2024-01-19T02:29:45.784+0000] {processor.py:161} INFO - Started process (PID=182) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:45.785+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:29:45.786+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:45.785+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:45.800+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:45.810+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:45.810+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:29:45.827+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:45.827+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:29:45.839+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.058 seconds
[2024-01-19T02:29:46.821+0000] {processor.py:161} INFO - Started process (PID=183) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:46.823+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:29:46.824+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:46.824+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:46.851+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:46.863+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:46.863+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:29:46.882+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:46.882+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:29:46.893+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T02:30:17.191+0000] {processor.py:161} INFO - Started process (PID=190) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:30:17.193+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:30:17.194+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:30:17.194+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:30:17.213+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:30:17.237+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:30:17.237+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:30:17.251+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:30:17.251+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T02:30:17.260+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T02:30:47.481+0000] {processor.py:161} INFO - Started process (PID=197) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:30:47.482+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:30:47.483+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:30:47.483+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:30:47.502+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:30:47.530+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:30:47.530+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:30:47.597+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.119 seconds
[2024-01-19T02:31:17.774+0000] {processor.py:161} INFO - Started process (PID=203) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:17.776+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:31:17.777+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:31:17.777+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:17.795+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:17.820+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:31:17.820+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:31:17.841+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T02:31:39.021+0000] {processor.py:161} INFO - Started process (PID=204) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:39.023+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:31:39.025+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:31:39.024+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:39.064+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:31:39.062+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    co
NameError: name 'co' is not defined
[2024-01-19T02:31:39.064+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:39.083+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T02:31:40.037+0000] {processor.py:161} INFO - Started process (PID=205) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:40.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:31:40.039+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:31:40.039+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:40.049+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:31:40.049+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    Conn
NameError: name 'Conn' is not defined
[2024-01-19T02:31:40.050+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:40.062+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.028 seconds
[2024-01-19T02:31:41.051+0000] {processor.py:161} INFO - Started process (PID=206) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:41.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:31:41.053+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:31:41.053+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:41.064+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:49:56.583+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:49:56.596+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:49:56.613+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:49:56.613+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:49:56.657+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:49:56.746+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:49:56.746+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:49:56.771+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.194 seconds
[2024-01-19T02:50:06.727+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:06.727+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:50:06.729+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.729+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:06.741+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:06.756+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.756+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'manual__2024-01-19T02:30:00.254994+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': 'b00e7f29-30c0-42e7-853a-ba983e12f84b'}
[2024-01-19T02:50:06.765+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.765+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240119T023000, start_date=20240119T023000, end_date=20240119T025006
[2024-01-19T02:50:06.781+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:06.782+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.782+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:06.782+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.782+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:06.784+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.784+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:06.784+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.784+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:06.786+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.785+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:06.792+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting manual__2024-01-19T02:30:00.254994+00:00 [failed]> in state failed
[2024-01-19T02:50:06.794+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.794+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:50:06.809+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.086 seconds
[2024-01-19T02:50:16.852+0000] {processor.py:161} INFO - Started process (PID=38) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:16.853+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:50:16.854+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.854+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:16.863+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:16.877+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.877+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-03T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '3d67ae9c-8d8a-4710-8cc7-d1b68ccee797'}
[2024-01-19T02:50:16.884+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.884+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240103T000000, start_date=20240119T023017, end_date=20240119T025016
[2024-01-19T02:50:16.893+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.894+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.894+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.894+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.894+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.896+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.895+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.896+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.896+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.897+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.896+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.902+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-03T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.903+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.903+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-05T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '52f0e0ba-bd2f-4404-8fcf-ed00fb5d1c69'}
[2024-01-19T02:50:16.906+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.906+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240105T000000, start_date=20240119T023018, end_date=20240119T025016
[2024-01-19T02:50:16.910+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.910+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.910+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.910+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.910+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.911+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.911+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.912+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.912+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.912+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.912+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.914+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-05T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.914+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.914+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-07T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': 'c99e0115-5427-48d7-a2a5-2da8dad029df'}
[2024-01-19T02:50:16.917+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.917+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240107T000000, start_date=20240119T023018, end_date=20240119T025016
[2024-01-19T02:50:16.921+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.922+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.922+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.922+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.922+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.923+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.923+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.923+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.923+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.924+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.923+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.925+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-07T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.926+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.926+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-09T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '63a8113e-43ce-44eb-bee7-895d6bcdbd57'}
[2024-01-19T02:50:16.928+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.928+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240109T000000, start_date=20240119T023018, end_date=20240119T025016
[2024-01-19T02:50:16.932+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.933+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.932+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.933+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.933+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.934+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.934+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.934+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.934+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.935+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.934+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.936+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-09T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.937+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.937+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-11T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '4b15f2f8-9bd0-491d-9ceb-4cd6007978f3'}
[2024-01-19T02:50:16.940+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.940+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240111T000000, start_date=20240119T023018, end_date=20240119T025016
[2024-01-19T02:50:16.944+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.944+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.944+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.945+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.945+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.946+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.946+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.946+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.946+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.947+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.946+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.949+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-11T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.950+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.949+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-13T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '93f8370e-9033-4275-b477-661186dcda19'}
[2024-01-19T02:50:16.953+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.953+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240113T000000, start_date=20240119T023019, end_date=20240119T025016
[2024-01-19T02:50:16.958+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.958+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.958+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.958+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.958+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.959+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.959+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.960+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.960+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.960+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.960+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.962+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-13T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.963+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.963+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-15T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': 'eaff8bf1-5257-4ab2-b5e9-7f3a2e58b7bb'}
[2024-01-19T02:50:16.966+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.966+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240115T000000, start_date=20240119T023019, end_date=20240119T025016
[2024-01-19T02:50:16.970+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.971+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.971+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.971+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.971+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.972+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.972+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.972+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.972+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.973+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.972+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.974+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-15T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.975+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.975+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-04T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '259814e0-393a-432d-998e-c1c9829be9d0'}
[2024-01-19T02:50:16.978+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.978+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240104T000000, start_date=20240119T023018, end_date=20240119T025016
[2024-01-19T02:50:16.982+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.982+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.982+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.983+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.983+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.984+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.984+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.984+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.984+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.984+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.984+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.986+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-04T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.987+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.987+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-02T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '98e07506-903d-4948-a6cb-45b922ec77aa'}
[2024-01-19T02:50:16.990+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.990+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240102T000000, start_date=20240119T023017, end_date=20240119T025016
[2024-01-19T02:50:16.994+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.995+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.994+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.995+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.995+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.996+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.996+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.996+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.996+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.996+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.996+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.998+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-02T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.999+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.999+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-01T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': 'cafaaadd-c44f-492c-a56f-50c6f15a3c61'}
[2024-01-19T02:50:17.001+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.001+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240101T000000, start_date=20240119T023017, end_date=20240119T025017
[2024-01-19T02:50:17.005+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:17.006+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.006+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.006+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.006+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.007+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.007+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.007+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.007+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.008+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.007+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:17.010+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-01T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:17.011+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.011+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-06T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '89996970-45f9-49ff-bb79-803f3d35c546'}
[2024-01-19T02:50:17.014+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.014+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240106T000000, start_date=20240119T023018, end_date=20240119T025017
[2024-01-19T02:50:17.019+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:17.019+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.019+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.020+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.019+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.021+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.021+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.021+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.021+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.022+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.021+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:17.024+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-06T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:17.025+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.025+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-08T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '55634143-ecdb-4626-ab10-0808ea841df0'}
[2024-01-19T02:50:17.028+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.028+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240108T000000, start_date=20240119T023018, end_date=20240119T025017
[2024-01-19T02:50:17.033+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:17.033+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.033+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.033+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.033+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.035+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.035+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.035+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.035+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.035+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.035+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:17.037+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-08T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:17.038+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.038+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-10T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '27e729e2-8fc2-4467-bae5-8422051a8a7d'}
[2024-01-19T02:50:17.041+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.041+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240110T000000, start_date=20240119T023018, end_date=20240119T025017
[2024-01-19T02:50:17.045+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:17.046+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.046+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.046+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.046+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.047+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.047+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.047+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.047+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.048+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.047+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:17.049+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-10T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:17.050+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.050+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-12T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '0592aee0-724f-4494-80e6-ab3694bafbd7'}
[2024-01-19T02:50:17.053+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.053+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240112T000000, start_date=20240119T023019, end_date=20240119T025017
[2024-01-19T02:50:17.057+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:17.058+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.058+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.058+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.058+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.059+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.059+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.059+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.059+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.060+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.059+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:17.062+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-12T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:17.063+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.063+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-14T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '27eefbb1-6501-4847-adb3-b5044f44280a'}
[2024-01-19T02:50:17.066+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.066+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240114T000000, start_date=20240119T023019, end_date=20240119T025017
[2024-01-19T02:50:17.070+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:17.070+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.070+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.071+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.071+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.072+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.072+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.072+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.072+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.073+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.072+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:17.075+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-14T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:17.076+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.076+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-03T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '3d67ae9c-8d8a-4710-8cc7-d1b68ccee797'}
[2024-01-19T02:50:17.078+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.078+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240103T000000, start_date=20240119T023017, end_date=20240119T025017
[2024-01-19T02:50:17.083+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:17.083+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.083+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.083+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.083+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.085+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.085+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.085+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.085+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.085+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.085+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:17.087+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-03T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:17.089+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.089+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:50:17.097+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.097+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-16T00:00:00+00:00, run_after=2024-01-17T00:00:00+00:00
[2024-01-19T02:50:17.104+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.255 seconds
[2024-01-19T02:50:47.328+0000] {processor.py:161} INFO - Started process (PID=45) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:47.331+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:50:47.337+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:47.336+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:47.361+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:47.401+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:47.401+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:50:47.412+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:47.412+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T02:50:47.424+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.102 seconds
[2024-01-19T02:51:17.654+0000] {processor.py:161} INFO - Started process (PID=52) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:51:17.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:51:17.659+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:51:17.658+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:51:17.678+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:51:17.699+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:51:17.699+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:51:17.712+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:51:17.712+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T02:51:17.721+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T02:51:47.989+0000] {processor.py:161} INFO - Started process (PID=59) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:51:47.990+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:51:47.994+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:51:47.993+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:51:48.012+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:51:48.037+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:51:48.037+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:51:48.051+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:51:48.050+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T02:51:48.059+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.076 seconds
[2024-01-19T02:52:18.303+0000] {processor.py:161} INFO - Started process (PID=66) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:52:18.305+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:52:18.309+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:52:18.308+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:52:18.327+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:52:18.349+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:52:18.349+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:52:18.377+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-19T02:52:48.671+0000] {processor.py:161} INFO - Started process (PID=73) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:52:48.673+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:52:48.677+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:52:48.676+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:52:48.697+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:52:48.723+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:52:48.723+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:52:48.751+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.084 seconds
[2024-01-19T02:53:18.994+0000] {processor.py:161} INFO - Started process (PID=80) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:53:18.995+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:53:18.997+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:53:18.997+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:53:19.009+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:53:19.027+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:53:19.027+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:53:19.048+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.057 seconds
[2024-01-19T02:53:49.299+0000] {processor.py:161} INFO - Started process (PID=87) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:53:49.301+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:53:49.306+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:53:49.305+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:53:49.327+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:53:49.348+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:53:49.348+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:53:49.368+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T02:54:19.653+0000] {processor.py:161} INFO - Started process (PID=94) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:54:19.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:54:19.658+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:54:19.658+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:54:19.678+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:54:19.702+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:54:19.701+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:54:19.723+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T02:54:49.973+0000] {processor.py:161} INFO - Started process (PID=101) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:54:49.974+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:54:49.977+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:54:49.976+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:54:49.996+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:54:50.022+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:54:50.022+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:54:50.049+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.080 seconds
[2024-01-19T02:55:20.264+0000] {processor.py:161} INFO - Started process (PID=108) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:55:20.266+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:55:20.270+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:55:20.270+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:55:20.291+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:55:20.315+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:55:20.315+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:55:20.337+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T02:55:50.603+0000] {processor.py:161} INFO - Started process (PID=115) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:55:50.605+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:55:50.609+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:55:50.609+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:55:50.631+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:55:50.656+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:55:50.656+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:55:50.676+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.080 seconds
[2024-01-19T02:56:20.982+0000] {processor.py:161} INFO - Started process (PID=122) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:56:20.984+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:56:20.986+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:56:20.986+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:56:21.003+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:56:21.021+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:56:21.021+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:56:21.041+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-19T02:56:51.251+0000] {processor.py:161} INFO - Started process (PID=129) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:56:51.252+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:56:51.256+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:56:51.255+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:56:51.272+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:56:51.293+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:56:51.292+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:56:51.320+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T02:57:21.546+0000] {processor.py:161} INFO - Started process (PID=136) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:21.547+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:21.550+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:21.550+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:21.568+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:21.590+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:21.590+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:57:21.615+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T02:57:44.766+0000] {processor.py:161} INFO - Started process (PID=143) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:44.767+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:44.769+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:44.769+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:44.794+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:44.818+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:44.818+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:57:44.834+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:44.834+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T02:57:44.844+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T02:57:46.876+0000] {processor.py:161} INFO - Started process (PID=144) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:46.877+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:46.879+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:46.879+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:46.896+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:46.920+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:46.920+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:57:46.932+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:46.932+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T02:57:46.943+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T02:57:48.931+0000] {processor.py:161} INFO - Started process (PID=145) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:48.933+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:48.936+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:48.936+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:48.950+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:57:48.952+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:48.951+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:57:48.953+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:48.975+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.049 seconds
[2024-01-19T02:57:50.001+0000] {processor.py:161} INFO - Started process (PID=146) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:50.001+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:50.003+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:50.003+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:50.014+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:57:50.017+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:50.015+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("co")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:57:50.018+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:50.034+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.037 seconds
[2024-01-19T02:57:51.026+0000] {processor.py:161} INFO - Started process (PID=147) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:51.027+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:51.029+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:51.029+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:51.040+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:57:51.042+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:51.041+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:57:51.042+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:51.054+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.031 seconds
[2024-01-19T02:57:52.062+0000] {processor.py:161} INFO - Started process (PID=148) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:52.063+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:52.064+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:52.064+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:52.073+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:57:52.074+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:52.073+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_pre")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:57:52.074+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:52.087+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.027 seconds
[2024-01-19T02:57:53.080+0000] {processor.py:161} INFO - Started process (PID=155) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:53.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:53.082+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:53.082+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:53.092+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:57:53.097+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:53.095+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_prf")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:57:53.103+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:53.123+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.046 seconds
[2024-01-19T02:57:55.125+0000] {processor.py:161} INFO - Started process (PID=156) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:55.126+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:55.128+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:55.128+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:55.136+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:57:55.138+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:55.137+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_pr")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:57:55.138+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:55.149+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.027 seconds
[2024-01-19T02:57:56.154+0000] {processor.py:161} INFO - Started process (PID=157) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:56.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:56.157+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:56.156+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:56.166+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:57:56.167+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:56.166+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:57:56.167+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:56.179+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.028 seconds
[2024-01-19T02:59:23.996+0000] {processor.py:161} INFO - Started process (PID=35) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:59:23.998+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:59:24.000+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:59:24.000+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:59:24.012+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:59:24.013+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:59:24.012+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:59:24.014+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:59:24.028+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.036 seconds
[2024-01-19T02:59:54.190+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:59:54.192+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:59:54.196+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:59:54.196+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:59:54.212+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:59:54.214+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:59:54.213+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:59:54.215+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:59:54.228+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.043 seconds
[2024-01-19T03:00:24.487+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:24.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:00:24.488+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:24.488+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:24.498+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:00:24.500+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:24.499+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T03:00:24.500+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:24.512+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.028 seconds
[2024-01-19T03:00:54.761+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:54.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:00:54.763+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:54.763+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:54.776+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:00:54.778+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:54.777+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T03:00:54.778+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:54.794+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.037 seconds
[2024-01-19T03:00:55.784+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:55.785+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:00:55.786+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:55.786+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:55.791+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:55.791+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 839, in exec_module
  File "<frozen importlib._bootstrap_external>", line 976, in get_code
  File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257
    A CheckTableExisting("company_profile")
      ^
SyntaxError: invalid syntax
[2024-01-19T03:00:55.792+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:55.808+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.028 seconds
[2024-01-19T03:00:56.818+0000] {processor.py:161} INFO - Started process (PID=58) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:56.818+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:00:56.819+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:56.819+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:56.828+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:00:56.829+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:56.828+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    A = CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T03:00:56.829+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:56.843+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.027 seconds
[2024-01-19T03:00:57.835+0000] {processor.py:161} INFO - Started process (PID=59) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:57.836+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:00:57.836+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:57.836+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:57.846+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:00:57.847+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:57.846+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    A = CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T03:00:57.847+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:57.859+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.026 seconds
[2024-01-19T03:00:58.867+0000] {processor.py:161} INFO - Started process (PID=60) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:58.868+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:00:58.868+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:58.868+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:58.877+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:00:58.878+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:58.877+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    A = CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T03:00:58.878+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:58.889+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.024 seconds
[2024-01-19T03:00:59.888+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:59.889+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:00:59.890+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:59.889+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:59.900+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:00:59.901+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:59.900+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    A = CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T03:00:59.901+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:59.914+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.028 seconds
[2024-01-19T03:11:33.013+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:11:33.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:11:33.026+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:11:33.026+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:11:33.046+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:11:33.149+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:11:33.149+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:11:33.163+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:11:33.163+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T03:11:33.177+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.169 seconds
[2024-01-19T03:12:03.258+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:12:03.259+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:12:03.264+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:12:03.263+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:12:03.285+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:12:03.310+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:12:03.310+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:12:03.324+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:12:03.324+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T03:12:03.334+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T03:12:33.574+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:12:33.575+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:12:33.580+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:12:33.579+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:12:33.602+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:12:33.627+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:12:33.627+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:12:33.640+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:12:33.640+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T03:12:33.648+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T03:13:03.864+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:03.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:13:03.869+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:03.868+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:03.888+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:03.907+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:03.907+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:13:03.958+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.100 seconds
[2024-01-19T03:13:34.216+0000] {processor.py:161} INFO - Started process (PID=68) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:34.218+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:13:34.222+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:34.221+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:34.242+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:34.262+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:34.262+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:13:34.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T03:13:47.366+0000] {processor.py:161} INFO - Started process (PID=69) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:47.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:13:47.369+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:47.369+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:47.384+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:47.458+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:47.458+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:13:47.480+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.117 seconds
[2024-01-19T03:13:48.392+0000] {processor.py:161} INFO - Started process (PID=70) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:48.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:13:48.393+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:48.393+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:48.404+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:48.411+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:48.411+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:13:48.429+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.040 seconds
[2024-01-19T03:14:18.650+0000] {processor.py:161} INFO - Started process (PID=82) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:14:18.653+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:14:18.672+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:14:18.671+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:14:18.692+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:14:18.716+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:14:18.716+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:14:18.730+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:14:18.730+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T03:14:18.740+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.105 seconds
[2024-01-19T03:14:48.929+0000] {processor.py:161} INFO - Started process (PID=179) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:14:48.932+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:14:48.937+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:14:48.936+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:14:48.959+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:14:48.981+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:14:48.981+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:14:48.994+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:14:48.994+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:14:49.002+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-19T03:15:19.316+0000] {processor.py:161} INFO - Started process (PID=186) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:19.318+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:15:19.321+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:19.321+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:19.342+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:19.369+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:19.369+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:15:19.390+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:19.390+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:15:19.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.096 seconds
[2024-01-19T03:15:21.361+0000] {processor.py:161} INFO - Started process (PID=187) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:21.362+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:15:21.365+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:21.365+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:21.373+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:21.371+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 839, in exec_module
  File "<frozen importlib._bootstrap_external>", line 976, in get_code
  File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 250
    )
    ^
SyntaxError: unmatched ')'
[2024-01-19T03:15:21.373+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:21.399+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.044 seconds
[2024-01-19T03:15:28.436+0000] {processor.py:161} INFO - Started process (PID=194) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:28.437+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:15:28.439+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:28.439+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:28.458+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:28.534+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:28.534+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:15:28.546+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:28.545+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:15:28.556+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.124 seconds
[2024-01-19T03:15:29.489+0000] {processor.py:161} INFO - Started process (PID=195) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:29.489+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:15:29.491+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:29.490+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:29.501+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:29.510+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:29.510+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:15:29.521+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:29.521+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:15:29.529+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.043 seconds
[2024-01-19T03:15:59.812+0000] {processor.py:161} INFO - Started process (PID=202) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:59.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:15:59.817+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:59.817+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:59.835+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:59.875+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:59.875+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:15:59.891+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:59.891+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T03:15:59.901+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.094 seconds
[2024-01-19T03:16:30.151+0000] {processor.py:161} INFO - Started process (PID=209) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:16:30.153+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:16:30.158+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:16:30.157+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:16:30.195+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:16:30.235+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:16:30.235+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:16:30.253+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:16:30.253+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:16:30.266+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.122 seconds
[2024-01-19T03:17:00.538+0000] {processor.py:161} INFO - Started process (PID=216) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:17:00.540+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:17:00.544+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:17:00.544+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:17:00.565+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:17:00.588+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:17:00.588+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:17:00.602+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:17:00.602+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:17:00.613+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.087 seconds
[2024-01-19T03:17:30.907+0000] {processor.py:161} INFO - Started process (PID=223) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:17:30.909+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:17:30.915+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:17:30.914+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:17:30.934+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:17:30.958+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:17:30.957+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:17:30.977+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:17:30.977+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:17:30.985+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.086 seconds
[2024-01-19T03:18:01.228+0000] {processor.py:161} INFO - Started process (PID=230) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:18:01.230+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:18:01.233+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:18:01.232+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:18:01.247+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:18:01.268+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:18:01.268+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:18:01.282+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:18:01.282+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:18:01.292+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T03:18:31.572+0000] {processor.py:161} INFO - Started process (PID=237) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:18:31.574+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:18:31.579+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:18:31.579+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:18:31.606+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:18:31.638+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:18:31.638+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:18:31.653+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:18:31.653+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:18:31.662+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.095 seconds
[2024-01-19T03:19:01.963+0000] {processor.py:161} INFO - Started process (PID=244) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:19:01.966+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:19:01.972+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:19:01.971+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:19:01.988+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:19:02.010+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:19:02.010+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:19:02.025+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:19:02.025+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:19:02.034+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-19T03:19:32.302+0000] {processor.py:161} INFO - Started process (PID=251) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:19:32.304+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:19:32.308+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:19:32.307+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:19:32.326+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:19:32.348+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:19:32.348+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:19:32.360+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:19:32.360+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:19:32.368+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T03:20:02.690+0000] {processor.py:161} INFO - Started process (PID=258) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:20:02.692+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:20:02.697+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:20:02.697+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:20:02.718+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:20:02.742+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:20:02.741+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:20:02.754+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:20:02.754+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:20:02.763+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T03:20:33.023+0000] {processor.py:161} INFO - Started process (PID=265) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:20:33.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:20:33.028+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:20:33.028+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:20:33.046+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:20:33.066+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:20:33.066+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:20:33.079+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:20:33.079+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:20:33.088+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T03:21:03.358+0000] {processor.py:161} INFO - Started process (PID=272) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:21:03.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:21:03.364+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:21:03.363+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:21:03.382+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:21:03.408+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:21:03.408+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:21:03.422+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:21:03.422+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:21:03.431+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T03:21:33.726+0000] {processor.py:161} INFO - Started process (PID=279) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:21:33.728+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:21:33.730+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:21:33.730+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:21:33.744+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:21:33.764+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:21:33.764+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:21:33.776+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:21:33.776+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:21:33.784+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T03:22:04.093+0000] {processor.py:161} INFO - Started process (PID=286) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:22:04.094+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:22:04.100+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:22:04.099+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:22:04.115+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:22:04.136+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:22:04.136+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:22:04.148+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:22:04.148+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:22:04.159+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T03:22:34.433+0000] {processor.py:161} INFO - Started process (PID=293) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:22:34.434+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:22:34.436+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:22:34.435+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:22:34.448+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:22:34.464+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:22:34.464+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:22:34.474+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:22:34.474+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:22:34.484+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.055 seconds
[2024-01-19T03:23:04.786+0000] {processor.py:161} INFO - Started process (PID=300) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:23:04.788+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:23:04.791+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:23:04.791+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:23:04.802+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:23:04.821+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:23:04.821+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:23:04.836+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:23:04.836+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:23:04.847+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T03:23:35.103+0000] {processor.py:161} INFO - Started process (PID=306) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:23:35.105+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:23:35.110+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:23:35.109+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:23:35.139+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:23:35.160+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:23:35.160+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:23:35.179+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:23:35.179+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:23:35.191+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.095 seconds
[2024-01-19T03:24:05.450+0000] {processor.py:161} INFO - Started process (PID=313) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:24:05.451+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:24:05.454+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:24:05.454+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:24:05.472+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:24:05.497+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:24:05.496+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:24:05.508+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:24:05.508+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:24:05.518+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T03:24:35.769+0000] {processor.py:161} INFO - Started process (PID=320) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:24:35.772+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:24:35.778+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:24:35.777+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:24:35.798+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:24:35.821+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:24:35.821+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:24:35.835+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:24:35.835+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:24:35.843+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T03:25:06.143+0000] {processor.py:161} INFO - Started process (PID=327) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:25:06.144+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:25:06.148+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:25:06.147+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:25:06.164+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:25:06.187+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:25:06.187+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:25:06.199+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:25:06.199+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:25:06.208+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T03:25:36.472+0000] {processor.py:161} INFO - Started process (PID=334) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:25:36.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:25:36.479+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:25:36.479+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:25:36.502+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:25:36.521+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:25:36.521+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:25:36.546+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:25:36.546+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:25:36.554+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.089 seconds
[2024-01-19T03:35:26.398+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:35:26.400+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:35:26.404+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:35:26.404+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:35:26.429+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:35:26.519+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:35:26.519+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:35:26.534+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:35:26.534+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:35:26.549+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.154 seconds
[2024-01-19T03:35:56.729+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:35:56.731+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:35:56.734+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:35:56.734+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:35:56.751+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:35:56.771+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:35:56.771+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:35:56.783+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:35:56.783+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:35:56.791+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T03:36:27.034+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:36:27.035+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:36:27.040+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:36:27.040+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:36:27.062+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:36:27.087+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:36:27.087+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:36:27.100+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:36:27.100+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:36:27.109+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T03:36:57.394+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:36:57.395+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:36:57.399+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:36:57.399+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:36:57.419+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:36:57.441+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:36:57.441+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:36:57.453+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:36:57.453+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:36:57.460+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T03:37:27.792+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:37:27.793+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:37:27.796+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:37:27.795+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:37:27.821+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:37:27.842+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:37:27.842+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:37:27.856+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:37:27.855+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:37:27.864+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T03:37:58.089+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:37:58.090+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:37:58.095+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:37:58.094+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:37:58.111+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:37:58.136+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:37:58.136+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:37:58.153+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:37:58.153+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:37:58.162+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T03:38:28.442+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:38:28.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:38:28.445+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:38:28.445+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:38:28.461+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:38:28.485+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:38:28.485+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:38:28.504+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:38:28.504+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:38:28.514+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.075 seconds
[2024-01-19T03:38:58.798+0000] {processor.py:161} INFO - Started process (PID=85) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:38:58.799+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:38:58.802+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:38:58.801+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:38:58.816+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:38:58.839+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:38:58.839+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:38:58.854+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:38:58.854+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:38:58.864+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.069 seconds
[2024-01-19T03:39:29.175+0000] {processor.py:161} INFO - Started process (PID=92) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:39:29.176+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:39:29.180+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:39:29.179+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:39:29.197+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:39:29.223+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:39:29.223+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:39:29.251+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:39:29.251+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:39:29.262+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.094 seconds
[2024-01-19T03:39:59.528+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:39:59.531+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:39:59.542+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:39:59.541+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:39:59.563+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:39:59.589+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:39:59.588+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:39:59.604+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:39:59.604+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:39:59.613+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.096 seconds
[2024-01-19T03:40:23.796+0000] {processor.py:161} INFO - Started process (PID=106) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:40:23.798+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:40:23.800+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:40:23.800+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:40:23.817+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:40:23.841+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:40:23.841+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:40:23.853+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:40:23.853+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:40:23.863+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T03:46:36.605+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:46:36.605+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:46:36.607+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:46:36.607+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:46:36.616+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:46:36.631+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:46:36.634+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:46:36.634+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:46:36.638+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:46:36.749+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:46:36.749+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:46:36.763+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:46:36.763+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:46:36.778+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.176 seconds
[2024-01-19T03:47:01.859+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:47:01.861+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:47:01.863+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:47:01.863+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:47:01.874+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:47:01.880+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:47:01.882+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:47:01.882+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:47:01.886+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:47:01.894+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:47:01.894+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:47:01.908+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:47:01.908+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:47:01.919+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-19T03:47:32.219+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:47:32.220+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:47:32.225+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:47:32.224+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:47:32.239+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:47:32.250+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:47:32.256+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:47:32.256+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:47:32.270+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:47:32.290+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:47:32.290+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:47:32.305+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:47:32.305+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:47:32.314+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.101 seconds
[2024-01-19T03:48:02.554+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:48:02.555+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:48:02.559+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:48:02.559+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:48:02.573+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:48:02.586+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:48:02.588+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:48:02.588+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:48:02.592+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:48:02.613+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:48:02.612+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:48:02.625+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:48:02.625+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:48:02.634+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.086 seconds
[2024-01-19T03:48:32.903+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:48:32.904+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:48:32.907+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:48:32.907+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:48:32.917+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:48:32.926+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:48:32.927+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:48:32.928+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:48:32.932+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:48:32.955+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:48:32.955+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:48:32.970+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:48:32.970+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:48:32.980+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T03:49:03.195+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:49:03.196+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:49:03.200+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:49:03.199+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:49:03.213+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:49:03.222+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:49:03.224+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:49:03.224+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:49:03.229+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:49:03.257+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:49:03.256+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:49:03.274+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:49:03.274+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:49:03.284+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.092 seconds
[2024-01-19T03:49:33.515+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:49:33.517+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:49:33.521+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:49:33.521+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:49:33.538+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:49:33.550+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:49:33.551+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:49:33.552+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:49:33.556+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:49:33.577+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:49:33.577+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:49:33.592+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:49:33.592+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:49:33.602+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.094 seconds
[2024-01-19T03:50:03.863+0000] {processor.py:161} INFO - Started process (PID=85) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:03.864+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:50:03.865+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:03.865+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:03.872+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:50:03.879+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:50:03.880+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:50:03.881+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:50:03.883+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:03.899+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:03.899+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:50:03.911+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:03.911+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:50:03.919+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.060 seconds
[2024-01-19T03:50:34.209+0000] {processor.py:161} INFO - Started process (PID=91) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:34.212+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:50:34.218+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:34.217+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:34.238+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:50:34.250+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:50:34.252+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:50:34.252+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:50:34.257+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:34.278+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:34.278+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:50:34.303+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:34.303+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:50:34.313+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.112 seconds
[2024-01-19T03:50:35.228+0000] {processor.py:161} INFO - Started process (PID=92) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:35.229+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:50:35.231+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:35.231+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:35.242+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:50:35.305+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:35.303+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 256, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "p" to address: Name or service not known
[2024-01-19T03:50:35.306+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:35.326+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.104 seconds
[2024-01-19T03:50:36.366+0000] {processor.py:161} INFO - Started process (PID=93) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:36.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:50:36.370+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:36.370+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:36.378+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:36.393+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.031 seconds
[2024-01-19T03:51:06.748+0000] {processor.py:161} INFO - Started process (PID=100) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:06.750+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:51:06.755+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:06.754+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:06.789+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:51:06.798+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:06.796+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 256, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.19.0.3), port 5432 failed: FATAL:  password authentication failed for user "postgres"
[2024-01-19T03:51:06.798+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:06.819+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.076 seconds
[2024-01-19T03:51:11.814+0000] {processor.py:161} INFO - Started process (PID=101) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:11.815+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:51:11.817+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:11.817+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:11.833+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:11.860+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:11.860+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:51:11.880+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:11.880+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:51:11.895+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.083 seconds
[2024-01-19T03:51:12.879+0000] {processor.py:161} INFO - Started process (PID=102) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:12.880+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:51:12.882+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:12.882+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:12.894+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:12.917+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:12.917+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:51:12.931+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:12.931+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:51:12.943+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T03:51:14.961+0000] {processor.py:161} INFO - Started process (PID=103) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:14.962+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:51:14.964+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:14.964+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:14.978+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:15.000+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:15.000+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:51:15.015+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:15.015+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:51:15.024+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T03:51:45.344+0000] {processor.py:161} INFO - Started process (PID=110) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:45.345+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:51:45.351+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:45.350+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:45.375+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:45.402+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:45.402+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:51:45.416+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:45.415+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:51:45.425+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.087 seconds
[2024-01-19T03:52:15.675+0000] {processor.py:161} INFO - Started process (PID=117) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:52:15.676+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:52:15.679+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:52:15.679+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:52:15.707+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:52:15.742+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:52:15.742+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:52:15.770+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:52:15.770+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:52:15.781+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.111 seconds
[2024-01-19T03:52:46.065+0000] {processor.py:161} INFO - Started process (PID=124) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:52:46.066+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:52:46.070+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:52:46.070+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:52:46.088+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:52:46.117+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:52:46.117+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:52:46.142+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:52:46.141+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:52:46.166+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.106 seconds
[2024-01-19T03:53:16.364+0000] {processor.py:161} INFO - Started process (PID=131) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:53:16.365+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:53:16.367+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:53:16.367+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:53:16.382+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:53:16.405+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:53:16.405+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:53:16.420+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:53:16.420+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:53:16.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.069 seconds
[2024-01-19T03:53:46.708+0000] {processor.py:161} INFO - Started process (PID=138) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:53:46.708+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:53:46.710+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:53:46.710+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:53:46.718+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:53:46.733+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:53:46.733+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:53:46.745+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:53:46.745+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:53:46.752+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.047 seconds
[2024-01-19T03:54:16.996+0000] {processor.py:161} INFO - Started process (PID=145) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:54:16.997+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:54:17.001+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:54:17.000+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:54:17.021+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:54:17.048+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:54:17.048+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:54:17.066+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:54:17.066+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:54:17.078+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.088 seconds
[2024-01-19T03:54:47.314+0000] {processor.py:161} INFO - Started process (PID=152) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:54:47.315+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:54:47.318+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:54:47.318+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:54:47.334+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:54:47.360+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:54:47.360+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:54:47.377+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:54:47.377+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:54:47.392+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T03:55:33.479+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:33.480+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:55:33.482+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:33.482+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:33.496+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:33.659+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:33.659+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:55:33.671+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:33.671+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:55:33.683+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.207 seconds
[2024-01-19T03:55:55.712+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:55.713+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:55:55.716+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:55.716+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:55.734+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:55.748+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:55.748+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:55:55.785+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:55.785+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:55:55.819+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.112 seconds
[2024-01-19T03:55:58.744+0000] {processor.py:161} INFO - Started process (PID=44) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:58.745+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:55:58.748+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:58.747+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:58.767+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:58.779+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:58.778+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:55:58.793+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:58.793+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:55:58.803+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-19T03:56:06.874+0000] {processor.py:161} INFO - Started process (PID=45) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:06.875+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:56:06.877+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:06.877+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:06.894+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:06.915+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:06.915+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:56:06.928+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:06.928+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:56:06.939+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-19T03:56:07.895+0000] {processor.py:161} INFO - Started process (PID=46) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:07.896+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:56:07.898+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:07.898+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:07.914+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:07.932+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:07.932+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:56:07.943+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:07.943+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:56:07.952+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.061 seconds
[2024-01-19T03:56:08.989+0000] {processor.py:161} INFO - Started process (PID=47) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:08.990+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:56:08.993+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:08.993+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:09.012+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:09.033+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:09.033+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:56:09.047+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:09.047+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:56:09.057+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T03:56:39.345+0000] {processor.py:161} INFO - Started process (PID=54) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:39.347+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:56:39.348+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:39.348+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:39.358+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:39.373+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:39.373+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:56:39.385+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:39.385+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:56:39.394+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.051 seconds
[2024-01-19T03:57:09.716+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:57:09.719+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:57:09.724+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:57:09.723+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:57:09.747+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:57:09.769+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:57:09.769+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:57:09.782+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:57:09.782+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:57:09.790+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T03:57:40.076+0000] {processor.py:161} INFO - Started process (PID=68) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:57:40.077+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:57:40.082+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:57:40.081+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:57:40.105+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:57:40.136+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:57:40.135+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:57:40.160+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:57:40.160+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:57:40.173+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.103 seconds
[2024-01-19T03:58:10.358+0000] {processor.py:161} INFO - Started process (PID=75) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:58:10.358+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:58:10.361+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:58:10.360+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:58:10.372+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:58:10.390+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:58:10.390+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:58:10.404+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:58:10.404+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:58:10.414+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.059 seconds
[2024-01-19T03:58:40.752+0000] {processor.py:161} INFO - Started process (PID=82) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:58:40.754+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:58:40.757+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:58:40.756+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:58:40.772+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:58:40.793+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:58:40.793+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:58:40.805+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:58:40.805+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:58:40.812+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-19T03:59:11.138+0000] {processor.py:161} INFO - Started process (PID=89) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:59:11.139+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:59:11.143+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:59:11.143+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:59:11.162+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:59:11.186+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:59:11.186+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:59:11.199+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:59:11.199+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:59:11.207+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.075 seconds
[2024-01-19T03:59:41.466+0000] {processor.py:161} INFO - Started process (PID=96) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:59:41.472+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:59:41.480+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:59:41.480+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:59:41.502+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:59:41.551+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:59:41.551+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:59:41.598+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:59:41.598+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:59:41.639+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.182 seconds
[2024-01-19T04:03:30.903+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:03:30.906+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:03:30.909+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:03:30.909+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:03:30.931+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:03:31.051+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:03:31.051+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:03:31.064+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:03:31.064+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:03:31.074+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.174 seconds
[2024-01-19T04:04:01.179+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:04:01.181+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:04:01.185+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:04:01.185+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:04:01.206+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:04:01.229+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:04:01.229+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:04:01.241+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:04:01.241+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:04:01.250+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T04:04:31.460+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:04:31.461+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:04:31.463+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:04:31.463+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:04:31.480+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:04:31.502+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:04:31.501+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:04:31.514+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:04:31.514+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:04:31.523+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T04:05:01.779+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:05:01.781+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:05:01.783+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:05:01.783+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:05:01.797+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:05:01.819+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:05:01.819+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:05:01.831+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:05:01.831+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:05:01.840+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-19T04:05:32.074+0000] {processor.py:161} INFO - Started process (PID=63) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:05:32.075+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:05:32.078+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:05:32.077+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:05:32.094+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:05:32.126+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:05:32.126+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:05:32.140+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:05:32.140+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:05:32.150+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T04:06:02.368+0000] {processor.py:161} INFO - Started process (PID=70) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:06:02.369+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:06:02.371+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:06:02.371+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:06:02.380+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:06:02.397+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:06:02.396+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:06:02.409+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:06:02.409+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:06:02.417+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.052 seconds
[2024-01-19T04:06:32.689+0000] {processor.py:161} INFO - Started process (PID=77) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:06:32.690+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:06:32.693+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:06:32.692+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:06:32.708+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:06:32.728+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:06:32.728+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:06:32.739+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:06:32.739+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:06:32.747+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-19T04:07:03.051+0000] {processor.py:161} INFO - Started process (PID=84) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:07:03.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:07:03.055+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:07:03.054+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:07:03.069+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:07:03.092+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:07:03.091+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:07:03.105+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:07:03.104+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:07:03.113+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T04:07:33.374+0000] {processor.py:161} INFO - Started process (PID=91) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:07:33.375+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:07:33.377+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:07:33.377+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:07:33.392+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:07:33.413+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:07:33.412+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:07:33.424+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:07:33.424+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:07:33.432+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T04:08:03.739+0000] {processor.py:161} INFO - Started process (PID=98) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:08:03.740+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:08:03.742+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:08:03.742+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:08:03.752+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:08:03.770+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:08:03.770+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:08:03.782+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:08:03.782+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:08:03.790+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.054 seconds
[2024-01-19T04:08:34.049+0000] {processor.py:161} INFO - Started process (PID=105) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:08:34.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:08:34.051+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:08:34.051+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:08:34.058+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:08:34.073+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:08:34.073+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:08:34.084+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:08:34.084+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:08:34.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.045 seconds
[2024-01-19T04:09:04.466+0000] {processor.py:161} INFO - Started process (PID=112) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:09:04.467+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:09:04.469+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:09:04.469+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:09:04.480+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:09:04.497+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:09:04.497+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:09:04.511+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:09:04.511+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:09:04.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T04:09:34.801+0000] {processor.py:161} INFO - Started process (PID=119) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:09:34.803+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:09:34.808+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:09:34.808+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:09:34.838+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:09:34.861+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:09:34.861+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:09:34.873+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:09:34.873+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:09:34.882+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.087 seconds
[2024-01-19T04:10:05.152+0000] {processor.py:161} INFO - Started process (PID=126) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:10:05.153+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:10:05.156+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:10:05.156+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:10:05.173+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:10:05.194+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:10:05.193+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:10:05.207+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:10:05.207+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:10:05.216+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.069 seconds
[2024-01-19T04:10:35.497+0000] {processor.py:161} INFO - Started process (PID=133) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:10:35.498+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:10:35.500+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:10:35.500+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:10:35.510+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:10:35.526+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:10:35.526+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:10:35.537+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:10:35.536+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:10:35.544+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.050 seconds
[2024-01-19T04:11:05.885+0000] {processor.py:161} INFO - Started process (PID=140) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:11:05.886+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:11:05.891+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:11:05.890+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:11:05.913+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:11:05.945+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:11:05.945+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:11:05.963+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:11:05.963+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:11:05.974+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.096 seconds
[2024-01-19T04:11:36.200+0000] {processor.py:161} INFO - Started process (PID=147) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:11:36.201+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:11:36.203+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:11:36.203+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:11:36.214+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:11:36.233+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:11:36.232+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:11:36.246+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:11:36.246+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:11:36.255+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.058 seconds
[2024-01-19T04:12:06.543+0000] {processor.py:161} INFO - Started process (PID=154) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:12:06.545+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:12:06.549+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:12:06.548+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:12:06.568+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:12:06.590+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:12:06.590+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:12:06.602+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:12:06.602+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:12:06.611+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T04:12:36.892+0000] {processor.py:161} INFO - Started process (PID=161) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:12:36.893+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:12:36.896+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:12:36.896+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:12:36.911+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:12:36.931+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:12:36.931+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:12:36.942+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:12:36.942+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:12:36.950+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T04:13:07.246+0000] {processor.py:161} INFO - Started process (PID=168) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:13:07.248+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:13:07.251+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:13:07.251+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:13:07.266+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:13:07.286+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:13:07.286+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:13:07.299+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:13:07.298+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:13:07.306+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-19T04:13:37.619+0000] {processor.py:161} INFO - Started process (PID=175) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:13:37.620+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:13:37.625+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:13:37.624+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:13:37.647+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:13:37.678+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:13:37.678+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:13:37.699+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:13:37.699+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:13:37.719+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.105 seconds
[2024-01-19T04:14:07.944+0000] {processor.py:161} INFO - Started process (PID=182) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:14:07.946+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:14:07.948+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:14:07.948+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:14:07.962+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:14:07.981+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:14:07.981+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:14:07.993+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:14:07.993+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:14:08.000+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.061 seconds
[2024-01-19T04:14:38.235+0000] {processor.py:161} INFO - Started process (PID=189) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:14:38.235+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:14:38.237+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:14:38.237+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:14:38.246+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:14:38.263+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:14:38.263+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:14:38.274+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:14:38.274+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:14:38.281+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.049 seconds
[2024-01-19T04:15:08.552+0000] {processor.py:161} INFO - Started process (PID=196) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:15:08.552+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:15:08.557+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:15:08.557+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:15:08.577+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:15:08.600+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:15:08.599+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:15:08.631+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:15:08.631+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:15:08.641+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.096 seconds
[2024-01-19T04:15:38.932+0000] {processor.py:161} INFO - Started process (PID=203) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:15:38.933+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:15:38.935+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:15:38.934+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:15:38.944+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:15:38.961+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:15:38.961+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:15:38.972+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:15:38.971+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:15:38.980+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.050 seconds
[2024-01-19T04:16:09.267+0000] {processor.py:161} INFO - Started process (PID=210) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:16:09.269+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:16:09.273+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:16:09.273+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:16:09.293+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:16:09.314+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:16:09.314+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:16:09.327+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:16:09.326+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:16:09.335+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T04:16:39.641+0000] {processor.py:161} INFO - Started process (PID=217) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:16:39.642+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:16:39.646+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:16:39.645+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:16:39.666+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:16:39.693+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:16:39.693+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:16:39.707+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:16:39.707+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:16:39.716+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
