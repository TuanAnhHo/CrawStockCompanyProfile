[2024-01-19T02:20:17.492+0000] {processor.py:161} INFO - Started process (PID=35) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:20:17.493+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:20:17.495+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:20:17.495+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:20:17.515+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:20:17.622+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:20:17.622+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:20:17.635+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:20:17.635+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-18T00:00:00+00:00, run_after=2024-01-19T00:00:00+00:00
[2024-01-19T02:20:17.649+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.159 seconds
[2024-01-19T02:20:47.833+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:20:47.834+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:20:47.839+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:20:47.839+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:20:47.861+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:20:47.883+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:20:47.883+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:20:47.895+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:20:47.895+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:20:47.903+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T02:21:18.131+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:21:18.132+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:21:18.134+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:21:18.134+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:21:18.150+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:21:18.170+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:21:18.170+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:21:18.181+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:21:18.181+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:21:18.188+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T02:21:48.413+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:21:48.414+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:21:48.415+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:21:48.415+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:21:48.426+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:21:48.444+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:21:48.443+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:21:48.455+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:21:48.455+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:21:48.463+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.053 seconds
[2024-01-19T02:22:18.696+0000] {processor.py:161} INFO - Started process (PID=63) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:18.697+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:22:18.700+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:18.700+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:18.715+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:18.737+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:18.737+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:22:18.751+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:18.751+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:22:18.761+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.069 seconds
[2024-01-19T02:22:48.997+0000] {processor.py:161} INFO - Started process (PID=70) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:48.998+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:22:49.002+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:49.001+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:49.020+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:49.043+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:49.043+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:22:49.055+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:49.055+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:22:49.063+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T02:22:53.072+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:53.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:22:53.076+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:53.076+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:53.083+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:53.082+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 839, in exec_module
  File "<frozen importlib._bootstrap_external>", line 976, in get_code
  File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 22
    params = {'host': 'Authorization=Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IkdYdExONzViZlZQakdvNERWdjV4QkRITHpnSSIsImtpZCI6IkdYdExONzViZlZQakdvNERWdjV4QkRITHpnSSJ9.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmZpcmVhbnQudm4iLCJhdWQiOiJodHRwczovL2FjY291bnRzLmZpcmVhbnQudm4vcmVzb3VyY2VzIiwiZXhwIjoxODg5NjIyNTMwLCJuYmYiOjE1ODk2MjI1MzAsImNsaWVudF9pZCI6ImZpcmVhbnQudHJhZGVzdGF0aW9uIiwic2NvcGUiOlsiYWNhZGVteS1yZWFkIiwiYWNhZGVteS13cml0ZSIsImFjY291bnRzLXJlYWQiLCJhY2NvdW50cy13cml0ZSIsImJsb2ctcmVhZCIsImNvbXBhbmllcy1yZWFkIiwiZmluYW5jZS1yZWFkIiwiaW5kaXZpZHVhbHMtcmVhZCIsImludmVzdG9wZWRpYS1yZWFkIiwib3JkZXJzLXJlYWQiLCJvcmRlcnMtd3JpdGUiLCJwb3N0cy1yZWFkIiwicG9zdHMtd3JpdGUiLCJzZWFyY2giLCJzeW1ib2xzLXJlYWQiLCJ1c2VyLWRhdGEtcmVhZCIsInVzZXItZGF0YS13cml0ZSIsInVzZXJzLXJlYWQiXSwianRpIjoiMjYxYTZhYWQ2MTQ5Njk1ZmJiYzcwODM5MjM0Njc1NWQifQ.dA5-HVzWv-BRfEiAd24uNBiBxASO-PAyWeWESovZm_hj4aXMAZA1-bWNZeXt88dqogo18AwpDQ-h6gefLPdZSFrG5umC1dVWaeYvUnGm62g4XS29fj6p01dhKNNqrsu5KrhnhdnKYVv9VdmbmqDfWR8wDgglk5cJFqalzq6dJWJInFQEPmUs9BW_Zs8tQDn-i5r4tYq2U8vCdqptXoM7YgPllXaPVDeccC9QNu2Xlp9WUvoROzoQXg25lFub1IYkTrM66gJ6t9fJRZToewCt495WNEOQFa_rwLCZ1QwzvL0iYkONHS_jZ0BOhBCdW9dWSawD6iF1SIQaFROvMDH1rg
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^
SyntaxError: EOL while scanning string literal
[2024-01-19T02:22:53.084+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:53.103+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.035 seconds
[2024-01-19T02:22:54.138+0000] {processor.py:161} INFO - Started process (PID=72) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:54.139+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:22:54.141+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:54.141+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:54.159+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:54.185+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:54.185+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:22:54.199+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:54.199+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:22:54.209+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.075 seconds
[2024-01-19T02:22:57.200+0000] {processor.py:161} INFO - Started process (PID=73) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:57.202+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:22:57.205+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:57.205+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:57.224+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:22:57.246+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:57.245+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:22:57.257+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:22:57.257+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:22:57.268+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T02:23:11.425+0000] {processor.py:161} INFO - Started process (PID=74) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:11.426+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:23:11.429+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:11.429+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:11.452+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:11.469+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:11.469+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:23:11.480+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:11.480+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:23:11.491+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T02:23:12.458+0000] {processor.py:161} INFO - Started process (PID=75) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:12.460+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:23:12.464+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:12.464+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:12.482+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:12.503+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:12.503+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:23:12.516+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:12.515+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:23:12.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T02:23:42.863+0000] {processor.py:161} INFO - Started process (PID=82) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:42.866+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:23:42.870+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:42.870+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:42.894+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:23:42.918+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:42.918+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:23:42.933+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:23:42.933+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T02:23:42.941+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.084 seconds
[2024-01-19T02:24:13.122+0000] {processor.py:161} INFO - Started process (PID=95) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:24:13.123+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:24:13.126+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:24:13.125+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:24:13.142+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:24:13.165+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:24:13.165+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:24:13.187+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T02:24:43.360+0000] {processor.py:161} INFO - Started process (PID=102) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:24:43.361+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:24:43.363+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:24:43.363+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:24:43.374+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:24:43.391+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:24:43.391+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:24:43.435+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T02:25:13.626+0000] {processor.py:161} INFO - Started process (PID=109) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:13.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:25:13.632+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:25:13.631+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:13.653+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:13.672+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:25:13.672+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:25:13.691+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T02:25:43.965+0000] {processor.py:161} INFO - Started process (PID=116) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:43.967+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:25:43.972+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:25:43.972+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:43.995+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:44.022+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:25:44.022+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:25:44.040+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:25:44.040+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-16T00:00:00+00:00, run_after=2024-01-17T00:00:00+00:00
[2024-01-19T02:25:44.049+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.091 seconds
[2024-01-19T02:25:47.013+0000] {processor.py:161} INFO - Started process (PID=117) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:47.015+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:25:47.020+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:25:47.020+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:47.046+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:25:47.064+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:25:47.064+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:25:47.087+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.085 seconds
[2024-01-19T02:26:17.347+0000] {processor.py:161} INFO - Started process (PID=124) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:17.349+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:26:17.353+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:17.352+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:17.372+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:17.398+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:17.398+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:26:17.415+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:17.415+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:26:17.437+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.095 seconds
[2024-01-19T02:26:47.645+0000] {processor.py:161} INFO - Started process (PID=131) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:47.648+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:26:47.653+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:47.652+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:47.685+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:47.707+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:47.707+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:26:47.721+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:47.721+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:26:47.730+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.092 seconds
[2024-01-19T02:26:48.668+0000] {processor.py:161} INFO - Started process (PID=132) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:48.669+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:26:48.672+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:48.672+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:48.686+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:48.684+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    C
NameError: name 'C' is not defined
[2024-01-19T02:26:48.687+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:48.702+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.037 seconds
[2024-01-19T02:26:49.680+0000] {processor.py:161} INFO - Started process (PID=133) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:49.682+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:26:49.684+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:49.684+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:49.702+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:49.701+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    Co
NameError: name 'Co' is not defined
[2024-01-19T02:26:49.703+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:49.719+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.041 seconds
[2024-01-19T02:26:50.745+0000] {processor.py:161} INFO - Started process (PID=134) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:50.745+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:26:50.747+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:50.747+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:50.761+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:50.778+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:50.778+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:26:50.789+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:50.789+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:26:50.798+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.057 seconds
[2024-01-19T02:26:51.765+0000] {processor.py:161} INFO - Started process (PID=135) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:51.766+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:26:51.768+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:51.768+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:51.778+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:51.778+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    Conn
NameError: name 'Conn' is not defined
[2024-01-19T02:26:51.779+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:51.791+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.030 seconds
[2024-01-19T02:26:53.791+0000] {processor.py:161} INFO - Started process (PID=136) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:53.792+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:26:53.794+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:26:53.794+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:26:53.803+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:28:08.468+0000] {processor.py:161} INFO - Started process (PID=149) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:08.469+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:28:08.472+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:08.471+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:08.483+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:28:08.674+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:08.671+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres(
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "loca" to address: Name or service not known
[2024-01-19T02:28:08.676+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:08.699+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.234 seconds
[2024-01-19T02:28:09.487+0000] {processor.py:161} INFO - Started process (PID=150) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:09.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:28:09.490+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:09.490+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:09.500+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:28:09.502+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:09.501+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:28:09.502+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:09.515+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.032 seconds
[2024-01-19T02:28:22.648+0000] {processor.py:161} INFO - Started process (PID=157) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:22.649+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:28:22.650+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:22.649+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:22.661+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:28:22.663+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:22.663+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:28:22.664+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:22.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.044 seconds
[2024-01-19T02:28:31.748+0000] {processor.py:161} INFO - Started process (PID=158) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:31.749+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:28:31.750+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:31.750+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:31.758+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:28:31.759+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:31.758+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:28:31.759+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:31.773+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.028 seconds
[2024-01-19T02:28:32.773+0000] {processor.py:161} INFO - Started process (PID=159) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:32.774+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:28:32.775+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:32.774+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:32.781+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:28:32.783+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:28:32.782+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:28:32.783+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:28:32.798+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.030 seconds
[2024-01-19T02:29:03.083+0000] {processor.py:161} INFO - Started process (PID=166) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:03.084+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:29:03.085+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:03.084+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:03.092+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:29:03.095+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:03.093+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:29:03.095+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:03.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.033 seconds
[2024-01-19T02:29:28.353+0000] {processor.py:161} INFO - Started process (PID=173) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:28.354+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:29:28.355+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:28.355+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:28.364+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:29:28.367+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:28.365+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:29:28.367+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:28.387+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.038 seconds
[2024-01-19T02:29:39.484+0000] {processor.py:161} INFO - Started process (PID=174) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:39.486+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:29:39.487+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:39.487+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:39.507+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:29:39.510+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:39.509+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:29:39.511+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:39.528+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.049 seconds
[2024-01-19T02:29:40.527+0000] {processor.py:161} INFO - Started process (PID=175) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:40.529+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:29:40.531+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:40.530+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:40.546+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:29:40.676+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:40.674+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "localhos" to address: Name or service not known
[2024-01-19T02:29:40.677+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:40.698+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.177 seconds
[2024-01-19T02:29:43.603+0000] {processor.py:161} INFO - Started process (PID=176) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:43.604+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:29:43.605+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:43.605+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:43.627+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:43.710+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:43.710+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:29:43.720+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:43.720+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:29:43.732+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.134 seconds
[2024-01-19T02:29:45.784+0000] {processor.py:161} INFO - Started process (PID=182) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:45.785+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:29:45.786+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:45.785+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:45.800+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:45.810+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:45.810+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:29:45.827+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:45.827+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:29:45.839+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.058 seconds
[2024-01-19T02:29:46.821+0000] {processor.py:161} INFO - Started process (PID=183) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:46.823+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:29:46.824+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:46.824+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:46.851+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:29:46.863+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:46.863+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:29:46.882+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:29:46.882+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T02:29:46.893+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T02:30:17.191+0000] {processor.py:161} INFO - Started process (PID=190) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:30:17.193+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:30:17.194+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:30:17.194+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:30:17.213+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:30:17.237+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:30:17.237+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:30:17.251+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:30:17.251+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T02:30:17.260+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T02:30:47.481+0000] {processor.py:161} INFO - Started process (PID=197) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:30:47.482+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:30:47.483+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:30:47.483+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:30:47.502+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:30:47.530+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:30:47.530+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:30:47.597+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.119 seconds
[2024-01-19T02:31:17.774+0000] {processor.py:161} INFO - Started process (PID=203) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:17.776+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:31:17.777+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:31:17.777+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:17.795+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:17.820+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:31:17.820+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:31:17.841+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T02:31:39.021+0000] {processor.py:161} INFO - Started process (PID=204) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:39.023+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:31:39.025+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:31:39.024+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:39.064+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:31:39.062+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    co
NameError: name 'co' is not defined
[2024-01-19T02:31:39.064+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:39.083+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T02:31:40.037+0000] {processor.py:161} INFO - Started process (PID=205) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:40.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:31:40.039+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:31:40.039+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:40.049+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:31:40.049+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    Conn
NameError: name 'Conn' is not defined
[2024-01-19T02:31:40.050+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:40.062+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.028 seconds
[2024-01-19T02:31:41.051+0000] {processor.py:161} INFO - Started process (PID=206) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:41.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:31:41.053+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:31:41.053+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:31:41.064+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:49:56.583+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:49:56.596+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:49:56.613+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:49:56.613+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:49:56.657+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:49:56.746+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:49:56.746+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:49:56.771+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.194 seconds
[2024-01-19T02:50:06.727+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:06.727+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:50:06.729+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.729+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:06.741+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:06.756+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.756+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'manual__2024-01-19T02:30:00.254994+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': 'b00e7f29-30c0-42e7-853a-ba983e12f84b'}
[2024-01-19T02:50:06.765+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.765+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240119T023000, start_date=20240119T023000, end_date=20240119T025006
[2024-01-19T02:50:06.781+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:06.782+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.782+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:06.782+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.782+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:06.784+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.784+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:06.784+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.784+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:06.786+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.785+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:06.792+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting manual__2024-01-19T02:30:00.254994+00:00 [failed]> in state failed
[2024-01-19T02:50:06.794+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:06.794+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:50:06.809+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.086 seconds
[2024-01-19T02:50:16.852+0000] {processor.py:161} INFO - Started process (PID=38) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:16.853+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:50:16.854+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.854+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:16.863+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:16.877+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.877+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-03T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '3d67ae9c-8d8a-4710-8cc7-d1b68ccee797'}
[2024-01-19T02:50:16.884+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.884+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240103T000000, start_date=20240119T023017, end_date=20240119T025016
[2024-01-19T02:50:16.893+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.894+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.894+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.894+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.894+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.896+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.895+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.896+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.896+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.897+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.896+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.902+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-03T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.903+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.903+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-05T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '52f0e0ba-bd2f-4404-8fcf-ed00fb5d1c69'}
[2024-01-19T02:50:16.906+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.906+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240105T000000, start_date=20240119T023018, end_date=20240119T025016
[2024-01-19T02:50:16.910+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.910+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.910+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.910+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.910+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.911+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.911+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.912+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.912+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.912+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.912+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.914+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-05T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.914+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.914+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-07T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': 'c99e0115-5427-48d7-a2a5-2da8dad029df'}
[2024-01-19T02:50:16.917+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.917+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240107T000000, start_date=20240119T023018, end_date=20240119T025016
[2024-01-19T02:50:16.921+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.922+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.922+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.922+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.922+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.923+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.923+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.923+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.923+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.924+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.923+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.925+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-07T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.926+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.926+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-09T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '63a8113e-43ce-44eb-bee7-895d6bcdbd57'}
[2024-01-19T02:50:16.928+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.928+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240109T000000, start_date=20240119T023018, end_date=20240119T025016
[2024-01-19T02:50:16.932+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.933+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.932+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.933+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.933+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.934+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.934+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.934+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.934+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.935+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.934+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.936+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-09T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.937+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.937+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-11T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '4b15f2f8-9bd0-491d-9ceb-4cd6007978f3'}
[2024-01-19T02:50:16.940+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.940+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240111T000000, start_date=20240119T023018, end_date=20240119T025016
[2024-01-19T02:50:16.944+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.944+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.944+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.945+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.945+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.946+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.946+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.946+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.946+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.947+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.946+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.949+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-11T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.950+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.949+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-13T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '93f8370e-9033-4275-b477-661186dcda19'}
[2024-01-19T02:50:16.953+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.953+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240113T000000, start_date=20240119T023019, end_date=20240119T025016
[2024-01-19T02:50:16.958+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.958+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.958+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.958+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.958+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.959+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.959+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.960+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.960+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.960+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.960+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.962+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-13T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.963+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.963+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-15T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': 'eaff8bf1-5257-4ab2-b5e9-7f3a2e58b7bb'}
[2024-01-19T02:50:16.966+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.966+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240115T000000, start_date=20240119T023019, end_date=20240119T025016
[2024-01-19T02:50:16.970+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.971+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.971+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.971+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.971+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.972+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.972+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.972+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.972+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.973+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.972+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.974+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-15T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.975+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.975+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-04T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '259814e0-393a-432d-998e-c1c9829be9d0'}
[2024-01-19T02:50:16.978+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.978+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240104T000000, start_date=20240119T023018, end_date=20240119T025016
[2024-01-19T02:50:16.982+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.982+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.982+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.983+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.983+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.984+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.984+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.984+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.984+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.984+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.984+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.986+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-04T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.987+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.987+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-02T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '98e07506-903d-4948-a6cb-45b922ec77aa'}
[2024-01-19T02:50:16.990+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.990+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240102T000000, start_date=20240119T023017, end_date=20240119T025016
[2024-01-19T02:50:16.994+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:16.995+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.994+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.995+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.995+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.996+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.996+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:16.996+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.996+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:16.996+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.996+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:16.998+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-02T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:16.999+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:16.999+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-01T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': 'cafaaadd-c44f-492c-a56f-50c6f15a3c61'}
[2024-01-19T02:50:17.001+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.001+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240101T000000, start_date=20240119T023017, end_date=20240119T025017
[2024-01-19T02:50:17.005+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:17.006+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.006+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.006+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.006+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.007+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.007+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.007+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.007+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.008+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.007+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:17.010+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-01T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:17.011+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.011+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-06T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '89996970-45f9-49ff-bb79-803f3d35c546'}
[2024-01-19T02:50:17.014+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.014+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240106T000000, start_date=20240119T023018, end_date=20240119T025017
[2024-01-19T02:50:17.019+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:17.019+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.019+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.020+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.019+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.021+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.021+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.021+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.021+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.022+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.021+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:17.024+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-06T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:17.025+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.025+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-08T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '55634143-ecdb-4626-ab10-0808ea841df0'}
[2024-01-19T02:50:17.028+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.028+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240108T000000, start_date=20240119T023018, end_date=20240119T025017
[2024-01-19T02:50:17.033+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:17.033+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.033+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.033+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.033+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.035+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.035+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.035+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.035+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.035+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.035+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:17.037+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-08T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:17.038+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.038+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-10T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '27e729e2-8fc2-4467-bae5-8422051a8a7d'}
[2024-01-19T02:50:17.041+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.041+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240110T000000, start_date=20240119T023018, end_date=20240119T025017
[2024-01-19T02:50:17.045+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:17.046+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.046+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.046+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.046+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.047+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.047+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.047+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.047+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.048+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.047+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:17.049+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-10T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:17.050+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.050+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-12T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '0592aee0-724f-4494-80e6-ab3694bafbd7'}
[2024-01-19T02:50:17.053+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.053+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240112T000000, start_date=20240119T023019, end_date=20240119T025017
[2024-01-19T02:50:17.057+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:17.058+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.058+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.058+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.058+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.059+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.059+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.059+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.059+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.060+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.059+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:17.062+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-12T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:17.063+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.063+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-14T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '27eefbb1-6501-4847-adb3-b5044f44280a'}
[2024-01-19T02:50:17.066+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.066+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240114T000000, start_date=20240119T023019, end_date=20240119T025017
[2024-01-19T02:50:17.070+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:17.070+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.070+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.071+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.071+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.072+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.072+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.072+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.072+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.073+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.072+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:17.075+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-14T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:17.076+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.076+0000] {taskinstance.py:2701} ERROR - {'DAG Id': 'company_profile_pipeline', 'Task Id': 'CheckTableExisting', 'Run Id': 'scheduled__2024-01-03T00:00:00+00:00', 'Hostname': 'a56b2d2a6d76', 'External Executor Id': '3d67ae9c-8d8a-4710-8cc7-d1b68ccee797'}
[2024-01-19T02:50:17.078+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.078+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=company_profile_pipeline, task_id=CheckTableExisting, execution_date=20240103T000000, start_date=20240119T023017, end_date=20240119T025017
[2024-01-19T02:50:17.083+0000] {logging_mixin.py:188} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py:154 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2024-01-19T02:50:17.083+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.083+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.083+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.083+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.085+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.085+0000] {configuration.py:1046} WARNING - section/key [smtp/smtp_user] not found in config
[2024-01-19T02:50:17.085+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.085+0000] {email.py:270} INFO - Email alerting: attempt 1
[2024-01-19T02:50:17.085+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.085+0000] {taskinstance.py:826} ERROR - Failed to send email to: anh.ho@tititada.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1000, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 824, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2947, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1002, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 272, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/email.py", line 316, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
  File "/usr/local/lib/python3.8/smtplib.py", line 339, in connect
    self.sock = self._get_socket(host, port, self.timeout)
  File "/usr/local/lib/python3.8/smtplib.py", line 310, in _get_socket
    return socket.create_connection((host, port), timeout,
  File "/usr/local/lib/python3.8/socket.py", line 808, in create_connection
    raise err
  File "/usr/local/lib/python3.8/socket.py", line 796, in create_connection
    sock.connect(sa)
OSError: [Errno 99] Cannot assign requested address
[2024-01-19T02:50:17.087+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: company_profile_pipeline.CheckTableExisting scheduled__2024-01-03T00:00:00+00:00 [failed]> in state failed
[2024-01-19T02:50:17.089+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.089+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:50:17.097+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:17.097+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-16T00:00:00+00:00, run_after=2024-01-17T00:00:00+00:00
[2024-01-19T02:50:17.104+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.255 seconds
[2024-01-19T02:50:47.328+0000] {processor.py:161} INFO - Started process (PID=45) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:47.331+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:50:47.337+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:47.336+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:47.361+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:50:47.401+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:47.401+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:50:47.412+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:50:47.412+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T02:50:47.424+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.102 seconds
[2024-01-19T02:51:17.654+0000] {processor.py:161} INFO - Started process (PID=52) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:51:17.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:51:17.659+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:51:17.658+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:51:17.678+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:51:17.699+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:51:17.699+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:51:17.712+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:51:17.712+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T02:51:17.721+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T02:51:47.989+0000] {processor.py:161} INFO - Started process (PID=59) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:51:47.990+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:51:47.994+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:51:47.993+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:51:48.012+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:51:48.037+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:51:48.037+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:51:48.051+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:51:48.050+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T02:51:48.059+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.076 seconds
[2024-01-19T02:52:18.303+0000] {processor.py:161} INFO - Started process (PID=66) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:52:18.305+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:52:18.309+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:52:18.308+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:52:18.327+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:52:18.349+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:52:18.349+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:52:18.377+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-19T02:52:48.671+0000] {processor.py:161} INFO - Started process (PID=73) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:52:48.673+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:52:48.677+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:52:48.676+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:52:48.697+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:52:48.723+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:52:48.723+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:52:48.751+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.084 seconds
[2024-01-19T02:53:18.994+0000] {processor.py:161} INFO - Started process (PID=80) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:53:18.995+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:53:18.997+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:53:18.997+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:53:19.009+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:53:19.027+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:53:19.027+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:53:19.048+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.057 seconds
[2024-01-19T02:53:49.299+0000] {processor.py:161} INFO - Started process (PID=87) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:53:49.301+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:53:49.306+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:53:49.305+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:53:49.327+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:53:49.348+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:53:49.348+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:53:49.368+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T02:54:19.653+0000] {processor.py:161} INFO - Started process (PID=94) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:54:19.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:54:19.658+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:54:19.658+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:54:19.678+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:54:19.702+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:54:19.701+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:54:19.723+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T02:54:49.973+0000] {processor.py:161} INFO - Started process (PID=101) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:54:49.974+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:54:49.977+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:54:49.976+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:54:49.996+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:54:50.022+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:54:50.022+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:54:50.049+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.080 seconds
[2024-01-19T02:55:20.264+0000] {processor.py:161} INFO - Started process (PID=108) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:55:20.266+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:55:20.270+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:55:20.270+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:55:20.291+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:55:20.315+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:55:20.315+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:55:20.337+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T02:55:50.603+0000] {processor.py:161} INFO - Started process (PID=115) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:55:50.605+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:55:50.609+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:55:50.609+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:55:50.631+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:55:50.656+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:55:50.656+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:55:50.676+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.080 seconds
[2024-01-19T02:56:20.982+0000] {processor.py:161} INFO - Started process (PID=122) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:56:20.984+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:56:20.986+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:56:20.986+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:56:21.003+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:56:21.021+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:56:21.021+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:56:21.041+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-19T02:56:51.251+0000] {processor.py:161} INFO - Started process (PID=129) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:56:51.252+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:56:51.256+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:56:51.255+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:56:51.272+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:56:51.293+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:56:51.292+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:56:51.320+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T02:57:21.546+0000] {processor.py:161} INFO - Started process (PID=136) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:21.547+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:21.550+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:21.550+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:21.568+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:21.590+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:21.590+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:57:21.615+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T02:57:44.766+0000] {processor.py:161} INFO - Started process (PID=143) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:44.767+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:44.769+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:44.769+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:44.794+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:44.818+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:44.818+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:57:44.834+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:44.834+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T02:57:44.844+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T02:57:46.876+0000] {processor.py:161} INFO - Started process (PID=144) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:46.877+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:46.879+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:46.879+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:46.896+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:46.920+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:46.920+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T02:57:46.932+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:46.932+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T02:57:46.943+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T02:57:48.931+0000] {processor.py:161} INFO - Started process (PID=145) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:48.933+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:48.936+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:48.936+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:48.950+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:57:48.952+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:48.951+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:57:48.953+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:48.975+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.049 seconds
[2024-01-19T02:57:50.001+0000] {processor.py:161} INFO - Started process (PID=146) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:50.001+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:50.003+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:50.003+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:50.014+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:57:50.017+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:50.015+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("co")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:57:50.018+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:50.034+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.037 seconds
[2024-01-19T02:57:51.026+0000] {processor.py:161} INFO - Started process (PID=147) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:51.027+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:51.029+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:51.029+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:51.040+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:57:51.042+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:51.041+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:57:51.042+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:51.054+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.031 seconds
[2024-01-19T02:57:52.062+0000] {processor.py:161} INFO - Started process (PID=148) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:52.063+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:52.064+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:52.064+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:52.073+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:57:52.074+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:52.073+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_pre")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:57:52.074+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:52.087+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.027 seconds
[2024-01-19T02:57:53.080+0000] {processor.py:161} INFO - Started process (PID=155) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:53.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:53.082+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:53.082+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:53.092+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:57:53.097+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:53.095+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_prf")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:57:53.103+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:53.123+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.046 seconds
[2024-01-19T02:57:55.125+0000] {processor.py:161} INFO - Started process (PID=156) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:55.126+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:55.128+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:55.128+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:55.136+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:57:55.138+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:55.137+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_pr")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:57:55.138+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:55.149+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.027 seconds
[2024-01-19T02:57:56.154+0000] {processor.py:161} INFO - Started process (PID=157) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:56.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:57:56.157+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:56.156+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:56.166+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:57:56.167+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:57:56.166+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:57:56.167+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:57:56.179+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.028 seconds
[2024-01-19T02:59:23.996+0000] {processor.py:161} INFO - Started process (PID=35) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:59:23.998+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:59:24.000+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:59:24.000+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:59:24.012+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:59:24.013+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:59:24.012+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:59:24.014+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:59:24.028+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.036 seconds
[2024-01-19T02:59:54.190+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:59:54.192+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T02:59:54.196+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:59:54.196+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:59:54.212+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T02:59:54.214+0000] {logging_mixin.py:188} INFO - [2024-01-19T02:59:54.213+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T02:59:54.215+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T02:59:54.228+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.043 seconds
[2024-01-19T03:00:24.487+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:24.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:00:24.488+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:24.488+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:24.498+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:00:24.500+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:24.499+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T03:00:24.500+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:24.512+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.028 seconds
[2024-01-19T03:00:54.761+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:54.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:00:54.763+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:54.763+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:54.776+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:00:54.778+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:54.777+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T03:00:54.778+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:54.794+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.037 seconds
[2024-01-19T03:00:55.784+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:55.785+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:00:55.786+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:55.786+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:55.791+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:55.791+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 839, in exec_module
  File "<frozen importlib._bootstrap_external>", line 976, in get_code
  File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257
    A CheckTableExisting("company_profile")
      ^
SyntaxError: invalid syntax
[2024-01-19T03:00:55.792+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:55.808+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.028 seconds
[2024-01-19T03:00:56.818+0000] {processor.py:161} INFO - Started process (PID=58) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:56.818+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:00:56.819+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:56.819+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:56.828+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:00:56.829+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:56.828+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    A = CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T03:00:56.829+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:56.843+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.027 seconds
[2024-01-19T03:00:57.835+0000] {processor.py:161} INFO - Started process (PID=59) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:57.836+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:00:57.836+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:57.836+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:57.846+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:00:57.847+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:57.846+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    A = CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T03:00:57.847+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:57.859+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.026 seconds
[2024-01-19T03:00:58.867+0000] {processor.py:161} INFO - Started process (PID=60) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:58.868+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:00:58.868+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:58.868+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:58.877+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:00:58.878+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:58.877+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    A = CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T03:00:58.878+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:58.889+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.024 seconds
[2024-01-19T03:00:59.888+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:59.889+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:00:59.890+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:59.889+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:59.900+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:00:59.901+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:00:59.900+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    A = CheckTableExisting("company_profile")
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 50, in CheckTableExisting
    cursor = ConnectPostgres().cursor()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "127.0.0.1", port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
[2024-01-19T03:00:59.901+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:00:59.914+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.028 seconds
[2024-01-19T03:11:33.013+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:11:33.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:11:33.026+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:11:33.026+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:11:33.046+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:11:33.149+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:11:33.149+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:11:33.163+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:11:33.163+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T03:11:33.177+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.169 seconds
[2024-01-19T03:12:03.258+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:12:03.259+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:12:03.264+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:12:03.263+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:12:03.285+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:12:03.310+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:12:03.310+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:12:03.324+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:12:03.324+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T03:12:03.334+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T03:12:33.574+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:12:33.575+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:12:33.580+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:12:33.579+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:12:33.602+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:12:33.627+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:12:33.627+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:12:33.640+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:12:33.640+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T03:12:33.648+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T03:13:03.864+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:03.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:13:03.869+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:03.868+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:03.888+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:03.907+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:03.907+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:13:03.958+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.100 seconds
[2024-01-19T03:13:34.216+0000] {processor.py:161} INFO - Started process (PID=68) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:34.218+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:13:34.222+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:34.221+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:34.242+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:34.262+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:34.262+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:13:34.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T03:13:47.366+0000] {processor.py:161} INFO - Started process (PID=69) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:47.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:13:47.369+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:47.369+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:47.384+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:47.458+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:47.458+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:13:47.480+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.117 seconds
[2024-01-19T03:13:48.392+0000] {processor.py:161} INFO - Started process (PID=70) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:48.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:13:48.393+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:48.393+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:48.404+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:13:48.411+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:13:48.411+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:13:48.429+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.040 seconds
[2024-01-19T03:14:18.650+0000] {processor.py:161} INFO - Started process (PID=82) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:14:18.653+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:14:18.672+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:14:18.671+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:14:18.692+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:14:18.716+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:14:18.716+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:14:18.730+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:14:18.730+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T03:14:18.740+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.105 seconds
[2024-01-19T03:14:48.929+0000] {processor.py:161} INFO - Started process (PID=179) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:14:48.932+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:14:48.937+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:14:48.936+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:14:48.959+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:14:48.981+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:14:48.981+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:14:48.994+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:14:48.994+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:14:49.002+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-19T03:15:19.316+0000] {processor.py:161} INFO - Started process (PID=186) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:19.318+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:15:19.321+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:19.321+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:19.342+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:19.369+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:19.369+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:15:19.390+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:19.390+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:15:19.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.096 seconds
[2024-01-19T03:15:21.361+0000] {processor.py:161} INFO - Started process (PID=187) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:21.362+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:15:21.365+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:21.365+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:21.373+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:21.371+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 839, in exec_module
  File "<frozen importlib._bootstrap_external>", line 976, in get_code
  File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 250
    )
    ^
SyntaxError: unmatched ')'
[2024-01-19T03:15:21.373+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:21.399+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.044 seconds
[2024-01-19T03:15:28.436+0000] {processor.py:161} INFO - Started process (PID=194) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:28.437+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:15:28.439+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:28.439+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:28.458+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:28.534+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:28.534+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:15:28.546+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:28.545+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:15:28.556+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.124 seconds
[2024-01-19T03:15:29.489+0000] {processor.py:161} INFO - Started process (PID=195) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:29.489+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:15:29.491+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:29.490+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:29.501+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:29.510+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:29.510+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:15:29.521+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:29.521+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:15:29.529+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.043 seconds
[2024-01-19T03:15:59.812+0000] {processor.py:161} INFO - Started process (PID=202) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:59.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:15:59.817+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:59.817+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:59.835+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:15:59.875+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:59.875+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:15:59.891+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:15:59.891+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T03:15:59.901+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.094 seconds
[2024-01-19T03:16:30.151+0000] {processor.py:161} INFO - Started process (PID=209) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:16:30.153+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:16:30.158+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:16:30.157+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:16:30.195+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:16:30.235+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:16:30.235+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:16:30.253+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:16:30.253+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:16:30.266+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.122 seconds
[2024-01-19T03:17:00.538+0000] {processor.py:161} INFO - Started process (PID=216) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:17:00.540+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:17:00.544+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:17:00.544+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:17:00.565+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:17:00.588+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:17:00.588+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:17:00.602+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:17:00.602+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:17:00.613+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.087 seconds
[2024-01-19T03:17:30.907+0000] {processor.py:161} INFO - Started process (PID=223) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:17:30.909+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:17:30.915+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:17:30.914+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:17:30.934+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:17:30.958+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:17:30.957+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:17:30.977+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:17:30.977+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:17:30.985+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.086 seconds
[2024-01-19T03:18:01.228+0000] {processor.py:161} INFO - Started process (PID=230) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:18:01.230+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:18:01.233+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:18:01.232+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:18:01.247+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:18:01.268+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:18:01.268+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:18:01.282+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:18:01.282+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:18:01.292+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T03:18:31.572+0000] {processor.py:161} INFO - Started process (PID=237) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:18:31.574+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:18:31.579+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:18:31.579+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:18:31.606+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:18:31.638+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:18:31.638+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:18:31.653+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:18:31.653+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:18:31.662+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.095 seconds
[2024-01-19T03:19:01.963+0000] {processor.py:161} INFO - Started process (PID=244) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:19:01.966+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:19:01.972+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:19:01.971+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:19:01.988+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:19:02.010+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:19:02.010+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:19:02.025+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:19:02.025+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:19:02.034+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-19T03:19:32.302+0000] {processor.py:161} INFO - Started process (PID=251) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:19:32.304+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:19:32.308+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:19:32.307+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:19:32.326+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:19:32.348+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:19:32.348+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:19:32.360+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:19:32.360+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:19:32.368+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T03:20:02.690+0000] {processor.py:161} INFO - Started process (PID=258) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:20:02.692+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:20:02.697+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:20:02.697+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:20:02.718+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:20:02.742+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:20:02.741+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:20:02.754+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:20:02.754+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:20:02.763+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T03:20:33.023+0000] {processor.py:161} INFO - Started process (PID=265) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:20:33.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:20:33.028+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:20:33.028+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:20:33.046+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:20:33.066+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:20:33.066+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:20:33.079+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:20:33.079+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:20:33.088+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T03:21:03.358+0000] {processor.py:161} INFO - Started process (PID=272) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:21:03.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:21:03.364+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:21:03.363+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:21:03.382+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:21:03.408+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:21:03.408+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:21:03.422+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:21:03.422+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:21:03.431+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T03:21:33.726+0000] {processor.py:161} INFO - Started process (PID=279) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:21:33.728+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:21:33.730+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:21:33.730+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:21:33.744+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:21:33.764+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:21:33.764+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:21:33.776+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:21:33.776+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:21:33.784+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T03:22:04.093+0000] {processor.py:161} INFO - Started process (PID=286) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:22:04.094+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:22:04.100+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:22:04.099+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:22:04.115+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:22:04.136+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:22:04.136+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:22:04.148+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:22:04.148+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:22:04.159+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T03:22:34.433+0000] {processor.py:161} INFO - Started process (PID=293) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:22:34.434+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:22:34.436+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:22:34.435+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:22:34.448+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:22:34.464+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:22:34.464+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:22:34.474+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:22:34.474+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:22:34.484+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.055 seconds
[2024-01-19T03:23:04.786+0000] {processor.py:161} INFO - Started process (PID=300) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:23:04.788+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:23:04.791+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:23:04.791+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:23:04.802+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:23:04.821+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:23:04.821+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:23:04.836+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:23:04.836+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:23:04.847+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T03:23:35.103+0000] {processor.py:161} INFO - Started process (PID=306) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:23:35.105+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:23:35.110+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:23:35.109+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:23:35.139+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:23:35.160+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:23:35.160+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:23:35.179+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:23:35.179+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:23:35.191+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.095 seconds
[2024-01-19T03:24:05.450+0000] {processor.py:161} INFO - Started process (PID=313) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:24:05.451+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:24:05.454+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:24:05.454+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:24:05.472+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:24:05.497+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:24:05.496+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:24:05.508+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:24:05.508+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:24:05.518+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T03:24:35.769+0000] {processor.py:161} INFO - Started process (PID=320) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:24:35.772+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:24:35.778+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:24:35.777+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:24:35.798+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:24:35.821+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:24:35.821+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:24:35.835+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:24:35.835+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:24:35.843+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T03:25:06.143+0000] {processor.py:161} INFO - Started process (PID=327) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:25:06.144+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:25:06.148+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:25:06.147+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:25:06.164+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:25:06.187+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:25:06.187+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:25:06.199+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:25:06.199+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:25:06.208+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T03:25:36.472+0000] {processor.py:161} INFO - Started process (PID=334) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:25:36.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:25:36.479+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:25:36.479+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:25:36.502+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:25:36.521+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:25:36.521+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:25:36.546+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:25:36.546+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:25:36.554+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.089 seconds
[2024-01-19T03:35:26.398+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:35:26.400+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:35:26.404+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:35:26.404+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:35:26.429+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:35:26.519+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:35:26.519+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:35:26.534+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:35:26.534+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:35:26.549+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.154 seconds
[2024-01-19T03:35:56.729+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:35:56.731+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:35:56.734+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:35:56.734+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:35:56.751+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:35:56.771+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:35:56.771+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:35:56.783+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:35:56.783+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:35:56.791+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T03:36:27.034+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:36:27.035+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:36:27.040+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:36:27.040+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:36:27.062+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:36:27.087+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:36:27.087+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:36:27.100+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:36:27.100+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:36:27.109+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T03:36:57.394+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:36:57.395+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:36:57.399+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:36:57.399+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:36:57.419+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:36:57.441+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:36:57.441+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:36:57.453+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:36:57.453+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:36:57.460+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T03:37:27.792+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:37:27.793+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:37:27.796+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:37:27.795+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:37:27.821+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:37:27.842+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:37:27.842+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:37:27.856+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:37:27.855+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:37:27.864+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T03:37:58.089+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:37:58.090+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:37:58.095+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:37:58.094+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:37:58.111+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:37:58.136+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:37:58.136+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:37:58.153+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:37:58.153+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:37:58.162+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T03:38:28.442+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:38:28.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:38:28.445+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:38:28.445+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:38:28.461+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:38:28.485+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:38:28.485+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:38:28.504+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:38:28.504+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:38:28.514+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.075 seconds
[2024-01-19T03:38:58.798+0000] {processor.py:161} INFO - Started process (PID=85) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:38:58.799+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:38:58.802+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:38:58.801+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:38:58.816+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:38:58.839+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:38:58.839+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:38:58.854+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:38:58.854+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:38:58.864+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.069 seconds
[2024-01-19T03:39:29.175+0000] {processor.py:161} INFO - Started process (PID=92) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:39:29.176+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:39:29.180+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:39:29.179+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:39:29.197+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:39:29.223+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:39:29.223+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:39:29.251+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:39:29.251+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:39:29.262+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.094 seconds
[2024-01-19T03:39:59.528+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:39:59.531+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:39:59.542+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:39:59.541+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:39:59.563+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:39:59.589+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:39:59.588+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:39:59.604+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:39:59.604+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:39:59.613+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.096 seconds
[2024-01-19T03:40:23.796+0000] {processor.py:161} INFO - Started process (PID=106) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:40:23.798+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:40:23.800+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:40:23.800+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:40:23.817+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:40:23.841+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:40:23.841+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:40:23.853+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:40:23.853+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:40:23.863+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T03:46:36.605+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:46:36.605+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:46:36.607+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:46:36.607+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:46:36.616+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:46:36.631+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:46:36.634+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:46:36.634+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:46:36.638+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:46:36.749+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:46:36.749+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:46:36.763+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:46:36.763+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:46:36.778+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.176 seconds
[2024-01-19T03:47:01.859+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:47:01.861+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:47:01.863+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:47:01.863+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:47:01.874+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:47:01.880+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:47:01.882+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:47:01.882+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:47:01.886+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:47:01.894+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:47:01.894+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:47:01.908+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:47:01.908+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:47:01.919+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-19T03:47:32.219+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:47:32.220+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:47:32.225+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:47:32.224+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:47:32.239+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:47:32.250+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:47:32.256+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:47:32.256+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:47:32.270+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:47:32.290+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:47:32.290+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:47:32.305+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:47:32.305+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:47:32.314+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.101 seconds
[2024-01-19T03:48:02.554+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:48:02.555+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:48:02.559+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:48:02.559+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:48:02.573+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:48:02.586+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:48:02.588+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:48:02.588+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:48:02.592+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:48:02.613+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:48:02.612+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:48:02.625+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:48:02.625+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:48:02.634+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.086 seconds
[2024-01-19T03:48:32.903+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:48:32.904+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:48:32.907+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:48:32.907+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:48:32.917+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:48:32.926+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:48:32.927+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:48:32.928+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:48:32.932+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:48:32.955+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:48:32.955+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:48:32.970+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:48:32.970+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:48:32.980+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T03:49:03.195+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:49:03.196+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:49:03.200+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:49:03.199+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:49:03.213+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:49:03.222+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:49:03.224+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:49:03.224+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:49:03.229+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:49:03.257+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:49:03.256+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:49:03.274+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:49:03.274+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:49:03.284+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.092 seconds
[2024-01-19T03:49:33.515+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:49:33.517+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:49:33.521+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:49:33.521+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:49:33.538+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:49:33.550+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:49:33.551+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:49:33.552+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:49:33.556+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:49:33.577+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:49:33.577+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:49:33.592+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:49:33.592+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:49:33.602+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.094 seconds
[2024-01-19T03:50:03.863+0000] {processor.py:161} INFO - Started process (PID=85) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:03.864+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:50:03.865+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:03.865+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:03.872+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:50:03.879+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:50:03.880+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:50:03.881+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:50:03.883+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:03.899+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:03.899+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:50:03.911+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:03.911+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:50:03.919+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.060 seconds
[2024-01-19T03:50:34.209+0000] {processor.py:161} INFO - Started process (PID=91) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:34.212+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:50:34.218+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:34.217+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:34.238+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:50:34.250+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T03:50:34.252+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T03:50:34.252+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T03:50:34.257+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:34.278+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:34.278+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:50:34.303+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:34.303+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:50:34.313+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.112 seconds
[2024-01-19T03:50:35.228+0000] {processor.py:161} INFO - Started process (PID=92) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:35.229+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:50:35.231+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:35.231+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:35.242+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:50:35.305+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:35.303+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 256, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "p" to address: Name or service not known
[2024-01-19T03:50:35.306+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:35.326+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.104 seconds
[2024-01-19T03:50:36.366+0000] {processor.py:161} INFO - Started process (PID=93) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:36.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:50:36.370+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:50:36.370+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:36.378+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:50:36.393+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.031 seconds
[2024-01-19T03:51:06.748+0000] {processor.py:161} INFO - Started process (PID=100) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:06.750+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:51:06.755+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:06.754+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:06.789+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T03:51:06.798+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:06.796+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 256, in <module>
    ConnectPostgres()
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 41, in ConnectPostgres
    raise e
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 26, in ConnectPostgres
    conn = psycopg2.connect(**params)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.19.0.3), port 5432 failed: FATAL:  password authentication failed for user "postgres"
[2024-01-19T03:51:06.798+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:06.819+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.076 seconds
[2024-01-19T03:51:11.814+0000] {processor.py:161} INFO - Started process (PID=101) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:11.815+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:51:11.817+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:11.817+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:11.833+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:11.860+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:11.860+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:51:11.880+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:11.880+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:51:11.895+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.083 seconds
[2024-01-19T03:51:12.879+0000] {processor.py:161} INFO - Started process (PID=102) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:12.880+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:51:12.882+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:12.882+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:12.894+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:12.917+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:12.917+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:51:12.931+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:12.931+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:51:12.943+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T03:51:14.961+0000] {processor.py:161} INFO - Started process (PID=103) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:14.962+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:51:14.964+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:14.964+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:14.978+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:15.000+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:15.000+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:51:15.015+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:15.015+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:51:15.024+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T03:51:45.344+0000] {processor.py:161} INFO - Started process (PID=110) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:45.345+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:51:45.351+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:45.350+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:45.375+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:51:45.402+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:45.402+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:51:45.416+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:51:45.415+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:51:45.425+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.087 seconds
[2024-01-19T03:52:15.675+0000] {processor.py:161} INFO - Started process (PID=117) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:52:15.676+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:52:15.679+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:52:15.679+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:52:15.707+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:52:15.742+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:52:15.742+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:52:15.770+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:52:15.770+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:52:15.781+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.111 seconds
[2024-01-19T03:52:46.065+0000] {processor.py:161} INFO - Started process (PID=124) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:52:46.066+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:52:46.070+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:52:46.070+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:52:46.088+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:52:46.117+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:52:46.117+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:52:46.142+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:52:46.141+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:52:46.166+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.106 seconds
[2024-01-19T03:53:16.364+0000] {processor.py:161} INFO - Started process (PID=131) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:53:16.365+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:53:16.367+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:53:16.367+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:53:16.382+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:53:16.405+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:53:16.405+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:53:16.420+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:53:16.420+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:53:16.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.069 seconds
[2024-01-19T03:53:46.708+0000] {processor.py:161} INFO - Started process (PID=138) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:53:46.708+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:53:46.710+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:53:46.710+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:53:46.718+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:53:46.733+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:53:46.733+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:53:46.745+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:53:46.745+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:53:46.752+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.047 seconds
[2024-01-19T03:54:16.996+0000] {processor.py:161} INFO - Started process (PID=145) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:54:16.997+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:54:17.001+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:54:17.000+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:54:17.021+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:54:17.048+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:54:17.048+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:54:17.066+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:54:17.066+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:54:17.078+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.088 seconds
[2024-01-19T03:54:47.314+0000] {processor.py:161} INFO - Started process (PID=152) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:54:47.315+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:54:47.318+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:54:47.318+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:54:47.334+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:54:47.360+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:54:47.360+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:54:47.377+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:54:47.377+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:54:47.392+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T03:55:33.479+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:33.480+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:55:33.482+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:33.482+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:33.496+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:33.659+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:33.659+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:55:33.671+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:33.671+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:55:33.683+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.207 seconds
[2024-01-19T03:55:55.712+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:55.713+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:55:55.716+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:55.716+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:55.734+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:55.748+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:55.748+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:55:55.785+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:55.785+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:55:55.819+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.112 seconds
[2024-01-19T03:55:58.744+0000] {processor.py:161} INFO - Started process (PID=44) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:58.745+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:55:58.748+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:58.747+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:58.767+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:55:58.779+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:58.778+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:55:58.793+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:55:58.793+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:55:58.803+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-19T03:56:06.874+0000] {processor.py:161} INFO - Started process (PID=45) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:06.875+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:56:06.877+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:06.877+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:06.894+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:06.915+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:06.915+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:56:06.928+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:06.928+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:56:06.939+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-19T03:56:07.895+0000] {processor.py:161} INFO - Started process (PID=46) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:07.896+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:56:07.898+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:07.898+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:07.914+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:07.932+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:07.932+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:56:07.943+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:07.943+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:56:07.952+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.061 seconds
[2024-01-19T03:56:08.989+0000] {processor.py:161} INFO - Started process (PID=47) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:08.990+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:56:08.993+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:08.993+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:09.012+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:09.033+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:09.033+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:56:09.047+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:09.047+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:56:09.057+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T03:56:39.345+0000] {processor.py:161} INFO - Started process (PID=54) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:39.347+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:56:39.348+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:39.348+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:39.358+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:56:39.373+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:39.373+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:56:39.385+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:56:39.385+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:56:39.394+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.051 seconds
[2024-01-19T03:57:09.716+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:57:09.719+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:57:09.724+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:57:09.723+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:57:09.747+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:57:09.769+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:57:09.769+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:57:09.782+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:57:09.782+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:57:09.790+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T03:57:40.076+0000] {processor.py:161} INFO - Started process (PID=68) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:57:40.077+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:57:40.082+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:57:40.081+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:57:40.105+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:57:40.136+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:57:40.135+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:57:40.160+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:57:40.160+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:57:40.173+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.103 seconds
[2024-01-19T03:58:10.358+0000] {processor.py:161} INFO - Started process (PID=75) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:58:10.358+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:58:10.361+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:58:10.360+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:58:10.372+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:58:10.390+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:58:10.390+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:58:10.404+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:58:10.404+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:58:10.414+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.059 seconds
[2024-01-19T03:58:40.752+0000] {processor.py:161} INFO - Started process (PID=82) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:58:40.754+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:58:40.757+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:58:40.756+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:58:40.772+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:58:40.793+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:58:40.793+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:58:40.805+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:58:40.805+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:58:40.812+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-19T03:59:11.138+0000] {processor.py:161} INFO - Started process (PID=89) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:59:11.139+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:59:11.143+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:59:11.143+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:59:11.162+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:59:11.186+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:59:11.186+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:59:11.199+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:59:11.199+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:59:11.207+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.075 seconds
[2024-01-19T03:59:41.466+0000] {processor.py:161} INFO - Started process (PID=96) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:59:41.472+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T03:59:41.480+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:59:41.480+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:59:41.502+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T03:59:41.551+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:59:41.551+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T03:59:41.598+0000] {logging_mixin.py:188} INFO - [2024-01-19T03:59:41.598+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T03:59:41.639+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.182 seconds
[2024-01-19T04:03:30.903+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:03:30.906+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:03:30.909+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:03:30.909+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:03:30.931+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:03:31.051+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:03:31.051+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:03:31.064+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:03:31.064+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:03:31.074+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.174 seconds
[2024-01-19T04:04:01.179+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:04:01.181+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:04:01.185+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:04:01.185+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:04:01.206+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:04:01.229+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:04:01.229+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:04:01.241+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:04:01.241+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:04:01.250+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T04:04:31.460+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:04:31.461+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:04:31.463+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:04:31.463+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:04:31.480+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:04:31.502+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:04:31.501+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:04:31.514+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:04:31.514+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:04:31.523+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T04:05:01.779+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:05:01.781+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:05:01.783+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:05:01.783+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:05:01.797+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:05:01.819+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:05:01.819+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:05:01.831+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:05:01.831+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:05:01.840+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-19T04:05:32.074+0000] {processor.py:161} INFO - Started process (PID=63) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:05:32.075+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:05:32.078+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:05:32.077+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:05:32.094+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:05:32.126+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:05:32.126+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:05:32.140+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:05:32.140+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:05:32.150+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T04:06:02.368+0000] {processor.py:161} INFO - Started process (PID=70) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:06:02.369+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:06:02.371+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:06:02.371+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:06:02.380+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:06:02.397+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:06:02.396+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:06:02.409+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:06:02.409+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:06:02.417+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.052 seconds
[2024-01-19T04:06:32.689+0000] {processor.py:161} INFO - Started process (PID=77) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:06:32.690+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:06:32.693+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:06:32.692+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:06:32.708+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:06:32.728+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:06:32.728+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:06:32.739+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:06:32.739+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:06:32.747+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-19T04:07:03.051+0000] {processor.py:161} INFO - Started process (PID=84) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:07:03.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:07:03.055+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:07:03.054+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:07:03.069+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:07:03.092+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:07:03.091+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:07:03.105+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:07:03.104+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:07:03.113+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T04:07:33.374+0000] {processor.py:161} INFO - Started process (PID=91) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:07:33.375+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:07:33.377+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:07:33.377+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:07:33.392+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:07:33.413+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:07:33.412+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:07:33.424+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:07:33.424+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:07:33.432+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T04:08:03.739+0000] {processor.py:161} INFO - Started process (PID=98) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:08:03.740+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:08:03.742+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:08:03.742+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:08:03.752+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:08:03.770+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:08:03.770+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:08:03.782+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:08:03.782+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:08:03.790+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.054 seconds
[2024-01-19T04:08:34.049+0000] {processor.py:161} INFO - Started process (PID=105) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:08:34.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:08:34.051+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:08:34.051+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:08:34.058+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:08:34.073+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:08:34.073+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:08:34.084+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:08:34.084+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:08:34.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.045 seconds
[2024-01-19T04:09:04.466+0000] {processor.py:161} INFO - Started process (PID=112) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:09:04.467+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:09:04.469+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:09:04.469+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:09:04.480+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:09:04.497+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:09:04.497+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:09:04.511+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:09:04.511+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:09:04.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T04:09:34.801+0000] {processor.py:161} INFO - Started process (PID=119) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:09:34.803+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:09:34.808+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:09:34.808+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:09:34.838+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:09:34.861+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:09:34.861+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:09:34.873+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:09:34.873+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:09:34.882+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.087 seconds
[2024-01-19T04:10:05.152+0000] {processor.py:161} INFO - Started process (PID=126) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:10:05.153+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:10:05.156+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:10:05.156+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:10:05.173+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:10:05.194+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:10:05.193+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:10:05.207+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:10:05.207+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:10:05.216+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.069 seconds
[2024-01-19T04:10:35.497+0000] {processor.py:161} INFO - Started process (PID=133) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:10:35.498+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:10:35.500+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:10:35.500+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:10:35.510+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:10:35.526+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:10:35.526+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:10:35.537+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:10:35.536+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:10:35.544+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.050 seconds
[2024-01-19T04:11:05.885+0000] {processor.py:161} INFO - Started process (PID=140) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:11:05.886+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:11:05.891+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:11:05.890+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:11:05.913+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:11:05.945+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:11:05.945+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:11:05.963+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:11:05.963+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:11:05.974+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.096 seconds
[2024-01-19T04:11:36.200+0000] {processor.py:161} INFO - Started process (PID=147) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:11:36.201+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:11:36.203+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:11:36.203+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:11:36.214+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:11:36.233+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:11:36.232+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:11:36.246+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:11:36.246+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:11:36.255+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.058 seconds
[2024-01-19T04:12:06.543+0000] {processor.py:161} INFO - Started process (PID=154) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:12:06.545+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:12:06.549+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:12:06.548+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:12:06.568+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:12:06.590+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:12:06.590+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:12:06.602+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:12:06.602+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:12:06.611+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T04:12:36.892+0000] {processor.py:161} INFO - Started process (PID=161) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:12:36.893+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:12:36.896+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:12:36.896+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:12:36.911+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:12:36.931+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:12:36.931+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:12:36.942+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:12:36.942+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:12:36.950+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T04:13:07.246+0000] {processor.py:161} INFO - Started process (PID=168) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:13:07.248+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:13:07.251+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:13:07.251+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:13:07.266+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:13:07.286+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:13:07.286+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:13:07.299+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:13:07.298+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:13:07.306+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-19T04:13:37.619+0000] {processor.py:161} INFO - Started process (PID=175) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:13:37.620+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:13:37.625+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:13:37.624+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:13:37.647+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:13:37.678+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:13:37.678+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:13:37.699+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:13:37.699+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:13:37.719+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.105 seconds
[2024-01-19T04:14:07.944+0000] {processor.py:161} INFO - Started process (PID=182) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:14:07.946+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:14:07.948+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:14:07.948+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:14:07.962+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:14:07.981+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:14:07.981+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:14:07.993+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:14:07.993+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:14:08.000+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.061 seconds
[2024-01-19T04:14:38.235+0000] {processor.py:161} INFO - Started process (PID=189) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:14:38.235+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:14:38.237+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:14:38.237+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:14:38.246+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:14:38.263+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:14:38.263+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:14:38.274+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:14:38.274+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:14:38.281+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.049 seconds
[2024-01-19T04:15:08.552+0000] {processor.py:161} INFO - Started process (PID=196) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:15:08.552+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:15:08.557+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:15:08.557+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:15:08.577+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:15:08.600+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:15:08.599+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:15:08.631+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:15:08.631+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:15:08.641+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.096 seconds
[2024-01-19T04:15:38.932+0000] {processor.py:161} INFO - Started process (PID=203) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:15:38.933+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:15:38.935+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:15:38.934+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:15:38.944+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:15:38.961+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:15:38.961+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:15:38.972+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:15:38.971+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:15:38.980+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.050 seconds
[2024-01-19T04:16:09.267+0000] {processor.py:161} INFO - Started process (PID=210) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:16:09.269+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:16:09.273+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:16:09.273+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:16:09.293+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:16:09.314+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:16:09.314+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:16:09.327+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:16:09.326+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:16:09.335+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T04:16:39.641+0000] {processor.py:161} INFO - Started process (PID=217) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:16:39.642+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:16:39.646+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:16:39.645+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:16:39.666+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:16:39.693+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:16:39.693+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:16:39.707+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:16:39.707+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:16:39.716+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T04:17:09.980+0000] {processor.py:161} INFO - Started process (PID=224) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:17:09.981+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:17:09.985+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:17:09.984+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:17:10.003+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:17:10.025+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:17:10.025+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:17:10.037+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:17:10.037+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:17:10.045+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T04:17:40.280+0000] {processor.py:161} INFO - Started process (PID=231) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:17:40.281+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:17:40.284+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:17:40.283+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:17:40.298+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:17:40.327+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:17:40.326+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:17:40.342+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:17:40.342+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:17:40.354+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T04:18:10.618+0000] {processor.py:161} INFO - Started process (PID=238) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:10.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:18:10.624+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:10.624+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:10.646+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:10.670+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:10.670+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:18:10.683+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:10.683+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:18:10.692+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T04:18:23.773+0000] {processor.py:161} INFO - Started process (PID=239) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:23.774+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:18:23.776+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:23.776+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:23.791+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:23.812+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:23.812+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:18:23.826+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:23.825+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:18:23.836+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T04:18:25.845+0000] {processor.py:161} INFO - Started process (PID=246) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:25.846+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:18:25.848+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:25.848+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:25.860+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:25.882+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:25.882+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:18:25.898+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:25.898+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:18:25.907+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-19T04:18:28.923+0000] {processor.py:161} INFO - Started process (PID=247) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:28.924+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:18:28.927+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:28.927+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:28.947+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:28.968+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:28.968+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:18:28.980+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:28.980+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:18:28.988+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T04:18:29.957+0000] {processor.py:161} INFO - Started process (PID=248) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:29.958+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:18:29.961+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:29.960+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:29.974+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:18:29.980+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:18:29.981+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:18:29.981+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:18:29.981+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:18:29.984+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:18:29.985+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:18:29.985+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:18:29.985+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:18:29.989+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:30.004+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:30.004+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:18:30.014+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:30.014+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:18:30.026+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T04:18:31.030+0000] {processor.py:161} INFO - Started process (PID=249) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:31.031+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:18:31.034+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:31.034+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:31.048+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:18:31.052+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:18:31.053+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:18:31.053+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:18:31.053+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:18:31.056+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:18:31.057+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:18:31.057+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:18:31.057+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:18:31.062+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:31.078+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:31.078+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:18:31.090+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:31.090+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:18:31.099+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T04:18:45.217+0000] {processor.py:161} INFO - Started process (PID=250) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:45.218+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:18:45.222+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:45.221+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:45.239+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:18:45.245+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:18:45.246+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:18:45.246+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:18:45.247+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:18:45.250+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:18:45.251+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:18:45.251+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:18:45.251+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:18:45.257+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:18:45.281+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:45.281+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:18:45.297+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:18:45.297+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:18:45.308+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.097 seconds
[2024-01-19T04:19:15.531+0000] {processor.py:161} INFO - Started process (PID=257) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:19:15.533+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:19:15.535+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:19:15.535+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:19:15.546+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:19:15.551+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:19:15.553+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:19:15.554+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:19:15.557+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:19:15.561+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:19:15.561+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:19:15.561+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:19:15.561+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:19:15.567+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:19:15.585+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:19:15.585+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:19:15.599+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:19:15.599+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:19:15.608+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.080 seconds
[2024-01-19T04:19:45.978+0000] {processor.py:161} INFO - Started process (PID=264) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:19:45.980+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:19:45.988+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:19:45.987+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:19:46.008+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:19:46.014+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:19:46.014+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:19:46.015+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:19:46.015+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:19:46.017+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:19:46.017+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:19:46.018+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:19:46.018+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:19:46.023+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:19:46.050+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:19:46.050+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:19:46.064+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:19:46.064+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:19:46.072+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.104 seconds
[2024-01-19T04:20:16.340+0000] {processor.py:161} INFO - Started process (PID=271) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:20:16.341+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:20:16.347+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:20:16.347+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:20:16.367+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:20:16.372+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:20:16.373+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:20:16.373+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:20:16.373+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:20:16.376+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:20:16.376+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:20:16.376+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:20:16.377+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:20:16.382+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:20:16.412+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:20:16.412+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:20:16.427+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:20:16.427+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:20:16.436+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.104 seconds
[2024-01-19T04:20:46.644+0000] {processor.py:161} INFO - Started process (PID=278) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:20:46.646+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:20:46.649+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:20:46.648+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:20:46.661+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:20:46.667+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:20:46.668+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:20:46.668+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:20:46.668+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:20:46.671+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:20:46.672+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:20:46.672+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:20:46.672+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:20:46.677+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:20:46.692+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:20:46.692+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:20:46.702+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:20:46.702+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:20:46.710+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T04:21:16.985+0000] {processor.py:161} INFO - Started process (PID=285) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:21:16.986+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:21:16.991+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:21:16.990+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:21:17.006+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:21:17.011+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:21:17.011+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:21:17.011+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:21:17.012+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:21:17.014+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:21:17.014+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:21:17.014+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:21:17.014+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:21:17.019+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:21:17.036+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:21:17.036+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:21:17.049+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:21:17.049+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:21:17.058+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T04:21:47.268+0000] {processor.py:161} INFO - Started process (PID=292) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:21:47.270+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:21:47.278+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:21:47.278+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:21:47.291+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:21:47.296+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:21:47.296+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:21:47.297+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:21:47.297+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:21:47.299+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:21:47.300+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:21:47.300+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:21:47.301+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:21:47.306+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:21:47.326+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:21:47.326+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:21:47.342+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:21:47.342+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:21:47.352+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.091 seconds
[2024-01-19T04:21:55.353+0000] {processor.py:161} INFO - Started process (PID=293) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:21:55.355+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:21:55.357+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:21:55.357+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:21:55.372+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:21:55.376+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:21:55.377+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:21:55.378+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:21:55.378+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:21:55.383+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:21:55.383+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:21:55.384+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:21:55.384+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:21:55.388+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:21:55.406+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:21:55.406+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:21:55.418+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:21:55.418+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:21:55.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T04:22:25.772+0000] {processor.py:161} INFO - Started process (PID=300) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:22:25.773+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:22:25.776+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:22:25.776+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:22:25.787+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:22:25.792+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:22:25.793+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:22:25.793+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:22:25.793+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:22:25.796+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:22:25.797+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:22:25.797+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:22:25.797+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:22:25.802+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:22:25.822+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:22:25.822+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:22:25.838+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:22:25.838+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:22:25.848+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.080 seconds
[2024-01-19T04:22:56.091+0000] {processor.py:161} INFO - Started process (PID=307) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:22:56.092+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:22:56.097+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:22:56.096+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:22:56.113+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:22:56.121+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:22:56.122+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:22:56.122+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:22:56.122+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:22:56.126+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:22:56.126+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:22:56.127+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:22:56.127+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:22:56.134+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:22:56.158+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:22:56.157+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:22:56.174+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:22:56.174+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:22:56.185+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.098 seconds
[2024-01-19T04:29:52.454+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:29:52.455+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:29:52.457+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:29:52.457+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:29:52.468+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:29:52.467+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    create_table >> insert_data
NameError: name 'insert_data' is not defined
[2024-01-19T04:29:52.468+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:29:52.483+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.032 seconds
[2024-01-19T04:29:54.522+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:29:54.525+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:29:54.529+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:29:54.529+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:29:54.546+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:29:54.545+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    create_table >> insert_data
NameError: name 'insert_data' is not defined
[2024-01-19T04:29:54.547+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:29:54.582+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T04:29:58.556+0000] {processor.py:161} INFO - Started process (PID=38) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:29:58.557+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:29:58.560+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:29:58.559+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:29:58.594+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:29:58.733+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:29:58.732+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:company_profile_pipeline
[2024-01-19T04:29:58.738+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:29:58.738+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:company_profile_pipeline
[2024-01-19T04:29:58.741+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:29:58.741+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:company_profile_pipeline
[2024-01-19T04:29:58.741+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:29:58.741+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:29:58.747+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:29:58.747+0000] {dag.py:3055} INFO - Creating ORM DAG for company_profile_pipeline
[2024-01-19T04:29:58.754+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:29:58.754+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:29:58.764+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.211 seconds
[2024-01-19T04:29:59.654+0000] {processor.py:161} INFO - Started process (PID=39) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:29:59.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:29:59.658+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:29:59.657+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:29:59.683+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:29:59.697+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:29:59.697+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:29:59.712+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:29:59.712+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:29:59.722+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T04:30:00.675+0000] {processor.py:161} INFO - Started process (PID=40) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:30:00.676+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:30:00.679+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:30:00.679+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:30:00.699+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:30:00.713+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:30:00.713+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:30:00.730+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:30:00.730+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:30:00.740+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-19T04:30:31.116+0000] {processor.py:161} INFO - Started process (PID=47) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:30:31.117+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:30:31.120+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:30:31.120+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:30:31.137+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:30:31.157+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:30:31.157+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:30:31.170+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:30:31.170+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:30:31.178+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T04:31:01.462+0000] {processor.py:161} INFO - Started process (PID=54) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:31:01.463+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:31:01.467+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:31:01.466+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:31:01.485+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:31:01.504+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:31:01.504+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:31:01.516+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:31:01.516+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:31:01.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T04:31:31.835+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:31:31.837+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:31:31.842+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:31:31.842+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:31:31.867+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:31:31.889+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:31:31.889+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:31:31.902+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:31:31.902+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:31:31.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T04:32:02.185+0000] {processor.py:161} INFO - Started process (PID=68) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:32:02.187+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:32:02.191+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:32:02.190+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:32:02.211+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:32:02.239+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:32:02.239+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:32:02.251+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:32:02.251+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:32:02.260+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T04:32:32.526+0000] {processor.py:161} INFO - Started process (PID=75) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:32:32.528+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:32:32.531+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:32:32.531+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:32:32.551+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:32:32.578+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:32:32.578+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:32:32.592+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:32:32.592+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:32:32.602+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T04:33:02.883+0000] {processor.py:161} INFO - Started process (PID=82) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:33:02.884+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:33:02.887+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:33:02.887+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:33:02.906+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:33:02.929+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:33:02.929+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:33:02.944+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:33:02.944+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:33:02.953+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.075 seconds
[2024-01-19T04:33:33.218+0000] {processor.py:161} INFO - Started process (PID=89) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:33:33.220+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:33:33.223+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:33:33.223+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:33:33.239+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:33:33.260+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:33:33.260+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:33:33.272+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:33:33.271+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:33:33.280+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T04:34:03.568+0000] {processor.py:161} INFO - Started process (PID=96) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:34:03.569+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:34:03.572+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:34:03.572+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:34:03.591+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:34:03.613+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:34:03.613+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:34:03.628+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:34:03.628+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:34:03.638+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T04:34:33.919+0000] {processor.py:161} INFO - Started process (PID=103) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:34:33.920+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:34:33.923+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:34:33.923+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:34:33.942+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:34:33.967+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:34:33.967+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:34:33.983+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:34:33.983+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:34:33.992+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T04:35:04.311+0000] {processor.py:161} INFO - Started process (PID=110) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:35:04.312+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:35:04.316+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:35:04.316+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:35:04.335+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:35:04.358+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:35:04.358+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:35:04.370+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:35:04.370+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:35:04.378+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T04:35:34.651+0000] {processor.py:161} INFO - Started process (PID=117) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:35:34.653+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:35:34.656+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:35:34.656+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:35:34.678+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:35:34.702+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:35:34.702+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:35:34.717+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:35:34.716+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:35:34.727+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.083 seconds
[2024-01-19T04:36:05.036+0000] {processor.py:161} INFO - Started process (PID=124) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:36:05.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:36:05.043+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:36:05.043+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:36:05.064+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:36:05.088+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:36:05.088+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:36:05.101+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:36:05.101+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:36:05.110+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-19T04:36:35.337+0000] {processor.py:161} INFO - Started process (PID=131) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:36:35.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:36:35.340+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:36:35.340+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:36:35.355+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:36:35.374+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:36:35.373+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:36:35.385+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:36:35.385+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:36:35.394+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T04:37:05.696+0000] {processor.py:161} INFO - Started process (PID=138) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:37:05.697+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:37:05.702+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:37:05.701+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:37:05.724+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:37:05.743+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:37:05.743+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:37:05.754+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:37:05.754+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:37:05.762+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T04:37:36.086+0000] {processor.py:161} INFO - Started process (PID=145) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:37:36.088+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:37:36.091+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:37:36.090+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:37:36.106+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:37:36.127+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:37:36.127+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:37:36.137+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:37:36.137+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:37:36.145+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-19T04:38:06.458+0000] {processor.py:161} INFO - Started process (PID=152) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:38:06.460+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:38:06.463+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:38:06.462+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:38:06.478+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:38:06.498+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:38:06.498+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:38:06.509+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:38:06.509+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:38:06.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-19T04:38:36.851+0000] {processor.py:161} INFO - Started process (PID=159) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:38:36.852+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:38:36.855+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:38:36.855+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:38:36.871+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:38:36.892+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:38:36.891+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:38:36.903+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:38:36.903+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:38:36.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-19T04:39:07.229+0000] {processor.py:161} INFO - Started process (PID=166) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:39:07.230+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:39:07.235+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:39:07.234+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:39:07.254+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:39:07.277+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:39:07.277+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:39:07.288+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:39:07.288+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T04:39:07.296+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T04:39:37.542+0000] {processor.py:161} INFO - Started process (PID=195) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:39:37.543+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:39:37.546+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:39:37.546+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:39:37.560+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:39:37.587+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:39:37.587+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:39:37.603+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:39:37.603+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:39:37.622+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.083 seconds
[2024-01-19T04:40:07.853+0000] {processor.py:161} INFO - Started process (PID=202) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:40:07.853+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:40:07.855+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:40:07.855+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:40:07.866+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:40:07.885+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:40:07.885+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:40:07.898+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:40:07.898+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:40:07.907+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.057 seconds
[2024-01-19T04:40:38.241+0000] {processor.py:161} INFO - Started process (PID=209) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:40:38.243+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:40:38.247+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:40:38.247+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:40:38.268+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:40:38.308+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:40:38.308+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:40:38.333+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:40:38.332+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:40:38.345+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.110 seconds
[2024-01-19T04:41:08.584+0000] {processor.py:161} INFO - Started process (PID=216) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:08.586+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:41:08.589+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:08.588+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:08.610+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:08.634+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:08.633+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:41:08.649+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:08.648+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:41:08.660+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T04:41:33.892+0000] {processor.py:161} INFO - Started process (PID=223) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:33.893+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:41:33.898+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:33.897+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:33.924+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:33.955+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:33.955+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:41:33.971+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:33.971+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:41:33.981+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.095 seconds
[2024-01-19T04:41:44.019+0000] {processor.py:161} INFO - Started process (PID=224) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:44.021+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:41:44.023+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:44.023+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:44.042+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:44.062+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:44.062+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:41:44.074+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:44.074+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:41:44.082+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T04:41:48.135+0000] {processor.py:161} INFO - Started process (PID=231) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:48.136+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:41:48.139+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:48.138+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:48.145+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:48.144+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 839, in exec_module
  File "<frozen importlib._bootstrap_external>", line 976, in get_code
  File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 254
    branch_check_existing_profile >> [insert_data, update_data]\\
                                                                ^
SyntaxError: unexpected character after line continuation character
[2024-01-19T04:41:48.145+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:48.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.031 seconds
[2024-01-19T04:41:49.179+0000] {processor.py:161} INFO - Started process (PID=232) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:49.181+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:41:49.183+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:49.183+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:49.200+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:49.199+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    c
NameError: name 'c' is not defined
[2024-01-19T04:41:49.200+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:49.215+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.040 seconds
[2024-01-19T04:41:50.226+0000] {processor.py:161} INFO - Started process (PID=233) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:50.227+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:41:50.229+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:50.228+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:50.243+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:50.242+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    ch
NameError: name 'ch' is not defined
[2024-01-19T04:41:50.244+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:50.262+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.041 seconds
[2024-01-19T04:41:51.268+0000] {processor.py:161} INFO - Started process (PID=234) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:51.269+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:41:51.271+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:51.271+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:51.290+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:51.288+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257, in <module>
    CheckTableExisting()
TypeError: CheckTableExisting() missing 1 required positional argument: 'table_name'
[2024-01-19T04:41:51.290+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:51.324+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.059 seconds
[2024-01-19T04:41:52.313+0000] {processor.py:161} INFO - Started process (PID=235) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:52.314+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:41:52.317+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:52.317+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:52.331+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:41:52.336+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:41:52.337+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:41:52.338+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:41:52.338+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:41:52.345+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:52.367+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:52.367+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:41:52.382+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:52.382+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:41:52.405+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.095 seconds
[2024-01-19T04:41:53.388+0000] {processor.py:161} INFO - Started process (PID=236) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:53.390+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:41:53.394+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:53.394+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:53.412+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:41:53.416+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:41:53.417+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:41:53.418+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:41:53.418+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:41:53.424+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:53.442+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:53.442+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:41:53.456+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:53.456+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:41:53.465+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.085 seconds
[2024-01-19T04:41:54.456+0000] {processor.py:161} INFO - Started process (PID=237) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:54.456+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:41:54.458+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:54.458+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:54.462+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:54.462+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 839, in exec_module
  File "<frozen importlib._bootstrap_external>", line 976, in get_code
  File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 257
    CheckTableExisting(table_name=)
                                  ^
SyntaxError: invalid syntax
[2024-01-19T04:41:54.463+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:54.479+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.027 seconds
[2024-01-19T04:41:55.507+0000] {processor.py:161} INFO - Started process (PID=238) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:55.509+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:41:55.512+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:55.512+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:55.526+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:41:55.530+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:41:55.531+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:41:55.532+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:41:55.532+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:41:55.538+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:55.559+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:55.559+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:41:55.570+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:55.570+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:41:55.581+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-19T04:41:56.536+0000] {processor.py:161} INFO - Started process (PID=239) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:56.537+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:41:56.540+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:56.540+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:56.553+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:41:56.558+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:41:56.558+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:41:56.559+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:41:56.559+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:41:56.565+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:56.587+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:56.587+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:41:56.600+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:56.600+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:41:56.611+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.080 seconds
[2024-01-19T04:41:57.629+0000] {processor.py:161} INFO - Started process (PID=240) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:57.630+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:41:57.632+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:57.632+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:57.646+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:41:57.650+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:41:57.650+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:41:57.651+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:41:57.651+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:41:57.656+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:57.674+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:57.674+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:41:57.686+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:57.686+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:41:57.695+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-19T04:41:58.660+0000] {processor.py:161} INFO - Started process (PID=241) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:58.661+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:41:58.663+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:58.663+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:58.677+0000] {logging_mixin.py:188} INFO - Connecting to the PostgreSQL database...
[2024-01-19T04:41:58.682+0000] {logging_mixin.py:188} INFO - PostgreSQL database version:
[2024-01-19T04:41:58.683+0000] {logging_mixin.py:188} INFO - Successfully connect to Postgresql
[2024-01-19T04:41:58.684+0000] {logging_mixin.py:188} INFO - ('PostgreSQL 13.13 (Debian 13.13-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit',)
[2024-01-19T04:41:58.685+0000] {logging_mixin.py:188} INFO - Checking existing table...
[2024-01-19T04:41:58.690+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:41:58.710+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:58.710+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:41:58.723+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:41:58.723+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:41:58.733+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T04:55:02.166+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:55:02.167+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:55:02.169+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:55:02.169+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:55:02.184+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:55:02.321+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:55:02.321+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:55:02.334+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:55:02.334+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:55:02.346+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.183 seconds
[2024-01-19T04:55:32.500+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:55:32.501+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:55:32.504+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:55:32.504+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:55:32.525+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:55:32.554+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:55:32.554+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:55:32.582+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:55:32.582+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:55:32.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.097 seconds
[2024-01-19T04:56:02.975+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:56:02.977+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:56:02.981+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:56:02.980+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:56:02.999+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:56:03.022+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:56:03.022+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:56:03.036+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:56:03.036+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:56:03.047+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T04:56:33.348+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:56:33.348+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:56:33.352+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:56:33.351+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:56:33.371+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:56:33.394+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:56:33.394+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:56:33.407+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:56:33.407+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:56:33.418+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T04:57:03.675+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:03.676+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:57:03.680+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:03.679+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:03.699+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:03.724+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:03.724+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:57:03.741+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:03.741+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:57:03.751+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.083 seconds
[2024-01-19T04:57:08.766+0000] {processor.py:161} INFO - Started process (PID=65) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:08.768+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:57:08.770+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:08.770+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:08.789+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:08.810+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:08.810+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:57:08.824+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:08.824+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:57:08.837+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.076 seconds
[2024-01-19T04:57:10.841+0000] {processor.py:161} INFO - Started process (PID=66) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:10.842+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:57:10.844+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:10.844+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:10.861+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:10.883+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:10.883+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:57:10.895+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:10.895+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:57:10.906+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T04:57:11.876+0000] {processor.py:161} INFO - Started process (PID=67) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:11.877+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:57:11.878+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:11.878+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:11.888+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:11.904+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:11.904+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:57:11.915+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:11.915+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:57:11.924+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.050 seconds
[2024-01-19T04:57:12.945+0000] {processor.py:161} INFO - Started process (PID=68) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:12.946+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:57:12.948+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:12.948+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:12.963+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:12.985+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:12.985+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:57:12.998+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:12.997+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:57:13.007+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T04:57:43.318+0000] {processor.py:161} INFO - Started process (PID=75) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:43.319+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:57:43.324+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:43.323+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:43.346+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:57:43.370+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:43.370+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:57:43.384+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:57:43.384+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:57:43.393+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T04:58:13.705+0000] {processor.py:161} INFO - Started process (PID=82) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:58:13.706+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:58:13.711+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:58:13.711+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:58:13.734+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:58:13.770+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:58:13.770+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:58:13.787+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:58:13.787+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:58:13.798+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.100 seconds
[2024-01-19T04:58:44.033+0000] {processor.py:161} INFO - Started process (PID=88) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:58:44.035+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:58:44.041+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:58:44.040+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:58:44.066+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:58:44.087+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:58:44.087+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:58:44.100+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:58:44.100+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:58:44.109+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.083 seconds
[2024-01-19T04:59:14.329+0000] {processor.py:161} INFO - Started process (PID=95) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:59:14.331+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:59:14.333+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:59:14.333+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:59:14.349+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:59:14.368+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:59:14.368+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:59:14.380+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:59:14.379+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:59:14.388+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-19T04:59:44.657+0000] {processor.py:161} INFO - Started process (PID=102) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:59:44.659+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T04:59:44.663+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:59:44.662+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:59:44.683+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T04:59:44.709+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:59:44.708+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T04:59:44.724+0000] {logging_mixin.py:188} INFO - [2024-01-19T04:59:44.724+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T04:59:44.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.083 seconds
[2024-01-19T05:00:49.419+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:00:49.420+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T05:00:49.422+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:00:49.422+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:00:49.436+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:00:49.583+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:00:49.582+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T05:00:49.611+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:00:49.611+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T05:00:49.622+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.206 seconds
[2024-01-19T05:01:19.721+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:01:19.722+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T05:01:19.726+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:01:19.726+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:01:19.748+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:01:19.772+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:01:19.772+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T05:01:19.785+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:01:19.785+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T05:01:19.794+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-19T05:01:41.935+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:01:41.936+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T05:01:41.939+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:01:41.939+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:01:41.957+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:01:41.984+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:01:41.984+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T05:01:42.001+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:01:42.001+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T05:01:42.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T05:02:12.295+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:02:12.296+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T05:02:12.301+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:02:12.300+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:02:12.321+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:02:12.346+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:02:12.346+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T05:02:12.381+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:02:12.381+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T05:02:12.391+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.102 seconds
[2024-01-19T05:02:42.689+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:02:42.690+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T05:02:42.694+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:02:42.693+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:02:42.713+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:02:42.754+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:02:42.754+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T05:02:42.770+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:02:42.770+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T05:02:42.782+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.097 seconds
[2024-01-19T05:03:13.014+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:03:13.015+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T05:03:13.021+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:03:13.020+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:03:13.046+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:03:13.100+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:03:13.100+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T05:03:13.119+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:03:13.119+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T05:03:13.138+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.130 seconds
[2024-01-19T05:03:17.051+0000] {processor.py:161} INFO - Started process (PID=72) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:03:17.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T05:03:17.054+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:03:17.054+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:03:17.069+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:03:17.091+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:03:17.091+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T05:03:17.115+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:03:17.115+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T05:03:17.142+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.093 seconds
[2024-01-19T05:03:18.061+0000] {processor.py:161} INFO - Started process (PID=73) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:03:18.062+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T05:03:18.064+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:03:18.064+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:03:18.075+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T05:03:18.094+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:03:18.094+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T05:03:18.106+0000] {logging_mixin.py:188} INFO - [2024-01-19T05:03:18.106+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T05:03:18.116+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.057 seconds
[2024-01-19T06:46:07.758+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:46:07.759+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:46:07.761+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:46:07.761+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:46:07.795+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:46:07.911+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:46:07.911+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:46:07.927+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:46:07.927+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:46:07.942+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.189 seconds
[2024-01-19T06:46:38.072+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:46:38.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:46:38.077+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:46:38.077+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:46:38.097+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:46:38.115+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:46:38.115+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:46:38.128+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:46:38.127+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:46:38.135+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-19T06:47:08.430+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:47:08.431+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:47:08.434+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:47:08.433+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:47:08.450+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:47:08.480+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:47:08.480+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:47:08.497+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:47:08.497+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:47:08.508+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T06:47:38.699+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:47:38.700+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:47:38.702+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:47:38.701+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:47:38.711+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:47:38.727+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:47:38.727+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:47:38.739+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:47:38.739+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:47:38.747+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.051 seconds
[2024-01-19T06:48:09.024+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:48:09.026+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:48:09.030+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:48:09.029+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:48:09.048+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:48:09.073+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:48:09.073+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:48:09.090+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:48:09.090+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:48:09.100+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-19T06:48:39.326+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:48:39.327+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:48:39.330+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:48:39.329+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:48:39.343+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:48:39.371+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:48:39.370+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:48:39.386+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:48:39.386+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:48:39.396+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T06:49:09.662+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:49:09.663+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:49:09.666+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:49:09.665+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:49:09.681+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:49:09.713+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:49:09.713+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:49:09.731+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:49:09.731+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:49:09.743+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.085 seconds
[2024-01-19T06:49:39.975+0000] {processor.py:161} INFO - Started process (PID=85) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:49:39.976+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:49:39.979+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:49:39.978+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:49:39.993+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:49:40.013+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:49:40.013+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:49:40.029+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:49:40.029+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:49:40.039+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T06:50:10.319+0000] {processor.py:161} INFO - Started process (PID=92) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:50:10.320+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:50:10.323+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:50:10.323+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:50:10.343+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:50:10.371+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:50:10.371+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:50:10.388+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:50:10.388+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:50:10.399+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.086 seconds
[2024-01-19T06:50:40.672+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:50:40.674+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:50:40.676+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:50:40.675+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:50:40.691+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:50:40.714+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:50:40.714+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:50:40.727+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:50:40.727+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:50:40.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T06:51:11.103+0000] {processor.py:161} INFO - Started process (PID=106) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:51:11.105+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:51:11.110+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:51:11.109+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:51:11.131+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:51:11.154+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:51:11.154+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:51:11.168+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:51:11.168+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:51:11.178+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T06:51:41.496+0000] {processor.py:161} INFO - Started process (PID=113) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:51:41.498+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:51:41.504+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:51:41.503+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:51:41.528+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:51:41.561+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:51:41.561+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:51:41.581+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:51:41.581+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:51:41.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.103 seconds
[2024-01-19T06:52:11.838+0000] {processor.py:161} INFO - Started process (PID=120) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:52:11.839+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:52:11.843+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:52:11.842+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:52:11.861+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:52:11.884+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:52:11.884+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:52:11.899+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:52:11.899+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:52:11.908+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T06:52:42.235+0000] {processor.py:161} INFO - Started process (PID=127) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:52:42.237+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:52:42.241+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:52:42.240+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:52:42.261+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:52:42.281+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:52:42.281+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:52:42.293+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:52:42.293+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:52:42.301+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T06:53:12.614+0000] {processor.py:161} INFO - Started process (PID=134) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:53:12.615+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:53:12.618+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:53:12.617+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:53:12.633+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:53:12.654+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:53:12.654+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:53:12.665+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:53:12.665+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:53:12.673+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-19T06:53:42.928+0000] {processor.py:161} INFO - Started process (PID=141) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:53:42.929+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:53:42.932+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:53:42.932+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:53:42.950+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:53:42.979+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:53:42.979+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:53:42.996+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:53:42.996+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:53:43.007+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T06:54:13.298+0000] {processor.py:161} INFO - Started process (PID=148) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:54:13.299+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:54:13.302+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:54:13.302+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:54:13.321+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:54:13.346+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:54:13.346+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:54:13.360+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:54:13.360+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:54:13.370+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T06:54:43.610+0000] {processor.py:161} INFO - Started process (PID=155) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:54:43.611+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:54:43.613+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:54:43.613+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:54:43.627+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:54:43.644+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:54:43.644+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:54:43.655+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:54:43.655+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:54:43.663+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.058 seconds
[2024-01-19T06:55:13.894+0000] {processor.py:161} INFO - Started process (PID=162) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:55:13.895+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:55:13.898+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:55:13.898+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:55:13.917+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:55:13.943+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:55:13.943+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:55:13.971+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:55:13.971+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:55:13.981+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.091 seconds
[2024-01-19T06:55:44.185+0000] {processor.py:161} INFO - Started process (PID=169) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:55:44.186+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:55:44.190+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:55:44.190+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:55:44.209+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:55:44.233+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:55:44.233+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:55:44.247+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:55:44.246+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:55:44.255+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T06:56:14.527+0000] {processor.py:161} INFO - Started process (PID=176) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:56:14.527+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:56:14.530+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:56:14.529+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:56:14.543+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:56:14.565+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:56:14.565+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:56:14.579+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:56:14.579+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:56:14.588+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-19T06:56:44.863+0000] {processor.py:161} INFO - Started process (PID=183) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:56:44.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:56:44.869+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:56:44.869+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:56:44.891+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:56:44.921+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:56:44.921+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:56:44.939+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:56:44.939+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:56:44.951+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.092 seconds
[2024-01-19T06:57:15.198+0000] {processor.py:161} INFO - Started process (PID=190) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:57:15.199+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:57:15.201+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:57:15.200+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:57:15.213+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:57:15.233+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:57:15.232+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:57:15.245+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:57:15.245+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:57:15.254+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.059 seconds
[2024-01-19T06:57:45.555+0000] {processor.py:161} INFO - Started process (PID=197) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:57:45.556+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:57:45.562+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:57:45.561+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:57:45.589+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:57:45.627+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:57:45.627+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:57:45.642+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:57:45.641+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:57:45.651+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.103 seconds
[2024-01-19T06:58:15.934+0000] {processor.py:161} INFO - Started process (PID=204) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:58:15.936+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:58:15.938+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:58:15.938+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:58:15.955+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:58:15.975+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:58:15.975+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:58:15.987+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:58:15.987+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:58:15.995+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T06:58:46.263+0000] {processor.py:161} INFO - Started process (PID=211) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:58:46.264+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:58:46.270+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:58:46.269+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:58:46.291+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:58:46.310+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:58:46.310+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:58:46.323+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:58:46.323+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:58:46.331+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T06:59:16.587+0000] {processor.py:161} INFO - Started process (PID=218) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:59:16.588+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:59:16.591+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:59:16.590+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:59:16.607+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:59:16.628+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:59:16.628+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:59:16.642+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:59:16.642+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:59:16.652+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-19T06:59:46.948+0000] {processor.py:161} INFO - Started process (PID=225) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:59:46.949+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T06:59:46.953+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:59:46.952+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:59:46.979+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T06:59:47.009+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:59:47.009+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T06:59:47.024+0000] {logging_mixin.py:188} INFO - [2024-01-19T06:59:47.024+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T06:59:47.035+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.096 seconds
[2024-01-19T07:00:17.283+0000] {processor.py:161} INFO - Started process (PID=232) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:00:17.284+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:00:17.289+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:00:17.288+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:00:17.310+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:00:17.329+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:00:17.329+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:00:17.342+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:00:17.342+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:00:17.352+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.075 seconds
[2024-01-19T07:00:47.657+0000] {processor.py:161} INFO - Started process (PID=239) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:00:47.658+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:00:47.661+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:00:47.660+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:00:47.677+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:00:47.696+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:00:47.696+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:00:47.707+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:00:47.707+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:00:47.716+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-19T07:01:18.033+0000] {processor.py:161} INFO - Started process (PID=246) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:01:18.035+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:01:18.038+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:01:18.038+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:01:18.059+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:01:18.086+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:01:18.086+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:01:18.101+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:01:18.101+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:01:18.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T07:01:48.389+0000] {processor.py:161} INFO - Started process (PID=253) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:01:48.390+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:01:48.394+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:01:48.394+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:01:48.414+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:01:48.435+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:01:48.434+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:01:48.448+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:01:48.448+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:01:48.456+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T07:02:18.763+0000] {processor.py:161} INFO - Started process (PID=260) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:02:18.765+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:02:18.770+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:02:18.769+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:02:18.795+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:02:18.817+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:02:18.817+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:02:18.833+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:02:18.832+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:02:18.842+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.086 seconds
[2024-01-19T07:02:49.115+0000] {processor.py:161} INFO - Started process (PID=267) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:02:49.117+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:02:49.122+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:02:49.122+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:02:49.145+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:02:49.171+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:02:49.171+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:02:49.186+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:02:49.186+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:02:49.196+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.087 seconds
[2024-01-19T07:03:19.455+0000] {processor.py:161} INFO - Started process (PID=274) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:03:19.456+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:03:19.460+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:03:19.459+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:03:19.478+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:03:19.498+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:03:19.498+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:03:19.510+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:03:19.510+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:03:19.518+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.069 seconds
[2024-01-19T07:03:49.831+0000] {processor.py:161} INFO - Started process (PID=281) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:03:49.833+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:03:49.836+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:03:49.835+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:03:49.854+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:03:49.878+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:03:49.878+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:03:49.893+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:03:49.893+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:03:49.903+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T07:04:20.222+0000] {processor.py:161} INFO - Started process (PID=288) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:04:20.224+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:04:20.228+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:04:20.227+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:04:20.251+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:04:20.295+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:04:20.295+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:04:20.321+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:04:20.321+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:04:20.330+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.113 seconds
[2024-01-19T07:04:50.536+0000] {processor.py:161} INFO - Started process (PID=295) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:04:50.537+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:04:50.542+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:04:50.542+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:04:50.564+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:04:50.590+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:04:50.590+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:04:50.603+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:04:50.603+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:04:50.612+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.083 seconds
[2024-01-19T07:05:20.890+0000] {processor.py:161} INFO - Started process (PID=302) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:05:20.891+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:05:20.894+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:05:20.893+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:05:20.910+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:05:20.930+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:05:20.930+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:05:20.943+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:05:20.943+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:05:20.952+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T07:05:51.199+0000] {processor.py:161} INFO - Started process (PID=309) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:05:51.201+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:05:51.203+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:05:51.203+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:05:51.218+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:05:51.240+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:05:51.240+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:05:51.253+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:05:51.253+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:05:51.262+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T07:06:21.492+0000] {processor.py:161} INFO - Started process (PID=316) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:06:21.493+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:06:21.495+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:06:21.495+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:06:21.508+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:06:21.529+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:06:21.529+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:06:21.543+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:06:21.543+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:06:21.552+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T07:06:51.839+0000] {processor.py:161} INFO - Started process (PID=322) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:06:51.840+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:06:51.843+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:06:51.842+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:06:51.856+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:06:51.877+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:06:51.877+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:06:51.892+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:06:51.892+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:06:51.903+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T07:07:22.218+0000] {processor.py:161} INFO - Started process (PID=329) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:07:22.223+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:07:22.231+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:07:22.230+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:07:22.251+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:07:22.279+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:07:22.279+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:07:22.296+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:07:22.296+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:07:22.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.118 seconds
[2024-01-19T07:07:52.551+0000] {processor.py:161} INFO - Started process (PID=336) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:07:52.552+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:07:52.556+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:07:52.556+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:07:52.575+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:07:52.596+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:07:52.596+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:07:52.609+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:07:52.609+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:07:52.618+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T07:08:22.929+0000] {processor.py:161} INFO - Started process (PID=343) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:08:22.930+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:08:22.933+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:08:22.932+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:08:22.955+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:08:22.990+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:08:22.989+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:08:23.010+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:08:23.010+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:08:23.021+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.097 seconds
[2024-01-19T07:08:53.281+0000] {processor.py:161} INFO - Started process (PID=350) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:08:53.282+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:08:53.287+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:08:53.287+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:08:53.311+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:08:53.346+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:08:53.346+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:08:53.362+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:08:53.362+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:08:53.372+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.098 seconds
[2024-01-19T07:09:24.132+0000] {processor.py:161} INFO - Started process (PID=357) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:09:24.134+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:09:24.138+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:09:24.137+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:09:24.160+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:09:24.203+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:09:24.203+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:09:24.219+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:09:24.219+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:09:24.230+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.104 seconds
[2024-01-19T07:10:13.766+0000] {processor.py:161} INFO - Started process (PID=364) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:10:13.769+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:10:13.777+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:10:13.776+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:10:13.799+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:10:13.821+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:10:13.821+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:10:13.843+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:10:13.843+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:10:13.853+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.104 seconds
[2024-01-19T07:11:58.202+0000] {processor.py:161} INFO - Started process (PID=371) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:11:58.202+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:11:58.204+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:11:58.204+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:11:58.215+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:11:58.253+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:11:58.253+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:11:58.269+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:11:58.269+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:11:58.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.083 seconds
[2024-01-19T07:48:33.482+0000] {processor.py:161} INFO - Started process (PID=378) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:48:33.483+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T07:48:33.486+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:48:33.486+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:48:33.499+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T07:48:33.519+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:48:33.519+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T07:48:33.533+0000] {logging_mixin.py:188} INFO - [2024-01-19T07:48:33.533+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T07:48:33.544+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-19T08:01:28.517+0000] {processor.py:161} INFO - Started process (PID=385) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:01:28.518+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:01:28.522+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:01:28.521+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:01:28.539+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:01:28.564+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:01:28.564+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:01:28.580+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:01:28.579+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:01:28.589+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.075 seconds
[2024-01-19T08:01:59.046+0000] {processor.py:161} INFO - Started process (PID=392) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:01:59.048+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:01:59.054+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:01:59.053+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:01:59.076+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:01:59.096+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:01:59.096+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:01:59.110+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:01:59.110+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:01:59.119+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.080 seconds
[2024-01-19T08:02:29.442+0000] {processor.py:161} INFO - Started process (PID=399) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:02:29.444+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:02:29.450+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:02:29.450+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:02:29.474+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:02:29.495+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:02:29.495+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:02:29.509+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:02:29.508+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:02:29.518+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.083 seconds
[2024-01-19T08:02:59.802+0000] {processor.py:161} INFO - Started process (PID=406) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:02:59.805+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:02:59.811+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:02:59.810+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:02:59.834+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:02:59.854+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:02:59.854+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:02:59.867+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:02:59.867+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:02:59.875+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.080 seconds
[2024-01-19T08:03:30.169+0000] {processor.py:161} INFO - Started process (PID=413) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:03:30.171+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:03:30.175+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:03:30.175+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:03:30.193+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:03:30.218+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:03:30.218+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:03:30.233+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:03:30.233+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:03:30.242+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-19T08:04:00.532+0000] {processor.py:161} INFO - Started process (PID=420) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:04:00.534+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:04:00.538+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:04:00.537+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:04:00.555+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:04:00.578+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:04:00.578+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:04:00.594+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:04:00.594+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:04:00.604+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T08:04:30.949+0000] {processor.py:161} INFO - Started process (PID=427) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:04:30.952+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:04:30.957+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:04:30.956+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:04:30.976+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:04:31.002+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:04:31.002+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:04:31.016+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:04:31.016+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:04:31.025+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.084 seconds
[2024-01-19T08:05:01.331+0000] {processor.py:161} INFO - Started process (PID=434) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:05:01.334+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:05:01.339+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:05:01.339+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:05:01.363+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:05:01.383+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:05:01.382+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:05:01.395+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:05:01.395+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:05:01.404+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T08:05:31.706+0000] {processor.py:161} INFO - Started process (PID=441) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:05:31.707+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:05:31.711+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:05:31.710+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:05:31.728+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:05:31.750+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:05:31.750+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:05:31.764+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:05:31.764+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:05:31.774+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.076 seconds
[2024-01-19T08:06:02.093+0000] {processor.py:161} INFO - Started process (PID=448) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:06:02.095+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:06:02.100+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:06:02.100+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:06:02.121+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:06:02.160+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:06:02.160+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:06:02.175+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:06:02.175+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:06:02.184+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.098 seconds
[2024-01-19T08:06:32.513+0000] {processor.py:161} INFO - Started process (PID=455) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:06:32.514+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:06:32.518+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:06:32.518+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:06:32.533+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:06:32.553+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:06:32.553+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:06:32.566+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:06:32.566+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:06:32.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T08:07:02.837+0000] {processor.py:161} INFO - Started process (PID=462) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:07:02.838+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:07:02.840+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:07:02.839+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:07:02.849+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:07:02.865+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:07:02.865+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:07:02.876+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:07:02.876+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:07:02.884+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.049 seconds
[2024-01-19T08:07:33.167+0000] {processor.py:161} INFO - Started process (PID=469) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:07:33.169+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:07:33.171+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:07:33.171+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:07:33.185+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:07:33.203+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:07:33.203+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:07:33.217+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:07:33.217+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:07:33.225+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.061 seconds
[2024-01-19T08:08:03.502+0000] {processor.py:161} INFO - Started process (PID=476) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:08:03.504+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:08:03.507+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:08:03.506+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:08:03.521+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:08:03.540+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:08:03.540+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:08:03.553+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:08:03.553+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:08:03.561+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-19T08:08:33.797+0000] {processor.py:161} INFO - Started process (PID=483) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:08:33.799+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:08:33.801+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:08:33.801+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:08:33.815+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:08:33.833+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:08:33.833+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:08:33.846+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:08:33.845+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:08:33.854+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.060 seconds
[2024-01-19T08:09:04.123+0000] {processor.py:161} INFO - Started process (PID=490) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:09:04.124+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:09:04.127+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:09:04.127+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:09:04.143+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:09:04.163+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:09:04.163+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:09:04.176+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:09:04.176+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:09:04.185+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T08:09:34.389+0000] {processor.py:161} INFO - Started process (PID=497) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:09:34.390+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:09:34.392+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:09:34.391+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:09:34.402+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:09:34.422+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:09:34.422+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:09:34.436+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:09:34.436+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:09:34.443+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.058 seconds
[2024-01-19T08:10:04.746+0000] {processor.py:161} INFO - Started process (PID=504) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:10:04.747+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:10:04.749+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:10:04.749+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:10:04.761+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:10:04.782+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:10:04.781+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:10:04.794+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:10:04.794+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:10:04.802+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.059 seconds
[2024-01-19T08:10:35.080+0000] {processor.py:161} INFO - Started process (PID=511) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:10:35.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:10:35.083+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:10:35.083+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:10:35.095+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:10:35.114+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:10:35.114+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:10:35.128+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:10:35.128+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:10:35.136+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.060 seconds
[2024-01-19T08:11:05.414+0000] {processor.py:161} INFO - Started process (PID=518) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:11:05.415+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:11:05.419+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:11:05.419+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:11:05.437+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:11:05.460+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:11:05.460+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:11:05.473+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:11:05.473+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:11:05.483+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T08:11:35.815+0000] {processor.py:161} INFO - Started process (PID=525) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:11:35.816+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:11:35.818+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:11:35.818+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:11:35.832+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:11:35.854+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:11:35.854+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:11:35.869+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:11:35.869+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:11:35.878+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T08:12:06.168+0000] {processor.py:161} INFO - Started process (PID=532) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:12:06.169+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:12:06.172+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:12:06.172+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:12:06.200+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:12:06.224+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:12:06.224+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:12:06.241+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:12:06.241+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:12:06.252+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.088 seconds
[2024-01-19T08:12:36.578+0000] {processor.py:161} INFO - Started process (PID=539) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:12:36.582+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:12:36.587+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:12:36.587+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:12:36.607+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:12:36.638+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:12:36.637+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:12:36.653+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:12:36.653+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:12:36.662+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.091 seconds
[2024-01-19T08:13:00.849+0000] {processor.py:161} INFO - Started process (PID=546) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:13:00.850+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:13:00.853+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:13:00.853+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:13:00.871+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:13:00.889+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:13:00.889+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:13:00.905+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:13:00.905+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:13:00.916+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T08:13:31.217+0000] {processor.py:161} INFO - Started process (PID=553) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:13:31.220+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:13:31.226+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:13:31.225+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:13:31.251+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:13:31.273+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:13:31.273+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:13:31.287+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:13:31.286+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-06T00:00:00+00:00, run_after=2024-01-07T00:00:00+00:00
[2024-01-19T08:13:31.296+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.085 seconds
[2024-01-19T08:14:01.518+0000] {processor.py:161} INFO - Started process (PID=584) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:14:01.522+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:14:01.528+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:14:01.527+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:14:01.550+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:14:01.574+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:14:01.574+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:14:01.588+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:14:01.587+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:14:01.597+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.085 seconds
[2024-01-19T08:14:31.847+0000] {processor.py:161} INFO - Started process (PID=592) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:14:31.849+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:14:31.866+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:14:31.866+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:14:31.884+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:14:31.908+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:14:31.908+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:14:31.923+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:14:31.923+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:14:31.934+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.091 seconds
[2024-01-19T08:15:02.205+0000] {processor.py:161} INFO - Started process (PID=599) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:15:02.207+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:15:02.211+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:15:02.211+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:15:02.229+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:15:02.253+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:15:02.253+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:15:02.267+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:15:02.267+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:15:02.276+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T08:15:32.547+0000] {processor.py:161} INFO - Started process (PID=606) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:15:32.552+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:15:32.558+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:15:32.557+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:15:32.590+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:15:32.615+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:15:32.615+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:15:32.631+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:15:32.631+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:15:32.641+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.101 seconds
[2024-01-19T08:16:02.900+0000] {processor.py:161} INFO - Started process (PID=613) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:16:02.903+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:16:02.906+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:16:02.905+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:16:02.924+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:16:02.949+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:16:02.949+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:16:02.963+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:16:02.963+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:16:02.972+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T08:16:23.116+0000] {processor.py:161} INFO - Started process (PID=620) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:16:23.117+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:16:23.120+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:16:23.119+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:16:23.136+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:16:23.163+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:16:23.163+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:16:23.178+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:16:23.178+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:16:23.188+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.076 seconds
[2024-01-19T08:16:53.511+0000] {processor.py:161} INFO - Started process (PID=627) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:16:53.513+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:16:53.519+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:16:53.518+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:16:53.545+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:16:53.578+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:16:53.578+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:16:53.593+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:16:53.593+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:16:53.603+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.098 seconds
[2024-01-19T08:17:23.874+0000] {processor.py:161} INFO - Started process (PID=634) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:17:23.875+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:17:23.880+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:17:23.879+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:17:23.900+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:17:23.921+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:17:23.921+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:17:23.933+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:17:23.933+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:17:23.941+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T08:17:54.242+0000] {processor.py:161} INFO - Started process (PID=641) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:17:54.244+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:17:54.246+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:17:54.246+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:17:54.260+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:17:54.278+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:17:54.278+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:17:54.293+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:17:54.293+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:17:54.301+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T08:18:24.598+0000] {processor.py:161} INFO - Started process (PID=648) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:18:24.599+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:18:24.602+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:18:24.601+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:18:24.615+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:18:24.645+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:18:24.645+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:18:24.668+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:18:24.668+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:18:24.678+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.083 seconds
[2024-01-19T08:18:54.979+0000] {processor.py:161} INFO - Started process (PID=655) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:18:54.981+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:18:54.986+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:18:54.985+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:18:55.027+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:18:55.058+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:18:55.058+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:18:55.074+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:18:55.073+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:18:55.084+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.113 seconds
[2024-01-19T08:19:25.308+0000] {processor.py:161} INFO - Started process (PID=662) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:19:25.310+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:19:25.314+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:19:25.314+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:19:25.338+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:19:25.365+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:19:25.365+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:19:25.379+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:19:25.379+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:19:25.389+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.087 seconds
[2024-01-19T08:19:55.727+0000] {processor.py:161} INFO - Started process (PID=669) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:19:55.729+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:19:55.733+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:19:55.732+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:19:55.750+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:19:55.794+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:19:55.794+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:19:55.811+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:19:55.811+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:19:55.821+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.099 seconds
[2024-01-19T08:20:26.087+0000] {processor.py:161} INFO - Started process (PID=676) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:20:26.089+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:20:26.100+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:20:26.095+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:20:26.136+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:20:26.165+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:20:26.165+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:20:26.195+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:20:26.194+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:20:26.209+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.128 seconds
[2024-01-19T08:20:56.482+0000] {processor.py:161} INFO - Started process (PID=683) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:20:56.484+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:20:56.488+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:20:56.487+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:20:56.518+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:20:56.546+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:20:56.546+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:20:56.566+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:20:56.566+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:20:56.579+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.101 seconds
[2024-01-19T08:21:26.906+0000] {processor.py:161} INFO - Started process (PID=690) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:21:26.907+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:21:26.910+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:21:26.910+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:21:26.943+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:21:26.974+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:21:26.974+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:21:27.003+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:21:27.003+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:21:27.015+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.113 seconds
[2024-01-19T08:21:57.318+0000] {processor.py:161} INFO - Started process (PID=697) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:21:57.320+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:21:57.326+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:21:57.325+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:21:57.347+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:21:57.383+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:21:57.382+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:21:57.398+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:21:57.398+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:21:57.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.096 seconds
[2024-01-19T08:22:18.566+0000] {processor.py:161} INFO - Started process (PID=698) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:18.569+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:22:18.574+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:18.573+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:18.598+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:18.596+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 253, in <module>
    create_table >> insert_data
NameError: name 'insert_data' is not defined
[2024-01-19T08:22:18.599+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:18.623+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-19T08:22:19.674+0000] {processor.py:161} INFO - Started process (PID=699) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:19.676+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:22:19.678+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:19.678+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:19.693+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:19.692+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 256, in <module>
    create_table >> insert_data
NameError: name 'insert_data' is not defined
[2024-01-19T08:22:19.693+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:19.708+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.038 seconds
[2024-01-19T08:22:20.719+0000] {processor.py:161} INFO - Started process (PID=706) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:20.721+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:22:20.724+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:20.723+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:20.738+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:20.736+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 245, in <module>
    insert_data = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 184, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 816, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'InsertCompanyProfile' has already been added to the DAG
[2024-01-19T08:22:20.739+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:20.754+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.040 seconds
[2024-01-19T08:22:22.774+0000] {processor.py:161} INFO - Started process (PID=707) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:22.775+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:22:22.778+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:22.778+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:22.796+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:22.794+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 245, in <module>
    insert_data2 = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 184, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 816, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'InsertCompanyProfile' has already been added to the DAG
[2024-01-19T08:22:22.797+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:22.819+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.050 seconds
[2024-01-19T08:22:24.837+0000] {processor.py:161} INFO - Started process (PID=708) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:24.839+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:22:24.843+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:24.842+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:24.860+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:24.858+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 245, in <module>
    insert_data2 = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 184, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 816, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'InsertCompanyProfile' has already been added to the DAG
[2024-01-19T08:22:24.860+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:24.877+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.045 seconds
[2024-01-19T08:22:27.894+0000] {processor.py:161} INFO - Started process (PID=709) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:27.895+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:22:27.898+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:27.897+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:27.913+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:27.911+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 245, in <module>
    insert_data2 = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 184, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 816, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'InsertCompanyProfile' has already been added to the DAG
[2024-01-19T08:22:27.914+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:27.931+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.041 seconds
[2024-01-19T08:22:28.941+0000] {processor.py:161} INFO - Started process (PID=710) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:28.943+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:22:28.945+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:28.944+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:28.958+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:28.955+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 245, in <module>
    insert_data2 = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 184, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 816, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'InsertCompanyProfile' has already been added to the DAG
[2024-01-19T08:22:28.959+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:28.980+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.041 seconds
[2024-01-19T08:22:43.091+0000] {processor.py:161} INFO - Started process (PID=711) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:43.093+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:22:43.097+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:43.096+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:43.115+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:22:43.113+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 245, in <module>
    insert_data2 = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 184, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 816, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'InsertCompanyProfile' has already been added to the DAG
[2024-01-19T08:22:43.115+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:22:43.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.047 seconds
[2024-01-19T08:23:11.381+0000] {processor.py:161} INFO - Started process (PID=718) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:11.384+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:23:11.394+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:11.394+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:11.417+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:11.488+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:11.488+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:23:11.500+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:11.499+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:23:11.510+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.142 seconds
[2024-01-19T08:23:12.408+0000] {processor.py:161} INFO - Started process (PID=719) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:12.409+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:23:12.412+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:12.411+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:12.425+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:12.423+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/CrawlCompanyInfo.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/CrawlCompanyInfo.py", line 245, in <module>
    insert_data2 = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 184, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 816, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'InsertCompanyProfile1' has already been added to the DAG
[2024-01-19T08:23:12.425+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:12.444+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.039 seconds
[2024-01-19T08:23:15.453+0000] {processor.py:161} INFO - Started process (PID=720) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:15.455+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:23:15.458+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:15.458+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:15.481+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:15.493+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:15.493+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:23:15.506+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:15.506+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:23:15.516+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T08:23:16.495+0000] {processor.py:161} INFO - Started process (PID=721) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:16.496+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:23:16.497+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:16.497+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:16.509+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:16.518+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:16.518+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:23:16.530+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:16.529+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:23:16.538+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.047 seconds
[2024-01-19T08:23:18.565+0000] {processor.py:161} INFO - Started process (PID=722) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:18.567+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:23:18.569+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:18.569+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:18.586+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:18.595+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:18.595+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:23:18.607+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:18.607+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:23:18.616+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.055 seconds
[2024-01-19T08:23:19.593+0000] {processor.py:161} INFO - Started process (PID=723) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:19.595+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:23:19.598+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:19.597+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:19.614+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:19.624+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:19.624+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:23:19.637+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:19.637+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:23:19.646+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.057 seconds
[2024-01-19T08:23:20.650+0000] {processor.py:161} INFO - Started process (PID=730) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:20.651+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:23:20.654+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:20.653+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:20.668+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:20.678+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:20.677+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:23:20.690+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:20.690+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:23:20.698+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.052 seconds
[2024-01-19T08:23:21.677+0000] {processor.py:161} INFO - Started process (PID=731) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:21.678+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:23:21.680+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:21.680+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:21.695+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:21.703+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:21.703+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:23:21.715+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:21.715+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:23:21.723+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.049 seconds
[2024-01-19T08:23:23.749+0000] {processor.py:161} INFO - Started process (PID=732) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:23.750+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:23:23.752+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:23.752+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:23.768+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:23.778+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:23.778+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:23:23.792+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:23.792+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:23:23.801+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.055 seconds
[2024-01-19T08:23:26.811+0000] {processor.py:161} INFO - Started process (PID=733) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:26.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:23:26.816+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:26.816+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:26.833+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:26.843+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:26.843+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:23:26.857+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:26.857+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:23:26.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.058 seconds
[2024-01-19T08:23:27.866+0000] {processor.py:161} INFO - Started process (PID=734) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:27.868+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:23:27.871+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:27.870+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:27.891+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:27.902+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:27.902+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:23:27.916+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:27.916+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:23:27.925+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-19T08:23:28.921+0000] {processor.py:161} INFO - Started process (PID=735) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:28.922+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:23:28.924+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:28.924+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:28.936+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:28.946+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:28.946+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:23:28.959+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:28.959+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:23:28.970+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.054 seconds
[2024-01-19T08:23:29.986+0000] {processor.py:161} INFO - Started process (PID=736) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:29.987+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:23:29.989+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:29.988+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:30.005+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:30.014+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:30.014+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:23:30.027+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:30.027+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:23:30.039+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.057 seconds
[2024-01-19T08:23:31.022+0000] {processor.py:161} INFO - Started process (PID=737) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:31.023+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:23:31.025+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:31.025+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:31.046+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:23:31.059+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:31.059+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:23:31.075+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:23:31.075+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:23:31.086+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T08:24:01.443+0000] {processor.py:161} INFO - Started process (PID=744) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:24:01.446+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:24:01.451+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:24:01.451+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:24:01.478+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:24:01.544+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:24:01.544+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:24:01.555+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:24:01.555+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:24:01.563+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.128 seconds
[2024-01-19T08:24:31.805+0000] {processor.py:161} INFO - Started process (PID=751) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:24:31.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:24:31.810+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:24:31.809+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:24:31.829+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:24:31.854+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:24:31.854+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:24:31.868+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:24:31.868+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:24:31.877+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T08:25:02.241+0000] {processor.py:161} INFO - Started process (PID=758) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:25:02.242+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:25:02.247+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:25:02.246+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:25:02.269+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:25:02.286+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:25:02.286+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:25:02.297+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:25:02.297+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:25:02.305+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T08:25:32.589+0000] {processor.py:161} INFO - Started process (PID=765) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:25:32.592+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:25:32.596+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:25:32.595+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:25:32.618+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:25:32.640+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:25:32.640+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:25:32.653+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:25:32.653+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:25:32.661+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-19T08:26:02.944+0000] {processor.py:161} INFO - Started process (PID=772) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:26:02.945+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:26:02.948+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:26:02.947+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:26:02.970+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:26:02.995+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:26:02.995+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:26:03.009+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:26:03.009+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:26:03.022+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T08:26:33.345+0000] {processor.py:161} INFO - Started process (PID=779) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:26:33.347+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:26:33.350+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:26:33.350+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:26:33.369+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:26:33.397+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:26:33.397+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:26:33.418+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:26:33.417+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:26:33.427+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.086 seconds
[2024-01-19T08:27:03.673+0000] {processor.py:161} INFO - Started process (PID=786) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:03.674+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:27:03.677+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:03.677+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:03.695+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:03.717+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:03.717+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:27:03.731+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:03.731+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:27:03.739+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T08:27:19.833+0000] {processor.py:161} INFO - Started process (PID=787) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:19.835+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:27:19.839+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:19.838+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:19.871+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:19.899+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:19.899+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:27:19.917+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:19.917+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:27:19.934+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.105 seconds
[2024-01-19T08:27:20.898+0000] {processor.py:161} INFO - Started process (PID=788) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:20.900+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:27:20.905+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:20.904+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:20.936+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:20.966+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:20.966+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:27:20.980+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:20.979+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:27:20.988+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.097 seconds
[2024-01-19T08:27:21.978+0000] {processor.py:161} INFO - Started process (PID=795) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:21.979+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:27:21.982+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:21.981+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:21.998+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:22.019+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:22.019+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:27:22.033+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:22.033+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:27:22.042+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.069 seconds
[2024-01-19T08:27:26.051+0000] {processor.py:161} INFO - Started process (PID=796) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:26.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:27:26.054+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:26.054+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:26.067+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:26.084+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:26.084+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:27:26.095+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:26.095+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:27:26.104+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-19T08:27:41.251+0000] {processor.py:161} INFO - Started process (PID=797) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:41.253+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:27:41.256+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:41.256+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:41.278+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:27:41.302+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:41.302+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:27:41.318+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:27:41.317+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:27:41.328+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T08:28:11.571+0000] {processor.py:161} INFO - Started process (PID=804) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:28:11.573+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:28:11.576+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:28:11.576+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:28:11.593+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:28:11.616+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:28:11.616+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:28:11.630+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:28:11.629+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:28:11.639+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T08:28:41.906+0000] {processor.py:161} INFO - Started process (PID=811) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:28:41.908+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:28:41.914+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:28:41.913+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:28:41.936+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:28:41.960+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:28:41.960+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:28:41.976+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:28:41.976+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:28:41.987+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.086 seconds
[2024-01-19T08:29:12.256+0000] {processor.py:161} INFO - Started process (PID=819) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:29:12.258+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:29:12.263+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:29:12.262+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:29:12.284+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:29:12.311+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:29:12.311+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:29:12.326+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:29:12.326+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:29:12.336+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.086 seconds
[2024-01-19T08:29:42.703+0000] {processor.py:161} INFO - Started process (PID=826) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:29:42.704+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:29:42.707+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:29:42.706+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:29:42.726+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:29:42.753+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:29:42.753+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:29:42.768+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:29:42.768+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:29:42.779+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.081 seconds
[2024-01-19T08:30:13.088+0000] {processor.py:161} INFO - Started process (PID=833) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:13.092+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:30:13.102+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:13.101+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:13.126+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:13.156+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:13.155+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:30:13.172+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:13.172+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:30:13.183+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.107 seconds
[2024-01-19T08:30:30.290+0000] {processor.py:161} INFO - Started process (PID=840) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:30.292+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:30:30.297+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:30.297+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:30.322+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:30.347+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:30.347+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:30:30.363+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:30.362+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:30:30.373+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.090 seconds
[2024-01-19T08:30:36.413+0000] {processor.py:161} INFO - Started process (PID=841) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:36.415+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:30:36.418+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:36.417+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:36.443+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:36.484+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:36.484+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:30:36.509+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:36.509+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:30:36.546+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.138 seconds
[2024-01-19T08:30:37.461+0000] {processor.py:161} INFO - Started process (PID=842) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:37.463+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:30:37.464+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:37.464+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:37.476+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:37.495+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:37.495+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:30:37.508+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:37.508+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:30:37.519+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T08:30:57.667+0000] {processor.py:161} INFO - Started process (PID=849) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:57.673+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:30:57.676+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:57.676+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:57.693+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:30:57.714+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:57.714+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:30:57.726+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:30:57.726+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:30:57.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T08:31:04.776+0000] {processor.py:161} INFO - Started process (PID=850) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:04.777+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:31:04.779+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:04.779+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:04.795+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:04.818+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:04.818+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:31:04.829+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:04.829+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:31:04.838+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T08:31:09.864+0000] {processor.py:161} INFO - Started process (PID=851) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:09.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:31:09.867+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:09.867+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:09.881+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:09.900+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:09.900+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:31:09.912+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:09.912+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:31:09.922+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T08:31:26.113+0000] {processor.py:161} INFO - Started process (PID=858) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:26.115+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:31:26.119+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:26.119+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:26.148+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:26.180+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:26.180+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:31:26.196+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:26.196+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:31:26.206+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.098 seconds
[2024-01-19T08:31:27.129+0000] {processor.py:161} INFO - Started process (PID=859) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:27.130+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:31:27.131+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:27.131+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:27.143+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:27.171+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:27.171+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:31:27.185+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:27.185+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:31:27.196+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-19T08:31:28.143+0000] {processor.py:161} INFO - Started process (PID=860) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:28.144+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:31:28.146+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:28.146+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:28.161+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:28.184+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:28.184+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:31:28.202+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:28.202+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:31:28.213+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T08:31:31.278+0000] {processor.py:161} INFO - Started process (PID=861) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:31.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:31:31.282+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:31.282+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:31.302+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:31.326+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:31.326+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:31:31.338+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:31.338+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:31:31.348+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.075 seconds
[2024-01-19T08:31:33.350+0000] {processor.py:161} INFO - Started process (PID=862) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:33.353+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:31:33.355+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:33.355+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:33.374+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:31:33.393+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:33.393+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:31:33.405+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:31:33.405+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:31:33.413+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T08:32:03.726+0000] {processor.py:161} INFO - Started process (PID=869) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:03.728+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:32:03.732+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:03.732+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:03.755+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:03.776+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:03.776+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:32:03.789+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:03.789+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:32:03.798+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.078 seconds
[2024-01-19T08:32:34.047+0000] {processor.py:161} INFO - Started process (PID=876) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:34.048+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:32:34.053+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:34.053+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:34.076+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:34.098+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:34.098+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:32:34.112+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:34.112+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:32:34.128+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.088 seconds
[2024-01-19T08:32:39.137+0000] {processor.py:161} INFO - Started process (PID=877) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:39.138+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:32:39.141+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:39.140+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:39.159+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:39.178+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:39.178+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:32:39.190+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:39.190+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:32:39.202+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-19T08:32:40.204+0000] {processor.py:161} INFO - Started process (PID=878) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:40.205+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:32:40.208+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:40.208+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:40.224+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:40.246+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:40.246+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:32:40.259+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:40.259+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:32:40.268+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T08:32:43.278+0000] {processor.py:161} INFO - Started process (PID=879) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:43.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:32:43.282+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:43.281+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:43.300+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:43.322+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:43.322+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:32:43.336+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:43.336+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:32:43.345+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T08:32:51.439+0000] {processor.py:161} INFO - Started process (PID=880) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:51.440+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:32:51.445+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:51.444+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:51.485+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:51.516+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:51.516+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:32:51.533+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:51.533+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:32:51.543+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.111 seconds
[2024-01-19T08:32:53.477+0000] {processor.py:161} INFO - Started process (PID=887) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:53.479+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:32:53.482+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:53.482+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:53.500+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:53.522+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:53.521+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:32:53.533+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:53.533+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:32:53.542+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T08:32:55.523+0000] {processor.py:161} INFO - Started process (PID=888) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:55.524+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:32:55.527+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:55.527+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:55.546+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:55.566+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:55.566+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:32:55.578+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:55.578+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:32:55.587+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-19T08:32:58.619+0000] {processor.py:161} INFO - Started process (PID=889) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:58.620+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:32:58.622+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:58.622+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:58.639+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:32:58.667+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:58.667+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:32:58.681+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:32:58.681+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:32:58.690+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T08:33:00.677+0000] {processor.py:161} INFO - Started process (PID=890) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:00.678+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:33:00.679+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:00.679+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:00.694+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:00.713+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:00.713+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:33:00.726+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:00.726+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:33:00.737+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.063 seconds
[2024-01-19T08:33:01.730+0000] {processor.py:161} INFO - Started process (PID=891) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:01.732+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:33:01.734+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:01.733+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:01.748+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:01.766+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:01.766+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:33:01.781+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:01.781+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:33:01.792+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-19T08:33:11.906+0000] {processor.py:161} INFO - Started process (PID=892) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:11.908+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:33:11.911+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:11.911+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:11.930+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:11.951+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:11.951+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:33:11.964+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:11.963+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:33:11.973+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T08:33:12.920+0000] {processor.py:161} INFO - Started process (PID=893) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:12.921+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:33:12.923+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:12.923+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:12.940+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:12.965+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:12.965+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:33:12.978+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:12.978+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:33:12.988+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T08:33:14.018+0000] {processor.py:161} INFO - Started process (PID=894) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:14.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:33:14.022+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:14.022+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:14.041+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:14.062+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:14.062+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:33:14.075+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:14.075+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:33:14.084+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-19T08:33:15.047+0000] {processor.py:161} INFO - Started process (PID=895) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:15.048+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:33:15.050+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:15.049+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:15.066+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:15.086+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:15.086+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:33:15.098+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:15.097+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:33:15.106+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T08:33:16.130+0000] {processor.py:161} INFO - Started process (PID=896) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:16.131+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:33:16.133+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:16.133+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:16.152+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:16.174+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:16.174+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:33:16.186+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:16.186+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:33:16.195+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.070 seconds
[2024-01-19T08:33:19.207+0000] {processor.py:161} INFO - Started process (PID=897) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:19.211+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:33:19.214+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:19.214+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:19.231+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:19.256+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:19.256+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:33:19.269+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:19.268+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:33:19.278+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T08:33:22.264+0000] {processor.py:161} INFO - Started process (PID=904) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:22.265+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:33:22.267+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:22.266+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:22.280+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:22.298+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:22.297+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:33:22.309+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:22.309+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:33:22.318+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.058 seconds
[2024-01-19T08:33:23.343+0000] {processor.py:161} INFO - Started process (PID=905) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:23.344+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:33:23.345+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:23.345+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:23.360+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:23.382+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:23.381+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:33:23.396+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:23.395+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:33:23.405+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T08:33:24.377+0000] {processor.py:161} INFO - Started process (PID=906) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:24.377+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:33:24.379+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:24.378+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:24.393+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:24.411+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:24.411+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:33:24.425+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:24.424+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:33:24.435+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.061 seconds
[2024-01-19T08:33:25.467+0000] {processor.py:161} INFO - Started process (PID=907) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:25.468+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:33:25.472+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:25.471+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:25.489+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:25.512+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:25.512+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:33:25.528+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:25.528+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:33:25.539+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-19T08:33:29.537+0000] {processor.py:161} INFO - Started process (PID=908) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:29.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:33:29.542+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:29.542+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:29.571+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:29.601+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:29.601+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:33:29.618+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:29.618+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:33:29.631+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.098 seconds
[2024-01-19T08:33:59.947+0000] {processor.py:161} INFO - Started process (PID=915) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:59.950+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:33:59.955+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:33:59.954+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:33:59.997+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:00.023+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:00.023+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:34:00.039+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:00.039+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:34:00.049+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.109 seconds
[2024-01-19T08:34:26.303+0000] {processor.py:161} INFO - Started process (PID=922) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:26.306+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:34:26.311+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:26.311+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:26.344+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:26.371+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:26.370+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:34:26.386+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:26.386+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:34:26.397+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.101 seconds
[2024-01-19T08:34:27.331+0000] {processor.py:161} INFO - Started process (PID=923) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:27.332+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:34:27.333+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:27.333+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:27.346+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:27.370+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:27.370+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:34:27.410+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:27.409+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:34:27.441+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.113 seconds
[2024-01-19T08:34:29.464+0000] {processor.py:161} INFO - Started process (PID=924) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:29.465+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:34:29.468+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:29.467+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:29.488+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:29.510+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:29.510+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:34:29.521+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:29.521+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:34:29.531+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T08:34:31.528+0000] {processor.py:161} INFO - Started process (PID=925) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:31.530+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:34:31.533+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:31.533+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:31.551+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:31.571+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:31.571+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:34:31.583+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:31.583+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:34:31.593+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T08:34:32.596+0000] {processor.py:161} INFO - Started process (PID=926) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:32.597+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:34:32.599+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:32.599+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:32.613+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:32.632+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:32.632+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:34:32.646+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:32.646+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:34:32.656+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-19T08:34:35.650+0000] {processor.py:161} INFO - Started process (PID=927) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:35.652+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:34:35.654+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:35.654+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:35.677+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:35.702+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:35.702+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:34:35.716+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:35.716+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:34:35.728+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T08:34:37.722+0000] {processor.py:161} INFO - Started process (PID=928) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:37.723+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:34:37.727+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:37.726+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:37.758+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:37.782+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:37.782+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:34:37.797+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:37.797+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:34:37.807+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.090 seconds
[2024-01-19T08:34:38.756+0000] {processor.py:161} INFO - Started process (PID=929) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:38.756+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:34:38.758+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:38.758+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:38.768+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:38.784+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:38.784+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:34:38.794+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:38.794+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:34:38.803+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.050 seconds
[2024-01-19T08:34:41.810+0000] {processor.py:161} INFO - Started process (PID=930) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:41.811+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:34:41.814+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:41.814+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:41.830+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:41.848+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:41.848+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:34:41.861+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:41.861+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:34:41.872+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T08:34:48.901+0000] {processor.py:161} INFO - Started process (PID=931) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:48.903+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T08:34:48.904+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:48.904+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:48.917+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T08:34:48.932+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:48.932+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T08:34:48.943+0000] {logging_mixin.py:188} INFO - [2024-01-19T08:34:48.943+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T08:34:48.952+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.054 seconds
[2024-01-19T09:14:07.252+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:14:07.253+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:14:07.256+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:14:07.256+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:14:07.354+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:14:07.481+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:14:07.481+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:14:07.511+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:14:07.510+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:14:07.546+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.297 seconds
[2024-01-19T09:14:19.454+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:14:19.456+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:14:19.459+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:14:19.459+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:14:19.483+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:14:19.500+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:14:19.500+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:14:19.519+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:14:19.518+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:14:19.530+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.080 seconds
[2024-01-19T09:14:20.465+0000] {processor.py:161} INFO - Started process (PID=38) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:14:20.466+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:14:20.468+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:14:20.468+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:14:20.482+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:14:20.504+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:14:20.504+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:14:20.523+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:14:20.523+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:14:20.537+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T09:14:45.783+0000] {processor.py:161} INFO - Started process (PID=45) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:14:45.784+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:14:45.787+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:14:45.786+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:14:45.800+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:14:45.867+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:14:45.867+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:14:45.878+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:14:45.878+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:14:45.889+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.109 seconds
[2024-01-19T09:15:16.192+0000] {processor.py:161} INFO - Started process (PID=52) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:15:16.195+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:15:16.201+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:15:16.200+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:15:16.226+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:15:16.246+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:15:16.246+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:15:16.261+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:15:16.261+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:15:16.271+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.086 seconds
[2024-01-19T09:15:46.602+0000] {processor.py:161} INFO - Started process (PID=59) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:15:46.604+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:15:46.608+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:15:46.607+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:15:46.626+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:15:46.648+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:15:46.648+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:15:46.678+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:15:46.678+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:15:46.687+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.091 seconds
[2024-01-19T09:16:16.925+0000] {processor.py:161} INFO - Started process (PID=66) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:16:16.927+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:16:16.931+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:16:16.930+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:16:16.951+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:16:16.974+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:16:16.974+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:16:16.987+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:16:16.987+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:16:16.996+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.077 seconds
[2024-01-19T09:16:32.122+0000] {processor.py:161} INFO - Started process (PID=73) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:16:32.123+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:16:32.126+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:16:32.126+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:16:32.163+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:16:32.189+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:16:32.189+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:16:32.209+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:16:32.208+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:16:32.232+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.115 seconds
[2024-01-19T09:17:02.507+0000] {processor.py:161} INFO - Started process (PID=80) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:17:02.509+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:17:02.514+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:17:02.514+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:17:02.534+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:17:02.554+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:17:02.554+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:17:02.566+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:17:02.566+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:17:02.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T09:17:32.875+0000] {processor.py:161} INFO - Started process (PID=87) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:17:32.878+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:17:32.881+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:17:32.881+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:17:32.902+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:17:32.932+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:17:32.931+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:17:32.948+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:17:32.948+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:17:32.960+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.090 seconds
[2024-01-19T09:18:03.253+0000] {processor.py:161} INFO - Started process (PID=94) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:18:03.254+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:18:03.259+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:18:03.258+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:18:03.280+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:18:03.301+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:18:03.301+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:18:03.313+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:18:03.312+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:18:03.321+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T09:18:33.609+0000] {processor.py:161} INFO - Started process (PID=101) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:18:33.610+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:18:33.613+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:18:33.613+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:18:33.630+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:18:33.650+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:18:33.650+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:18:33.661+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:18:33.661+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:18:33.669+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-19T09:19:03.928+0000] {processor.py:161} INFO - Started process (PID=108) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:19:03.929+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:19:03.932+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:19:03.932+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:19:03.948+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:19:03.969+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:19:03.969+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:19:03.983+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:19:03.982+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:19:03.991+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.066 seconds
[2024-01-19T09:19:34.343+0000] {processor.py:161} INFO - Started process (PID=115) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:19:34.344+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:19:34.347+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:19:34.347+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:19:34.364+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:19:34.389+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:19:34.389+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:19:34.402+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:19:34.402+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:19:34.410+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T09:20:04.687+0000] {processor.py:161} INFO - Started process (PID=122) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:20:04.688+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:20:04.694+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:20:04.693+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:20:04.719+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:20:04.746+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:20:04.746+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:20:04.762+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:20:04.762+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:20:04.772+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.091 seconds
[2024-01-19T09:20:35.003+0000] {processor.py:161} INFO - Started process (PID=129) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:20:35.004+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:20:35.007+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:20:35.007+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:20:35.025+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:20:35.048+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:20:35.048+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:20:35.060+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:20:35.059+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:20:35.067+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T09:21:21.863+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:21:21.864+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:21:21.866+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:21:21.866+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:21:21.898+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:21:22.036+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:21:22.036+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:21:22.059+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:21:22.059+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:21:22.070+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.210 seconds
[2024-01-19T09:21:52.202+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:21:52.205+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:21:52.209+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:21:52.209+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:21:52.232+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:21:52.252+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:21:52.252+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:21:52.264+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:21:52.264+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:21:52.272+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.076 seconds
[2024-01-19T09:22:22.540+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:22:22.541+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:22:22.545+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:22:22.544+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:22:22.565+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:22:22.587+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:22:22.587+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:22:22.600+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:22:22.600+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:22:22.608+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.074 seconds
[2024-01-19T09:22:52.872+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:22:52.874+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:22:52.877+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:22:52.876+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:22:52.895+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:22:52.924+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:22:52.924+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:22:52.939+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:22:52.939+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:22:52.949+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T09:23:39.645+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:23:39.646+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:23:39.648+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:23:39.648+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:23:39.662+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:23:39.782+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:23:39.782+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:23:39.794+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:23:39.794+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:23:39.820+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.178 seconds
[2024-01-19T09:24:09.945+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:24:09.947+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:24:09.951+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:24:09.951+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:24:09.974+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:24:09.992+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:24:09.992+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:24:10.003+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:24:10.003+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:24:10.011+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.071 seconds
[2024-01-19T09:24:40.283+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:24:40.286+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:24:40.290+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:24:40.289+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:24:40.309+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:24:40.334+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:24:40.334+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:24:40.349+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:24:40.349+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:24:40.359+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T09:25:10.608+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:25:10.609+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:25:10.614+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:25:10.614+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:25:10.635+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:25:10.658+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:25:10.658+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:25:10.670+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:25:10.670+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:25:10.678+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.076 seconds
[2024-01-19T09:25:40.907+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:25:40.908+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:25:40.911+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:25:40.910+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:25:40.926+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:25:40.949+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:25:40.949+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:25:40.960+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:25:40.960+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:25:40.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.065 seconds
[2024-01-19T09:26:11.222+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:26:11.223+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:26:11.228+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:26:11.227+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:26:11.249+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:26:11.269+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:26:11.269+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:26:11.281+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:26:11.281+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:26:11.289+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.073 seconds
[2024-01-19T09:26:41.502+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:26:41.502+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:26:41.505+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:26:41.505+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:26:41.521+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:26:41.541+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:26:41.541+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:26:41.553+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:26:41.553+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:26:41.561+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.064 seconds
[2024-01-19T09:27:11.798+0000] {processor.py:161} INFO - Started process (PID=85) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:27:11.799+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:27:11.804+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:27:11.803+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:27:11.826+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:27:11.854+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:27:11.854+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:27:11.874+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:27:11.874+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:27:11.885+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.091 seconds
[2024-01-19T09:27:42.074+0000] {processor.py:161} INFO - Started process (PID=92) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:27:42.074+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:27:42.076+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:27:42.076+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:27:42.084+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:27:42.100+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:27:42.100+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:27:42.110+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:27:42.110+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:27:42.118+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.046 seconds
[2024-01-19T09:28:12.416+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:28:12.417+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:28:12.423+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:28:12.422+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:28:12.452+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:28:12.477+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:28:12.477+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:28:12.494+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:28:12.494+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:28:12.505+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.093 seconds
[2024-01-19T09:28:21.524+0000] {processor.py:161} INFO - Started process (PID=100) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:28:21.526+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:28:21.529+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:28:21.529+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:28:21.553+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:28:21.576+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:28:21.576+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:28:21.587+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:28:21.587+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:28:21.595+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.079 seconds
[2024-01-19T09:28:51.961+0000] {processor.py:161} INFO - Started process (PID=107) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:28:51.963+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:28:51.966+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:28:51.966+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:28:51.985+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:28:52.015+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:28:52.015+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:28:52.030+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:28:52.030+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:28:52.039+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T09:29:08.157+0000] {processor.py:161} INFO - Started process (PID=114) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:29:08.159+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:29:08.161+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:29:08.161+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:29:08.179+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:29:08.208+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:29:08.207+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:29:08.223+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:29:08.223+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:29:08.235+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.082 seconds
[2024-01-19T09:29:09.238+0000] {processor.py:161} INFO - Started process (PID=115) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:29:09.239+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:29:09.242+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:29:09.242+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:29:09.246+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:29:09.245+0000] {dagbag.py:325} INFO - File /opt/airflow/dags/CrawlCompanyInfo.py assumed to contain no DAGs. Skipping.
[2024-01-19T09:29:09.246+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:29:09.264+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.030 seconds
[2024-01-19T09:29:10.295+0000] {processor.py:161} INFO - Started process (PID=116) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:29:10.296+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:29:10.299+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:29:10.299+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:29:10.322+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:29:10.342+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:29:10.342+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:29:10.355+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:29:10.355+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:29:10.366+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.076 seconds
[2024-01-19T09:29:40.634+0000] {processor.py:161} INFO - Started process (PID=123) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:29:40.637+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:29:40.641+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:29:40.640+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:29:40.662+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:29:40.699+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:29:40.698+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:29:40.716+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:29:40.716+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:29:40.727+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.098 seconds
[2024-01-19T09:30:10.999+0000] {processor.py:161} INFO - Started process (PID=130) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:30:11.000+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:30:11.007+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:30:11.006+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:30:11.032+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:30:11.056+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:30:11.056+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:30:11.071+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:30:11.071+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:30:11.080+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.089 seconds
[2024-01-19T09:30:41.399+0000] {processor.py:161} INFO - Started process (PID=137) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:30:41.402+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:30:41.407+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:30:41.406+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:30:41.432+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:30:41.462+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:30:41.462+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:30:41.479+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:30:41.479+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:30:41.490+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.098 seconds
[2024-01-19T09:30:58.586+0000] {processor.py:161} INFO - Started process (PID=138) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:30:58.588+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:30:58.593+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:30:58.592+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:30:58.629+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:30:58.654+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:30:58.653+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:30:58.668+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:30:58.668+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:30:58.679+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.100 seconds
[2024-01-19T09:30:59.648+0000] {processor.py:161} INFO - Started process (PID=139) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:30:59.649+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:30:59.651+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:30:59.651+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:30:59.664+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:30:59.725+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:30:59.725+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:30:59.734+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:30:59.734+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:30:59.746+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.101 seconds
[2024-01-19T09:31:00.734+0000] {processor.py:161} INFO - Started process (PID=140) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:31:00.735+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:31:00.737+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:31:00.736+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:31:00.753+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:31:00.764+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:31:00.764+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:31:00.780+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:31:00.780+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:31:00.791+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.061 seconds
[2024-01-19T09:31:31.082+0000] {processor.py:161} INFO - Started process (PID=147) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:31:31.086+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:31:31.092+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:31:31.091+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:31:31.117+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:31:31.139+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:31:31.139+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:31:31.153+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:31:31.153+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:31:31.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.088 seconds
[2024-01-19T09:32:01.401+0000] {processor.py:161} INFO - Started process (PID=154) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:32:01.402+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:32:01.405+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:32:01.405+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:32:01.424+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:32:01.460+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:32:01.460+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:32:01.476+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:32:01.476+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:32:01.489+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.092 seconds
[2024-01-19T09:32:31.797+0000] {processor.py:161} INFO - Started process (PID=161) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:32:31.799+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:32:31.802+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:32:31.802+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:32:31.819+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:32:31.841+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:32:31.841+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:32:31.854+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:32:31.854+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:32:31.863+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T09:33:02.146+0000] {processor.py:161} INFO - Started process (PID=168) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:33:02.149+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:33:02.153+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:33:02.153+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:33:02.171+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:33:02.196+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:33:02.196+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:33:02.212+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:33:02.212+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:33:02.222+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.080 seconds
[2024-01-19T09:33:32.493+0000] {processor.py:161} INFO - Started process (PID=175) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:33:32.495+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T09:33:32.499+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:33:32.498+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:33:32.528+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T09:33:32.556+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:33:32.556+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T09:33:32.571+0000] {logging_mixin.py:188} INFO - [2024-01-19T09:33:32.571+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T09:33:32.581+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.092 seconds
[2024-01-19T10:44:27.934+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T10:44:27.945+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T10:44:27.969+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:44:27.966+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T10:44:28.079+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T10:44:28.613+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:44:28.612+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:company_profile_pipeline
[2024-01-19T10:44:28.625+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:44:28.625+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:company_profile_pipeline
[2024-01-19T10:44:28.632+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:44:28.632+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:company_profile_pipeline
[2024-01-19T10:44:28.633+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:44:28.633+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T10:44:28.647+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:44:28.647+0000] {dag.py:3055} INFO - Creating ORM DAG for company_profile_pipeline
[2024-01-19T10:44:28.687+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:44:28.686+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T10:44:28.715+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.795 seconds
[2024-01-19T10:44:59.135+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T10:44:59.139+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T10:44:59.152+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:44:59.150+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T10:44:59.275+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T10:44:59.386+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:44:59.386+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T10:44:59.814+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:44:59.813+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T10:44:59.884+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.809 seconds
[2024-01-19T10:45:30.224+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T10:45:30.226+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T10:45:30.237+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:45:30.236+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T10:45:30.275+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T10:45:30.330+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:45:30.330+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T10:45:30.365+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:45:30.365+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T10:45:30.383+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.167 seconds
[2024-01-19T10:46:00.611+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T10:46:00.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T10:46:00.644+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:46:00.628+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T10:46:00.905+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T10:46:01.100+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:46:01.100+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T10:46:01.203+0000] {logging_mixin.py:188} INFO - [2024-01-19T10:46:01.203+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T10:46:01.250+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.663 seconds
[2024-01-19T12:11:30.512+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:11:30.512+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T12:11:30.515+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:11:30.514+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:11:30.536+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:11:30.629+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:11:30.629+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T12:11:30.643+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:11:30.643+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T12:11:30.653+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.145 seconds
[2024-01-19T12:12:00.847+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:12:00.848+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T12:12:00.852+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:12:00.851+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:12:00.871+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:12:00.893+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:12:00.893+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T12:12:00.906+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:12:00.906+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-01T00:00:00+00:00, run_after=2024-01-02T00:00:00+00:00
[2024-01-19T12:12:00.914+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T12:12:31.229+0000] {processor.py:161} INFO - Started process (PID=54) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:12:31.230+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T12:12:31.232+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:12:31.231+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:12:31.243+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:12:31.264+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:12:31.264+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T12:12:31.278+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:12:31.278+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T12:12:31.289+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.062 seconds
[2024-01-19T12:13:01.560+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:13:01.562+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T12:13:01.566+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:13:01.565+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:13:01.587+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:13:01.613+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:13:01.613+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T12:13:01.627+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:13:01.627+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T12:13:01.638+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.083 seconds
[2024-01-19T12:13:02.597+0000] {processor.py:161} INFO - Started process (PID=62) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:13:02.598+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T12:13:02.601+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:13:02.600+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:13:02.619+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:13:02.639+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:13:02.639+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T12:13:02.651+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:13:02.650+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T12:13:02.660+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.068 seconds
[2024-01-19T12:13:08.745+0000] {processor.py:161} INFO - Started process (PID=63) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:13:08.747+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T12:13:08.749+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:13:08.749+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:13:08.772+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:13:08.797+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:13:08.797+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T12:13:08.812+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:13:08.812+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T12:13:08.822+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.080 seconds
[2024-01-19T12:13:39.051+0000] {processor.py:161} INFO - Started process (PID=70) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:13:39.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T12:13:39.054+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:13:39.054+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:13:39.067+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:13:39.090+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:13:39.090+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T12:13:39.105+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:13:39.105+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T12:13:39.115+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.067 seconds
[2024-01-19T12:14:09.367+0000] {processor.py:161} INFO - Started process (PID=77) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:14:09.369+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T12:14:09.372+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:14:09.371+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:14:09.392+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:14:09.413+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:14:09.413+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T12:14:09.426+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:14:09.426+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T12:14:09.434+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.072 seconds
[2024-01-19T12:14:39.666+0000] {processor.py:161} INFO - Started process (PID=84) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:14:39.667+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T12:14:39.670+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:14:39.669+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:14:39.687+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:14:39.708+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:14:39.708+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T12:14:39.722+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:14:39.721+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T12:14:39.730+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.069 seconds
[2024-01-19T12:15:09.971+0000] {processor.py:161} INFO - Started process (PID=91) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:15:09.972+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T12:15:09.973+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:15:09.973+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:15:09.983+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:15:10.000+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:15:10.000+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T12:15:10.013+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:15:10.012+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T12:15:10.022+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.054 seconds
[2024-01-19T12:15:40.307+0000] {processor.py:161} INFO - Started process (PID=98) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:15:40.307+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T12:15:40.309+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:15:40.309+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:15:40.319+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:15:40.336+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:15:40.336+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T12:15:40.350+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:15:40.350+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T12:15:40.360+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.056 seconds
[2024-01-19T12:16:10.592+0000] {processor.py:161} INFO - Started process (PID=105) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:16:10.593+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T12:16:10.594+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:16:10.594+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:16:10.605+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:16:10.623+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:16:10.623+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T12:16:10.637+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:16:10.636+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T12:16:10.646+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.056 seconds
[2024-01-19T12:16:40.932+0000] {processor.py:161} INFO - Started process (PID=112) to work on /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:16:40.934+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/CrawlCompanyInfo.py for tasks to queue
[2024-01-19T12:16:40.938+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:16:40.937+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:16:40.961+0000] {processor.py:840} INFO - DAG(s) 'company_profile_pipeline' retrieved from /opt/airflow/dags/CrawlCompanyInfo.py
[2024-01-19T12:16:40.990+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:16:40.990+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-01-19T12:16:41.004+0000] {logging_mixin.py:188} INFO - [2024-01-19T12:16:41.004+0000] {dag.py:3820} INFO - Setting next_dagrun for company_profile_pipeline to 2024-01-19T00:00:00+00:00, run_after=2024-01-20T00:00:00+00:00
[2024-01-19T12:16:41.014+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/CrawlCompanyInfo.py took 0.088 seconds
